{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "예측값 사용 VS 사용하지 않았을 때의 비교."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: swig in c:\\users\\se99a\\anaconda3\\lib\\site-packages (4.1.1)\n",
      "Requirement already satisfied: wrds in c:\\users\\se99a\\anaconda3\\lib\\site-packages (3.1.6)\n",
      "Requirement already satisfied: pandas in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from wrds) (1.5.3)\n",
      "Requirement already satisfied: psycopg2-binary in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from wrds) (2.9.6)\n",
      "Requirement already satisfied: scipy in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from wrds) (1.10.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from wrds) (1.23.5)\n",
      "Requirement already satisfied: sqlalchemy<2 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from wrds) (1.4.39)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from sqlalchemy<2->wrds) (2.0.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from pandas->wrds) (2022.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\se99a\\appdata\\roaming\\python\\python310\\site-packages (from pandas->wrds) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\se99a\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil>=2.8.1->pandas->wrds) (1.16.0)\n",
      "Collecting pyportfolioopt\n",
      "  Using cached pyportfolioopt-1.5.5-py3-none-any.whl (61 kB)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.22.4 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from pyportfolioopt) (1.23.5)\n",
      "Collecting cvxpy<2.0.0,>=1.1.19\n",
      "  Downloading cvxpy-1.3.2-cp310-cp310-win_amd64.whl (892 kB)\n",
      "     -------------------------------------- 892.6/892.6 kB 2.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pandas>=0.19 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from pyportfolioopt) (1.5.3)\n",
      "Requirement already satisfied: scipy<2.0,>=1.3 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from pyportfolioopt) (1.10.0)\n",
      "Collecting scs>=1.1.6\n",
      "  Using cached scs-3.2.3-cp310-cp310-win_amd64.whl (8.2 MB)\n",
      "Collecting osqp>=0.4.1\n",
      "  Downloading osqp-0.6.3-cp310-cp310-win_amd64.whl (292 kB)\n",
      "     -------------------------------------- 292.9/292.9 kB 1.1 MB/s eta 0:00:00\n",
      "Collecting ecos>=2\n",
      "  Using cached ecos-2.0.12-cp310-cp310-win_amd64.whl (72 kB)\n",
      "Requirement already satisfied: setuptools>65.5.1 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from cvxpy<2.0.0,>=1.1.19->pyportfolioopt) (65.6.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\se99a\\appdata\\roaming\\python\\python310\\site-packages (from pandas>=0.19->pyportfolioopt) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from pandas>=0.19->pyportfolioopt) (2022.7)\n",
      "Collecting qdldl\n",
      "  Using cached qdldl-0.1.7-cp310-cp310-win_amd64.whl (83 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\se99a\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil>=2.8.1->pandas>=0.19->pyportfolioopt) (1.16.0)\n",
      "Installing collected packages: scs, qdldl, ecos, osqp, cvxpy, pyportfolioopt\n",
      "Successfully installed cvxpy-1.3.2 ecos-2.0.12 osqp-0.6.3 pyportfolioopt-1.5.5 qdldl-0.1.7 scs-3.2.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'apt-get'��(��) ���� �Ǵ� �ܺ� ����, ������ �� �ִ� ���α׷�, �Ǵ�\n",
      "��ġ ������ �ƴմϴ�.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/AI4Finance-Foundation/FinRL.git\n",
      "  Cloning https://github.com/AI4Finance-Foundation/FinRL.git to c:\\users\\public\\documents\\estsoft\\creatortemp\\pip-req-build-2wreq9ph\n",
      "  Resolved https://github.com/AI4Finance-Foundation/FinRL.git to commit cfdbe462627bd6355dccee215c1aac65873c6d67\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git#egg=elegantrl\n",
      "  Cloning https://github.com/AI4Finance-Foundation/ElegantRL.git to c:\\users\\public\\documents\\estsoft\\creatortemp\\pip-install-qru5k456\\elegantrl_35eccca952f24aa4a66b05925c0b0253\n",
      "  Resolved https://github.com/AI4Finance-Foundation/ElegantRL.git to commit 59fe4b4a1cca0a28e8a5fa4fb80eed9b5d472978\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: ray[default,tune]<3,>=2 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from finrl==0.3.6) (2.5.1)\n",
      "Requirement already satisfied: wrds<4,>=3 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from finrl==0.3.6) (3.1.6)\n",
      "Requirement already satisfied: alpaca-trade-api<4,>=3 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from finrl==0.3.6) (3.0.2)\n",
      "Requirement already satisfied: scikit-learn<2,>=1 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from finrl==0.3.6) (1.2.1)\n",
      "Requirement already satisfied: yfinance<0.3,>=0.2 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from finrl==0.3.6) (0.2.22)\n",
      "Requirement already satisfied: pyportfolioopt<2,>=1 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from finrl==0.3.6) (1.5.5)\n",
      "Requirement already satisfied: jqdatasdk<2,>=1 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from finrl==0.3.6) (1.8.11)\n",
      "Requirement already satisfied: pyfolio<0.10,>=0.9 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from finrl==0.3.6) (0.9.2)\n",
      "Collecting stockstats<0.6,>=0.5\n",
      "  Downloading stockstats-0.5.4-py2.py3-none-any.whl (21 kB)\n",
      "Collecting ccxt<4,>=3\n",
      "  Downloading ccxt-3.1.60-py2.py3-none-any.whl (4.0 MB)\n",
      "     ---------------------------------------- 4.0/4.0 MB 10.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: exchange-calendars<5,>=4 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from finrl==0.3.6) (4.2.8)\n",
      "Requirement already satisfied: stable-baselines3[extra]>=2.0.0a5 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from finrl==0.3.6) (2.0.0)\n",
      "Requirement already satisfied: urllib3<2,>1.24 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.6) (1.26.14)\n",
      "Requirement already satisfied: PyYAML==6.0 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.6) (6.0)\n",
      "Requirement already satisfied: msgpack==1.0.3 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.6) (1.0.3)\n",
      "Requirement already satisfied: deprecation==2.1.0 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.6) (2.1.0)\n",
      "Requirement already satisfied: websockets<11,>=9.0 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.6) (10.4)\n",
      "Requirement already satisfied: requests<3,>2 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.6) (2.28.1)\n",
      "Requirement already satisfied: aiohttp==3.8.2 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.6) (3.8.2)\n",
      "Requirement already satisfied: pandas>=0.18.1 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.6) (1.5.3)\n",
      "Requirement already satisfied: websocket-client<2,>=0.56.0 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.6) (0.58.0)\n",
      "Requirement already satisfied: numpy>=1.11.1 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.6) (1.23.5)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from aiohttp==3.8.2->alpaca-trade-api<4,>=3->finrl==0.3.6) (4.0.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from aiohttp==3.8.2->alpaca-trade-api<4,>=3->finrl==0.3.6) (22.1.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from aiohttp==3.8.2->alpaca-trade-api<4,>=3->finrl==0.3.6) (2.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from aiohttp==3.8.2->alpaca-trade-api<4,>=3->finrl==0.3.6) (1.9.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from aiohttp==3.8.2->alpaca-trade-api<4,>=3->finrl==0.3.6) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from aiohttp==3.8.2->alpaca-trade-api<4,>=3->finrl==0.3.6) (1.3.3)\n",
      "Requirement already satisfied: multidict<6.0,>=4.5 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from aiohttp==3.8.2->alpaca-trade-api<4,>=3->finrl==0.3.6) (5.2.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\se99a\\appdata\\roaming\\python\\python310\\site-packages (from deprecation==2.1.0->alpaca-trade-api<4,>=3->finrl==0.3.6) (23.0)\n",
      "Requirement already satisfied: aiodns>=1.1.1 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from ccxt<4,>=3->finrl==0.3.6) (3.0.0)\n",
      "Requirement already satisfied: setuptools>=60.9.0 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from ccxt<4,>=3->finrl==0.3.6) (65.6.3)\n",
      "Requirement already satisfied: certifi>=2018.1.18 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from ccxt<4,>=3->finrl==0.3.6) (2022.12.7)\n",
      "Requirement already satisfied: cryptography>=2.6.1 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from ccxt<4,>=3->finrl==0.3.6) (39.0.1)\n",
      "Requirement already satisfied: pytz in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from exchange-calendars<5,>=4->finrl==0.3.6) (2022.7)\n",
      "Requirement already satisfied: toolz in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from exchange-calendars<5,>=4->finrl==0.3.6) (0.12.0)\n",
      "Requirement already satisfied: pyluach in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from exchange-calendars<5,>=4->finrl==0.3.6) (2.2.0)\n",
      "Requirement already satisfied: korean-lunar-calendar in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from exchange-calendars<5,>=4->finrl==0.3.6) (0.3.1)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\se99a\\appdata\\roaming\\python\\python310\\site-packages (from exchange-calendars<5,>=4->finrl==0.3.6) (2.8.2)\n",
      "Requirement already satisfied: thriftpy2>=0.3.9 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from jqdatasdk<2,>=1->finrl==0.3.6) (0.4.16)\n",
      "Requirement already satisfied: pymysql>=0.7.6 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from jqdatasdk<2,>=1->finrl==0.3.6) (1.1.0)\n",
      "Requirement already satisfied: six in c:\\users\\se99a\\appdata\\roaming\\python\\python310\\site-packages (from jqdatasdk<2,>=1->finrl==0.3.6) (1.16.0)\n",
      "Requirement already satisfied: SQLAlchemy>=1.2.8 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from jqdatasdk<2,>=1->finrl==0.3.6) (1.4.39)\n",
      "Requirement already satisfied: matplotlib>=1.4.0 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from pyfolio<0.10,>=0.9->finrl==0.3.6) (3.7.0)\n",
      "Requirement already satisfied: scipy>=0.14.0 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from pyfolio<0.10,>=0.9->finrl==0.3.6) (1.10.0)\n",
      "Requirement already satisfied: ipython>=3.2.3 in c:\\users\\se99a\\appdata\\roaming\\python\\python310\\site-packages (from pyfolio<0.10,>=0.9->finrl==0.3.6) (8.11.0)\n",
      "Requirement already satisfied: empyrical>=0.5.0 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from pyfolio<0.10,>=0.9->finrl==0.3.6) (0.5.5)\n",
      "Requirement already satisfied: seaborn>=0.7.1 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from pyfolio<0.10,>=0.9->finrl==0.3.6) (0.12.2)\n",
      "Requirement already satisfied: cvxpy<2.0.0,>=1.1.19 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from pyportfolioopt<2,>=1->finrl==0.3.6) (1.3.2)\n",
      "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (4.23.3)\n",
      "Requirement already satisfied: grpcio<=1.51.3,>=1.42.0 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (1.51.3)\n",
      "Requirement already satisfied: click>=7.0 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (8.0.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (3.9.0)\n",
      "Requirement already satisfied: jsonschema in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (4.17.3)\n",
      "Requirement already satisfied: pyarrow<7.0.0,>=6.0.1 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (6.0.1)\n",
      "Requirement already satisfied: tensorboardX>=1.9 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (2.6.1)\n",
      "Requirement already satisfied: aiohttp-cors in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (0.7.0)\n",
      "Requirement already satisfied: smart-open in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (5.2.1)\n",
      "Requirement already satisfied: gpustat>=1.0.0 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (1.1)\n",
      "Requirement already satisfied: colorful in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (0.5.5)\n",
      "Requirement already satisfied: pydantic in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (2.0)\n",
      "Requirement already satisfied: virtualenv<20.21.1,>=20.0.24 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (20.21.0)\n",
      "Requirement already satisfied: opencensus in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (0.11.2)\n",
      "Requirement already satisfied: py-spy>=0.2.0 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (0.3.14)\n",
      "Requirement already satisfied: prometheus-client>=0.7.1 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (0.14.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from scikit-learn<2,>=1->finrl==0.3.6) (1.1.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from scikit-learn<2,>=1->finrl==0.3.6) (2.2.0)\n",
      "Requirement already satisfied: torch>=1.11 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (2.0.1)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (2.0.0)\n",
      "Requirement already satisfied: gymnasium==0.28.1 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (0.28.1)\n",
      "Requirement already satisfied: rich in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (13.4.2)\n",
      "Requirement already satisfied: autorom[accept-rom-license]~=0.6.0 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (0.6.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (4.64.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\se99a\\appdata\\roaming\\python\\python310\\site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (5.9.4)\n",
      "Requirement already satisfied: tensorboard>=2.9.1 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (2.12.3)\n",
      "Requirement already satisfied: shimmy[atari]~=0.2.1 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (0.2.1)\n",
      "Requirement already satisfied: pillow in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (9.4.0)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (4.8.0.74)\n",
      "Requirement already satisfied: pygame in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (2.1.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from gymnasium==0.28.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (0.0.4)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from gymnasium==0.28.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (4.7.1)\n",
      "Requirement already satisfied: jax-jumpy>=1.0.0 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from gymnasium==0.28.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (1.0.0)\n",
      "Requirement already satisfied: psycopg2-binary in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from wrds<4,>=3->finrl==0.3.6) (2.9.6)\n",
      "Requirement already satisfied: multitasking>=0.0.7 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from yfinance<0.3,>=0.2->finrl==0.3.6) (0.0.11)\n",
      "Requirement already satisfied: frozendict>=2.3.4 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from yfinance<0.3,>=0.2->finrl==0.3.6) (2.3.8)\n",
      "Requirement already satisfied: appdirs>=1.4.4 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from yfinance<0.3,>=0.2->finrl==0.3.6) (1.4.4)\n",
      "Requirement already satisfied: lxml>=4.9.1 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from yfinance<0.3,>=0.2->finrl==0.3.6) (4.9.1)\n",
      "Requirement already satisfied: beautifulsoup4>=4.11.1 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from yfinance<0.3,>=0.2->finrl==0.3.6) (4.11.1)\n",
      "Requirement already satisfied: html5lib>=1.1 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from yfinance<0.3,>=0.2->finrl==0.3.6) (1.1)\n",
      "Requirement already satisfied: gym in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git#egg=elegantrl->finrl==0.3.6) (0.26.2)\n",
      "Requirement already satisfied: pycares>=4.0.0 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from aiodns>=1.1.1->ccxt<4,>=3->finrl==0.3.6) (4.3.0)\n",
      "Requirement already satisfied: AutoROM.accept-rom-license in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from autorom[accept-rom-license]~=0.6.0->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (0.6.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from beautifulsoup4>=4.11.1->yfinance<0.3,>=0.2->finrl==0.3.6) (2.3.2.post1)\n",
      "Requirement already satisfied: colorama in c:\\users\\se99a\\appdata\\roaming\\python\\python310\\site-packages (from click>=7.0->ray[default,tune]<3,>=2->finrl==0.3.6) (0.4.6)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from cryptography>=2.6.1->ccxt<4,>=3->finrl==0.3.6) (1.15.1)\n",
      "Requirement already satisfied: osqp>=0.4.1 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from cvxpy<2.0.0,>=1.1.19->pyportfolioopt<2,>=1->finrl==0.3.6) (0.6.3)\n",
      "Requirement already satisfied: ecos>=2 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from cvxpy<2.0.0,>=1.1.19->pyportfolioopt<2,>=1->finrl==0.3.6) (2.0.12)\n",
      "Requirement already satisfied: scs>=1.1.6 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from cvxpy<2.0.0,>=1.1.19->pyportfolioopt<2,>=1->finrl==0.3.6) (3.2.3)\n",
      "Requirement already satisfied: pandas-datareader>=0.2 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from empyrical>=0.5.0->pyfolio<0.10,>=0.9->finrl==0.3.6) (0.10.0)\n",
      "Requirement already satisfied: blessed>=1.17.1 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from gpustat>=1.0.0->ray[default,tune]<3,>=2->finrl==0.3.6) (1.20.0)\n",
      "Requirement already satisfied: nvidia-ml-py>=11.450.129 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from gpustat>=1.0.0->ray[default,tune]<3,>=2->finrl==0.3.6) (11.525.131)\n",
      "Requirement already satisfied: webencodings in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from html5lib>=1.1->yfinance<0.3,>=0.2->finrl==0.3.6) (0.5.1)\n",
      "Requirement already satisfied: backcall in c:\\users\\se99a\\appdata\\roaming\\python\\python310\\site-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (0.2.0)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\se99a\\appdata\\roaming\\python\\python310\\site-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (0.1.6)\n",
      "Requirement already satisfied: stack-data in c:\\users\\se99a\\appdata\\roaming\\python\\python310\\site-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (0.6.2)\n",
      "Requirement already satisfied: decorator in c:\\users\\se99a\\appdata\\roaming\\python\\python310\\site-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\se99a\\appdata\\roaming\\python\\python310\\site-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (0.18.2)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in c:\\users\\se99a\\appdata\\roaming\\python\\python310\\site-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (3.0.38)\n",
      "Requirement already satisfied: traitlets>=5 in c:\\users\\se99a\\appdata\\roaming\\python\\python310\\site-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (5.9.0)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\se99a\\appdata\\roaming\\python\\python310\\site-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (2.14.0)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\se99a\\appdata\\roaming\\python\\python310\\site-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (0.7.5)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from matplotlib>=1.4.0->pyfolio<0.10,>=0.9->finrl==0.3.6) (0.11.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from matplotlib>=1.4.0->pyfolio<0.10,>=0.9->finrl==0.3.6) (1.0.5)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from matplotlib>=1.4.0->pyfolio<0.10,>=0.9->finrl==0.3.6) (4.25.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from matplotlib>=1.4.0->pyfolio<0.10,>=0.9->finrl==0.3.6) (3.0.9)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from matplotlib>=1.4.0->pyfolio<0.10,>=0.9->finrl==0.3.6) (1.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from requests<3,>2->alpaca-trade-api<4,>=3->finrl==0.3.6) (3.4)\n",
      "Requirement already satisfied: ale-py~=0.8.1 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from shimmy[atari]~=0.2.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (0.8.1)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from SQLAlchemy>=1.2.8->jqdatasdk<2,>=1->finrl==0.3.6) (2.0.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (1.0.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (0.7.1)\n",
      "Requirement already satisfied: absl-py>=0.4 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (1.4.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (2.21.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (3.4.1)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (0.38.4)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (2.2.2)\n",
      "Requirement already satisfied: ply<4.0,>=3.4 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from thriftpy2>=0.3.9->jqdatasdk<2,>=1->finrl==0.3.6) (3.11)\n",
      "Requirement already satisfied: networkx in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from torch>=1.11->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (2.8.4)\n",
      "Requirement already satisfied: sympy in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from torch>=1.11->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (1.11.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from torch>=1.11->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (3.1.2)\n",
      "Requirement already satisfied: platformdirs<4,>=2.4 in c:\\users\\se99a\\appdata\\roaming\\python\\python310\\site-packages (from virtualenv<20.21.1,>=20.0.24->ray[default,tune]<3,>=2->finrl==0.3.6) (3.1.1)\n",
      "Requirement already satisfied: distlib<1,>=0.3.6 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from virtualenv<20.21.1,>=20.0.24->ray[default,tune]<3,>=2->finrl==0.3.6) (0.3.6)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from gym->elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git#egg=elegantrl->finrl==0.3.6) (0.0.8)\n",
      "Requirement already satisfied: box2d-py==2.3.5 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from gym->elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git#egg=elegantrl->finrl==0.3.6) (2.3.5)\n",
      "Requirement already satisfied: swig==4.* in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from gym->elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git#egg=elegantrl->finrl==0.3.6) (4.1.1)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from jsonschema->ray[default,tune]<3,>=2->finrl==0.3.6) (0.18.0)\n",
      "Requirement already satisfied: google-api-core<3.0.0,>=1.0.0 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from opencensus->ray[default,tune]<3,>=2->finrl==0.3.6) (2.11.1)\n",
      "Requirement already satisfied: opencensus-context>=0.1.3 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from opencensus->ray[default,tune]<3,>=2->finrl==0.3.6) (0.1.3)\n",
      "Requirement already satisfied: pydantic-core==2.0.1 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from pydantic->ray[default,tune]<3,>=2->finrl==0.3.6) (2.0.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from pydantic->ray[default,tune]<3,>=2->finrl==0.3.6) (0.5.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from rich->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (3.0.0)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from ale-py~=0.8.1->shimmy[atari]~=0.2.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (5.12.0)\n",
      "Requirement already satisfied: jinxed>=1.1.0 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from blessed>=1.17.1->gpustat>=1.0.0->ray[default,tune]<3,>=2->finrl==0.3.6) (1.2.0)\n",
      "Requirement already satisfied: wcwidth>=0.1.4 in c:\\users\\se99a\\appdata\\roaming\\python\\python310\\site-packages (from blessed>=1.17.1->gpustat>=1.0.0->ray[default,tune]<3,>=2->finrl==0.3.6) (0.2.6)\n",
      "Requirement already satisfied: pycparser in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from cffi>=1.12->cryptography>=2.6.1->ccxt<4,>=3->finrl==0.3.6) (2.21)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.6) (1.59.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (5.3.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (1.3.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\users\\se99a\\appdata\\roaming\\python\\python310\\site-packages (from jedi>=0.16->ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (0.8.3)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (0.1.2)\n",
      "Requirement already satisfied: qdldl in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from osqp>=0.4.1->cvxpy<2.0.0,>=1.1.19->pyportfolioopt<2,>=1->finrl==0.3.6) (0.1.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (2.1.1)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\se99a\\appdata\\roaming\\python\\python310\\site-packages (from stack-data->ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (0.2.2)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\se99a\\appdata\\roaming\\python\\python310\\site-packages (from stack-data->ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (1.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\se99a\\appdata\\roaming\\python\\python310\\site-packages (from stack-data->ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (2.2.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from sympy->torch>=1.11->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (1.2.1)\n",
      "Requirement already satisfied: ansicon in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from jinxed>=1.1.0->blessed>=1.17.1->gpustat>=1.0.0->ray[default,tune]<3,>=2->finrl==0.3.6) (1.89.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (3.2.2)\n",
      "Building wheels for collected packages: finrl\n",
      "  Building wheel for finrl (pyproject.toml): started\n",
      "  Building wheel for finrl (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for finrl: filename=finrl-0.3.6-py3-none-any.whl size=4670209 sha256=720c4fb35e48bc0b8ec8aacb445b3f943085bbe4726ce0b96212bd95be457a70\n",
      "  Stored in directory: C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\pip-ephem-wheel-cache-hhlnhuie\\wheels\\72\\3b\\1a\\0fc805a8cc65ecd5bfe4f74a3c586b6075678b8ba53fd8f749\n",
      "Successfully built finrl\n",
      "Installing collected packages: stockstats, ccxt, finrl\n",
      "  Attempting uninstall: stockstats\n",
      "    Found existing installation: stockstats 0.6.0\n",
      "    Uninstalling stockstats-0.6.0:\n",
      "      Successfully uninstalled stockstats-0.6.0\n",
      "  Attempting uninstall: ccxt\n",
      "    Found existing installation: ccxt 4.0.6\n",
      "    Uninstalling ccxt-4.0.6:\n",
      "      Successfully uninstalled ccxt-4.0.6\n",
      "  Attempting uninstall: finrl\n",
      "    Found existing installation: finrl 0.3.4\n",
      "    Uninstalling finrl-0.3.4:\n",
      "      Successfully uninstalled finrl-0.3.4\n",
      "Successfully installed ccxt-3.1.60 finrl-0.3.6 stockstats-0.5.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/AI4Finance-Foundation/FinRL.git 'C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\pip-req-build-2wreq9ph'\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/AI4Finance-Foundation/ElegantRL.git 'C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\pip-install-qru5k456\\elegantrl_35eccca952f24aa4a66b05925c0b0253'\n"
     ]
    }
   ],
   "source": [
    "## install required packages\n",
    "!pip install swig\n",
    "!pip install wrds\n",
    "!pip install pyportfolioopt\n",
    "## install finrl library\n",
    "!pip install -q condacolab\n",
    "!apt-get update -y -qq && apt-get install -y -qq cmake libopenmpi-dev python3-dev zlib1g-dev libgl1-mesa-glx swig\n",
    "!pip install git+https://github.com/AI4Finance-Foundation/FinRL.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\se99a\\anaconda3\\lib\\site-packages\\pyfolio\\pos.py:26: UserWarning: Module \"zipline.assets\" not found; mutltipliers will not be applied to position notionals.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "# matplotlib.use('Agg')\n",
    "import datetime\n",
    "\n",
    "%matplotlib inline\n",
    "from finrl.meta.preprocessor.yahoodownloader import YahooDownloader\n",
    "from finrl.meta.preprocessor.preprocessors import FeatureEngineer, data_split\n",
    "from finrl.meta.env_stock_trading.env_stocktrading import StockTradingEnv\n",
    "from finrl.agents.stablebaselines3.models import DRLAgent\n",
    "from stable_baselines3.common.logger import configure\n",
    "from finrl.meta.data_processor import DataProcessor\n",
    "\n",
    "from finrl.plot import backtest_stats, backtest_plot, get_daily_return, get_baseline\n",
    "from pprint import pprint\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../FinRL\")\n",
    "\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finrl import config\n",
    "from finrl import config_tickers\n",
    "import os\n",
    "from finrl.main import check_and_make_directories\n",
    "from finrl.config import (\n",
    "    DATA_SAVE_DIR,\n",
    "    TRAINED_MODEL_DIR,\n",
    "    TENSORBOARD_LOG_DIR,\n",
    "    RESULTS_DIR,\n",
    "    INDICATORS,\n",
    "    TRAIN_START_DATE,\n",
    "    TRAIN_END_DATE,\n",
    "    TEST_START_DATE,\n",
    "    TEST_END_DATE,\n",
    "    TRADE_START_DATE,\n",
    "    TRADE_END_DATE,\n",
    ")\n",
    "check_and_make_directories([DATA_SAVE_DIR, TRAINED_MODEL_DIR, TENSORBOARD_LOG_DIR, RESULTS_DIR])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2020-07-31'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from config.py, TRAIN_START_DATE is a string\n",
    "TRAIN_START_DATE\n",
    "# from config.py, TRAIN_END_DATE is a string\n",
    "TRAIN_END_DATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_START_DATE = '2010-01-01'\n",
    "TRAIN_END_DATE = '2021-10-01'\n",
    "TRADE_START_DATE = '2021-10-01'\n",
    "TRADE_END_DATE = '2023-05-05'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = ['XLB','XLE','XLF','XLI','XLK','XLP','XLU','XLV','XLY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Shape of DataFrame:  (30213, 8)\n"
     ]
    }
   ],
   "source": [
    "df = YahooDownloader(start_date = TRAIN_START_DATE,\n",
    "                     end_date = TRADE_END_DATE,\n",
    "                     ticker_list = tickers).fetch_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>tic</th>\n",
       "      <th>day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>33.580002</td>\n",
       "      <td>34.020000</td>\n",
       "      <td>33.450001</td>\n",
       "      <td>25.364519</td>\n",
       "      <td>7567500</td>\n",
       "      <td>XLB</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>57.919998</td>\n",
       "      <td>58.810001</td>\n",
       "      <td>57.790001</td>\n",
       "      <td>37.747696</td>\n",
       "      <td>16928400</td>\n",
       "      <td>XLE</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>11.795288</td>\n",
       "      <td>11.965881</td>\n",
       "      <td>11.770918</td>\n",
       "      <td>9.344018</td>\n",
       "      <td>92942347</td>\n",
       "      <td>XLF</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>28.090000</td>\n",
       "      <td>28.320000</td>\n",
       "      <td>27.959999</td>\n",
       "      <td>21.793045</td>\n",
       "      <td>7471500</td>\n",
       "      <td>XLI</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>23.139999</td>\n",
       "      <td>23.290001</td>\n",
       "      <td>23.100000</td>\n",
       "      <td>19.110191</td>\n",
       "      <td>8449400</td>\n",
       "      <td>XLK</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30208</th>\n",
       "      <td>2023-05-04</td>\n",
       "      <td>147.660004</td>\n",
       "      <td>148.539993</td>\n",
       "      <td>146.929993</td>\n",
       "      <td>147.231598</td>\n",
       "      <td>5430500</td>\n",
       "      <td>XLK</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30209</th>\n",
       "      <td>2023-05-04</td>\n",
       "      <td>76.739998</td>\n",
       "      <td>76.860001</td>\n",
       "      <td>76.220001</td>\n",
       "      <td>75.931320</td>\n",
       "      <td>11682600</td>\n",
       "      <td>XLP</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30210</th>\n",
       "      <td>2023-05-04</td>\n",
       "      <td>68.190002</td>\n",
       "      <td>68.860001</td>\n",
       "      <td>67.550003</td>\n",
       "      <td>68.042702</td>\n",
       "      <td>14631100</td>\n",
       "      <td>XLU</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30211</th>\n",
       "      <td>2023-05-04</td>\n",
       "      <td>132.899994</td>\n",
       "      <td>132.979996</td>\n",
       "      <td>131.839996</td>\n",
       "      <td>131.857208</td>\n",
       "      <td>9625300</td>\n",
       "      <td>XLV</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30212</th>\n",
       "      <td>2023-05-04</td>\n",
       "      <td>145.410004</td>\n",
       "      <td>146.039993</td>\n",
       "      <td>144.039993</td>\n",
       "      <td>143.980392</td>\n",
       "      <td>5331500</td>\n",
       "      <td>XLY</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30213 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             date        open        high         low       close    volume  \\\n",
       "0      2010-01-04   33.580002   34.020000   33.450001   25.364519   7567500   \n",
       "1      2010-01-04   57.919998   58.810001   57.790001   37.747696  16928400   \n",
       "2      2010-01-04   11.795288   11.965881   11.770918    9.344018  92942347   \n",
       "3      2010-01-04   28.090000   28.320000   27.959999   21.793045   7471500   \n",
       "4      2010-01-04   23.139999   23.290001   23.100000   19.110191   8449400   \n",
       "...           ...         ...         ...         ...         ...       ...   \n",
       "30208  2023-05-04  147.660004  148.539993  146.929993  147.231598   5430500   \n",
       "30209  2023-05-04   76.739998   76.860001   76.220001   75.931320  11682600   \n",
       "30210  2023-05-04   68.190002   68.860001   67.550003   68.042702  14631100   \n",
       "30211  2023-05-04  132.899994  132.979996  131.839996  131.857208   9625300   \n",
       "30212  2023-05-04  145.410004  146.039993  144.039993  143.980392   5331500   \n",
       "\n",
       "       tic  day  \n",
       "0      XLB    0  \n",
       "1      XLE    0  \n",
       "2      XLF    0  \n",
       "3      XLI    0  \n",
       "4      XLK    0  \n",
       "...    ...  ...  \n",
       "30208  XLK    3  \n",
       "30209  XLP    3  \n",
       "30210  XLU    3  \n",
       "30211  XLV    3  \n",
       "30212  XLY    3  \n",
       "\n",
       "[30213 rows x 8 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values(['date','tic'],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['macd',\n",
       " 'boll_ub',\n",
       " 'boll_lb',\n",
       " 'rsi_30',\n",
       " 'cci_30',\n",
       " 'dx_30',\n",
       " 'close_30_sma',\n",
       " 'close_60_sma']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INDICATORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully added technical indicators\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Shape of DataFrame:  (3356, 8)\n",
      "Successfully added vix\n",
      "Successfully added turbulence index\n"
     ]
    }
   ],
   "source": [
    "fe = FeatureEngineer(\n",
    "                    use_technical_indicator=True,\n",
    "                    tech_indicator_list = INDICATORS,\n",
    "                    use_vix=True,\n",
    "                    use_turbulence=True,\n",
    "                    user_defined_feature = False)\n",
    "\n",
    "processed = fe.preprocess_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>tic</th>\n",
       "      <th>day</th>\n",
       "      <th>macd</th>\n",
       "      <th>boll_ub</th>\n",
       "      <th>boll_lb</th>\n",
       "      <th>rsi_30</th>\n",
       "      <th>cci_30</th>\n",
       "      <th>dx_30</th>\n",
       "      <th>close_30_sma</th>\n",
       "      <th>close_60_sma</th>\n",
       "      <th>vix</th>\n",
       "      <th>turbulence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>33.580002</td>\n",
       "      <td>34.020000</td>\n",
       "      <td>33.450001</td>\n",
       "      <td>25.364519</td>\n",
       "      <td>7567500</td>\n",
       "      <td>XLB</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>25.521512</td>\n",
       "      <td>25.289541</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>25.364519</td>\n",
       "      <td>25.364519</td>\n",
       "      <td>20.040001</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>57.919998</td>\n",
       "      <td>58.810001</td>\n",
       "      <td>57.790001</td>\n",
       "      <td>37.747696</td>\n",
       "      <td>16928400</td>\n",
       "      <td>XLE</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>25.521512</td>\n",
       "      <td>25.289541</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>37.747696</td>\n",
       "      <td>37.747696</td>\n",
       "      <td>20.040001</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>11.795288</td>\n",
       "      <td>11.965881</td>\n",
       "      <td>11.770918</td>\n",
       "      <td>9.344018</td>\n",
       "      <td>92942347</td>\n",
       "      <td>XLF</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>25.521512</td>\n",
       "      <td>25.289541</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>9.344018</td>\n",
       "      <td>9.344018</td>\n",
       "      <td>20.040001</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>28.090000</td>\n",
       "      <td>28.320000</td>\n",
       "      <td>27.959999</td>\n",
       "      <td>21.793045</td>\n",
       "      <td>7471500</td>\n",
       "      <td>XLI</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>25.521512</td>\n",
       "      <td>25.289541</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>21.793045</td>\n",
       "      <td>21.793045</td>\n",
       "      <td>20.040001</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>23.139999</td>\n",
       "      <td>23.290001</td>\n",
       "      <td>23.100000</td>\n",
       "      <td>19.110191</td>\n",
       "      <td>8449400</td>\n",
       "      <td>XLK</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>25.521512</td>\n",
       "      <td>25.289541</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>19.110191</td>\n",
       "      <td>19.110191</td>\n",
       "      <td>20.040001</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30199</th>\n",
       "      <td>2023-05-03</td>\n",
       "      <td>149.690002</td>\n",
       "      <td>150.660004</td>\n",
       "      <td>148.179993</td>\n",
       "      <td>147.940125</td>\n",
       "      <td>5815100</td>\n",
       "      <td>XLK</td>\n",
       "      <td>2</td>\n",
       "      <td>1.101265</td>\n",
       "      <td>151.344400</td>\n",
       "      <td>144.908068</td>\n",
       "      <td>55.182961</td>\n",
       "      <td>46.442993</td>\n",
       "      <td>6.225843</td>\n",
       "      <td>147.699630</td>\n",
       "      <td>143.517544</td>\n",
       "      <td>18.340000</td>\n",
       "      <td>4.219994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30200</th>\n",
       "      <td>2023-05-03</td>\n",
       "      <td>77.059998</td>\n",
       "      <td>77.379997</td>\n",
       "      <td>76.639999</td>\n",
       "      <td>76.119987</td>\n",
       "      <td>11656600</td>\n",
       "      <td>XLP</td>\n",
       "      <td>2</td>\n",
       "      <td>0.875702</td>\n",
       "      <td>77.189108</td>\n",
       "      <td>74.079747</td>\n",
       "      <td>58.998054</td>\n",
       "      <td>93.547472</td>\n",
       "      <td>22.508083</td>\n",
       "      <td>74.795711</td>\n",
       "      <td>73.269686</td>\n",
       "      <td>18.340000</td>\n",
       "      <td>4.219994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30201</th>\n",
       "      <td>2023-05-03</td>\n",
       "      <td>68.639999</td>\n",
       "      <td>69.050003</td>\n",
       "      <td>67.989998</td>\n",
       "      <td>67.526855</td>\n",
       "      <td>11857200</td>\n",
       "      <td>XLU</td>\n",
       "      <td>2</td>\n",
       "      <td>0.319781</td>\n",
       "      <td>69.807814</td>\n",
       "      <td>67.590043</td>\n",
       "      <td>50.161291</td>\n",
       "      <td>8.126302</td>\n",
       "      <td>8.132944</td>\n",
       "      <td>67.692193</td>\n",
       "      <td>66.656867</td>\n",
       "      <td>18.340000</td>\n",
       "      <td>4.219994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30202</th>\n",
       "      <td>2023-05-03</td>\n",
       "      <td>134.259995</td>\n",
       "      <td>134.899994</td>\n",
       "      <td>133.270004</td>\n",
       "      <td>132.912888</td>\n",
       "      <td>8961500</td>\n",
       "      <td>XLV</td>\n",
       "      <td>2</td>\n",
       "      <td>0.915705</td>\n",
       "      <td>134.927106</td>\n",
       "      <td>131.520086</td>\n",
       "      <td>53.906948</td>\n",
       "      <td>54.096702</td>\n",
       "      <td>6.687990</td>\n",
       "      <td>131.318106</td>\n",
       "      <td>129.596837</td>\n",
       "      <td>18.340000</td>\n",
       "      <td>4.219994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30203</th>\n",
       "      <td>2023-05-03</td>\n",
       "      <td>145.940002</td>\n",
       "      <td>147.889999</td>\n",
       "      <td>145.279999</td>\n",
       "      <td>145.098145</td>\n",
       "      <td>4256000</td>\n",
       "      <td>XLY</td>\n",
       "      <td>2</td>\n",
       "      <td>0.418781</td>\n",
       "      <td>148.824533</td>\n",
       "      <td>143.067334</td>\n",
       "      <td>50.679820</td>\n",
       "      <td>23.335247</td>\n",
       "      <td>0.925005</td>\n",
       "      <td>145.369260</td>\n",
       "      <td>145.068299</td>\n",
       "      <td>18.340000</td>\n",
       "      <td>4.219994</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30204 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             date        open        high         low       close    volume  \\\n",
       "0      2010-01-04   33.580002   34.020000   33.450001   25.364519   7567500   \n",
       "1      2010-01-04   57.919998   58.810001   57.790001   37.747696  16928400   \n",
       "2      2010-01-04   11.795288   11.965881   11.770918    9.344018  92942347   \n",
       "3      2010-01-04   28.090000   28.320000   27.959999   21.793045   7471500   \n",
       "4      2010-01-04   23.139999   23.290001   23.100000   19.110191   8449400   \n",
       "...           ...         ...         ...         ...         ...       ...   \n",
       "30199  2023-05-03  149.690002  150.660004  148.179993  147.940125   5815100   \n",
       "30200  2023-05-03   77.059998   77.379997   76.639999   76.119987  11656600   \n",
       "30201  2023-05-03   68.639999   69.050003   67.989998   67.526855  11857200   \n",
       "30202  2023-05-03  134.259995  134.899994  133.270004  132.912888   8961500   \n",
       "30203  2023-05-03  145.940002  147.889999  145.279999  145.098145   4256000   \n",
       "\n",
       "       tic  day      macd     boll_ub     boll_lb      rsi_30     cci_30  \\\n",
       "0      XLB    0  0.000000   25.521512   25.289541  100.000000  66.666667   \n",
       "1      XLE    0  0.000000   25.521512   25.289541  100.000000  66.666667   \n",
       "2      XLF    0  0.000000   25.521512   25.289541  100.000000  66.666667   \n",
       "3      XLI    0  0.000000   25.521512   25.289541  100.000000  66.666667   \n",
       "4      XLK    0  0.000000   25.521512   25.289541  100.000000  66.666667   \n",
       "...    ...  ...       ...         ...         ...         ...        ...   \n",
       "30199  XLK    2  1.101265  151.344400  144.908068   55.182961  46.442993   \n",
       "30200  XLP    2  0.875702   77.189108   74.079747   58.998054  93.547472   \n",
       "30201  XLU    2  0.319781   69.807814   67.590043   50.161291   8.126302   \n",
       "30202  XLV    2  0.915705  134.927106  131.520086   53.906948  54.096702   \n",
       "30203  XLY    2  0.418781  148.824533  143.067334   50.679820  23.335247   \n",
       "\n",
       "            dx_30  close_30_sma  close_60_sma        vix  turbulence  \n",
       "0      100.000000     25.364519     25.364519  20.040001    0.000000  \n",
       "1      100.000000     37.747696     37.747696  20.040001    0.000000  \n",
       "2      100.000000      9.344018      9.344018  20.040001    0.000000  \n",
       "3      100.000000     21.793045     21.793045  20.040001    0.000000  \n",
       "4      100.000000     19.110191     19.110191  20.040001    0.000000  \n",
       "...           ...           ...           ...        ...         ...  \n",
       "30199    6.225843    147.699630    143.517544  18.340000    4.219994  \n",
       "30200   22.508083     74.795711     73.269686  18.340000    4.219994  \n",
       "30201    8.132944     67.692193     66.656867  18.340000    4.219994  \n",
       "30202    6.687990    131.318106    129.596837  18.340000    4.219994  \n",
       "30203    0.925005    145.369260    145.068299  18.340000    4.219994  \n",
       "\n",
       "[30204 rows x 18 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>tic</th>\n",
       "      <th>day</th>\n",
       "      <th>macd</th>\n",
       "      <th>boll_ub</th>\n",
       "      <th>boll_lb</th>\n",
       "      <th>rsi_30</th>\n",
       "      <th>cci_30</th>\n",
       "      <th>dx_30</th>\n",
       "      <th>close_30_sma</th>\n",
       "      <th>close_60_sma</th>\n",
       "      <th>vix</th>\n",
       "      <th>turbulence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>33.580002</td>\n",
       "      <td>34.020000</td>\n",
       "      <td>33.450001</td>\n",
       "      <td>25.364519</td>\n",
       "      <td>7567500</td>\n",
       "      <td>XLB</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.521512</td>\n",
       "      <td>25.289541</td>\n",
       "      <td>100.0</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>100.0</td>\n",
       "      <td>25.364519</td>\n",
       "      <td>25.364519</td>\n",
       "      <td>20.040001</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>57.919998</td>\n",
       "      <td>58.810001</td>\n",
       "      <td>57.790001</td>\n",
       "      <td>37.747696</td>\n",
       "      <td>16928400</td>\n",
       "      <td>XLE</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.521512</td>\n",
       "      <td>25.289541</td>\n",
       "      <td>100.0</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>100.0</td>\n",
       "      <td>37.747696</td>\n",
       "      <td>37.747696</td>\n",
       "      <td>20.040001</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>11.795288</td>\n",
       "      <td>11.965881</td>\n",
       "      <td>11.770918</td>\n",
       "      <td>9.344018</td>\n",
       "      <td>92942347</td>\n",
       "      <td>XLF</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.521512</td>\n",
       "      <td>25.289541</td>\n",
       "      <td>100.0</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>100.0</td>\n",
       "      <td>9.344018</td>\n",
       "      <td>9.344018</td>\n",
       "      <td>20.040001</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>28.090000</td>\n",
       "      <td>28.320000</td>\n",
       "      <td>27.959999</td>\n",
       "      <td>21.793045</td>\n",
       "      <td>7471500</td>\n",
       "      <td>XLI</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.521512</td>\n",
       "      <td>25.289541</td>\n",
       "      <td>100.0</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>100.0</td>\n",
       "      <td>21.793045</td>\n",
       "      <td>21.793045</td>\n",
       "      <td>20.040001</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>23.139999</td>\n",
       "      <td>23.290001</td>\n",
       "      <td>23.100000</td>\n",
       "      <td>19.110191</td>\n",
       "      <td>8449400</td>\n",
       "      <td>XLK</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.521512</td>\n",
       "      <td>25.289541</td>\n",
       "      <td>100.0</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>100.0</td>\n",
       "      <td>19.110191</td>\n",
       "      <td>19.110191</td>\n",
       "      <td>20.040001</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date       open       high        low      close    volume  tic  day  \\\n",
       "0  2010-01-04  33.580002  34.020000  33.450001  25.364519   7567500  XLB    0   \n",
       "1  2010-01-04  57.919998  58.810001  57.790001  37.747696  16928400  XLE    0   \n",
       "2  2010-01-04  11.795288  11.965881  11.770918   9.344018  92942347  XLF    0   \n",
       "3  2010-01-04  28.090000  28.320000  27.959999  21.793045   7471500  XLI    0   \n",
       "4  2010-01-04  23.139999  23.290001  23.100000  19.110191   8449400  XLK    0   \n",
       "\n",
       "   macd    boll_ub    boll_lb  rsi_30     cci_30  dx_30  close_30_sma  \\\n",
       "0   0.0  25.521512  25.289541   100.0  66.666667  100.0     25.364519   \n",
       "1   0.0  25.521512  25.289541   100.0  66.666667  100.0     37.747696   \n",
       "2   0.0  25.521512  25.289541   100.0  66.666667  100.0      9.344018   \n",
       "3   0.0  25.521512  25.289541   100.0  66.666667  100.0     21.793045   \n",
       "4   0.0  25.521512  25.289541   100.0  66.666667  100.0     19.110191   \n",
       "\n",
       "   close_60_sma        vix  turbulence  \n",
       "0     25.364519  20.040001         0.0  \n",
       "1     37.747696  20.040001         0.0  \n",
       "2      9.344018  20.040001         0.0  \n",
       "3     21.793045  20.040001         0.0  \n",
       "4     19.110191  20.040001         0.0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed.sort_values(['date','tic'],ignore_index=True).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>tic</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>day</th>\n",
       "      <th>macd</th>\n",
       "      <th>boll_ub</th>\n",
       "      <th>boll_lb</th>\n",
       "      <th>rsi_30</th>\n",
       "      <th>cci_30</th>\n",
       "      <th>dx_30</th>\n",
       "      <th>close_30_sma</th>\n",
       "      <th>close_60_sma</th>\n",
       "      <th>vix</th>\n",
       "      <th>turbulence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>XLB</td>\n",
       "      <td>33.580002</td>\n",
       "      <td>34.020000</td>\n",
       "      <td>33.450001</td>\n",
       "      <td>25.364519</td>\n",
       "      <td>7567500.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>25.521512</td>\n",
       "      <td>25.289541</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>25.364519</td>\n",
       "      <td>25.364519</td>\n",
       "      <td>20.040001</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>XLE</td>\n",
       "      <td>57.919998</td>\n",
       "      <td>58.810001</td>\n",
       "      <td>57.790001</td>\n",
       "      <td>37.747696</td>\n",
       "      <td>16928400.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>25.521512</td>\n",
       "      <td>25.289541</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>37.747696</td>\n",
       "      <td>37.747696</td>\n",
       "      <td>20.040001</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>XLF</td>\n",
       "      <td>11.795288</td>\n",
       "      <td>11.965881</td>\n",
       "      <td>11.770918</td>\n",
       "      <td>9.344018</td>\n",
       "      <td>92942347.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>25.521512</td>\n",
       "      <td>25.289541</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>9.344018</td>\n",
       "      <td>9.344018</td>\n",
       "      <td>20.040001</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>XLI</td>\n",
       "      <td>28.090000</td>\n",
       "      <td>28.320000</td>\n",
       "      <td>27.959999</td>\n",
       "      <td>21.793045</td>\n",
       "      <td>7471500.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>25.521512</td>\n",
       "      <td>25.289541</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>21.793045</td>\n",
       "      <td>21.793045</td>\n",
       "      <td>20.040001</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>XLK</td>\n",
       "      <td>23.139999</td>\n",
       "      <td>23.290001</td>\n",
       "      <td>23.100000</td>\n",
       "      <td>19.110191</td>\n",
       "      <td>8449400.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>25.521512</td>\n",
       "      <td>25.289541</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>19.110191</td>\n",
       "      <td>19.110191</td>\n",
       "      <td>20.040001</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43807</th>\n",
       "      <td>2023-05-03</td>\n",
       "      <td>XLK</td>\n",
       "      <td>149.690002</td>\n",
       "      <td>150.660004</td>\n",
       "      <td>148.179993</td>\n",
       "      <td>147.940125</td>\n",
       "      <td>5815100.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.101265</td>\n",
       "      <td>151.344400</td>\n",
       "      <td>144.908068</td>\n",
       "      <td>55.182961</td>\n",
       "      <td>46.442993</td>\n",
       "      <td>6.225843</td>\n",
       "      <td>147.699630</td>\n",
       "      <td>143.517544</td>\n",
       "      <td>18.340000</td>\n",
       "      <td>4.219994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43808</th>\n",
       "      <td>2023-05-03</td>\n",
       "      <td>XLP</td>\n",
       "      <td>77.059998</td>\n",
       "      <td>77.379997</td>\n",
       "      <td>76.639999</td>\n",
       "      <td>76.119987</td>\n",
       "      <td>11656600.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.875702</td>\n",
       "      <td>77.189108</td>\n",
       "      <td>74.079747</td>\n",
       "      <td>58.998054</td>\n",
       "      <td>93.547472</td>\n",
       "      <td>22.508083</td>\n",
       "      <td>74.795711</td>\n",
       "      <td>73.269686</td>\n",
       "      <td>18.340000</td>\n",
       "      <td>4.219994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43809</th>\n",
       "      <td>2023-05-03</td>\n",
       "      <td>XLU</td>\n",
       "      <td>68.639999</td>\n",
       "      <td>69.050003</td>\n",
       "      <td>67.989998</td>\n",
       "      <td>67.526855</td>\n",
       "      <td>11857200.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.319781</td>\n",
       "      <td>69.807814</td>\n",
       "      <td>67.590043</td>\n",
       "      <td>50.161291</td>\n",
       "      <td>8.126302</td>\n",
       "      <td>8.132944</td>\n",
       "      <td>67.692193</td>\n",
       "      <td>66.656867</td>\n",
       "      <td>18.340000</td>\n",
       "      <td>4.219994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43810</th>\n",
       "      <td>2023-05-03</td>\n",
       "      <td>XLV</td>\n",
       "      <td>134.259995</td>\n",
       "      <td>134.899994</td>\n",
       "      <td>133.270004</td>\n",
       "      <td>132.912888</td>\n",
       "      <td>8961500.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.915705</td>\n",
       "      <td>134.927106</td>\n",
       "      <td>131.520086</td>\n",
       "      <td>53.906948</td>\n",
       "      <td>54.096702</td>\n",
       "      <td>6.687990</td>\n",
       "      <td>131.318106</td>\n",
       "      <td>129.596837</td>\n",
       "      <td>18.340000</td>\n",
       "      <td>4.219994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43811</th>\n",
       "      <td>2023-05-03</td>\n",
       "      <td>XLY</td>\n",
       "      <td>145.940002</td>\n",
       "      <td>147.889999</td>\n",
       "      <td>145.279999</td>\n",
       "      <td>145.098145</td>\n",
       "      <td>4256000.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.418781</td>\n",
       "      <td>148.824533</td>\n",
       "      <td>143.067334</td>\n",
       "      <td>50.679820</td>\n",
       "      <td>23.335247</td>\n",
       "      <td>0.925005</td>\n",
       "      <td>145.369260</td>\n",
       "      <td>145.068299</td>\n",
       "      <td>18.340000</td>\n",
       "      <td>4.219994</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30204 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             date  tic        open        high         low       close  \\\n",
       "0      2010-01-04  XLB   33.580002   34.020000   33.450001   25.364519   \n",
       "1      2010-01-04  XLE   57.919998   58.810001   57.790001   37.747696   \n",
       "2      2010-01-04  XLF   11.795288   11.965881   11.770918    9.344018   \n",
       "3      2010-01-04  XLI   28.090000   28.320000   27.959999   21.793045   \n",
       "4      2010-01-04  XLK   23.139999   23.290001   23.100000   19.110191   \n",
       "...           ...  ...         ...         ...         ...         ...   \n",
       "43807  2023-05-03  XLK  149.690002  150.660004  148.179993  147.940125   \n",
       "43808  2023-05-03  XLP   77.059998   77.379997   76.639999   76.119987   \n",
       "43809  2023-05-03  XLU   68.639999   69.050003   67.989998   67.526855   \n",
       "43810  2023-05-03  XLV  134.259995  134.899994  133.270004  132.912888   \n",
       "43811  2023-05-03  XLY  145.940002  147.889999  145.279999  145.098145   \n",
       "\n",
       "           volume  day      macd     boll_ub     boll_lb      rsi_30  \\\n",
       "0       7567500.0  0.0  0.000000   25.521512   25.289541  100.000000   \n",
       "1      16928400.0  0.0  0.000000   25.521512   25.289541  100.000000   \n",
       "2      92942347.0  0.0  0.000000   25.521512   25.289541  100.000000   \n",
       "3       7471500.0  0.0  0.000000   25.521512   25.289541  100.000000   \n",
       "4       8449400.0  0.0  0.000000   25.521512   25.289541  100.000000   \n",
       "...           ...  ...       ...         ...         ...         ...   \n",
       "43807   5815100.0  2.0  1.101265  151.344400  144.908068   55.182961   \n",
       "43808  11656600.0  2.0  0.875702   77.189108   74.079747   58.998054   \n",
       "43809  11857200.0  2.0  0.319781   69.807814   67.590043   50.161291   \n",
       "43810   8961500.0  2.0  0.915705  134.927106  131.520086   53.906948   \n",
       "43811   4256000.0  2.0  0.418781  148.824533  143.067334   50.679820   \n",
       "\n",
       "          cci_30       dx_30  close_30_sma  close_60_sma        vix  \\\n",
       "0      66.666667  100.000000     25.364519     25.364519  20.040001   \n",
       "1      66.666667  100.000000     37.747696     37.747696  20.040001   \n",
       "2      66.666667  100.000000      9.344018      9.344018  20.040001   \n",
       "3      66.666667  100.000000     21.793045     21.793045  20.040001   \n",
       "4      66.666667  100.000000     19.110191     19.110191  20.040001   \n",
       "...          ...         ...           ...           ...        ...   \n",
       "43807  46.442993    6.225843    147.699630    143.517544  18.340000   \n",
       "43808  93.547472   22.508083     74.795711     73.269686  18.340000   \n",
       "43809   8.126302    8.132944     67.692193     66.656867  18.340000   \n",
       "43810  54.096702    6.687990    131.318106    129.596837  18.340000   \n",
       "43811  23.335247    0.925005    145.369260    145.068299  18.340000   \n",
       "\n",
       "       turbulence  \n",
       "0        0.000000  \n",
       "1        0.000000  \n",
       "2        0.000000  \n",
       "3        0.000000  \n",
       "4        0.000000  \n",
       "...           ...  \n",
       "43807    4.219994  \n",
       "43808    4.219994  \n",
       "43809    4.219994  \n",
       "43810    4.219994  \n",
       "43811    4.219994  \n",
       "\n",
       "[30204 rows x 18 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_ticker = processed[\"tic\"].unique().tolist() #ticker 리스트 불러오기\n",
    "list_date = list(pd.date_range(processed['date'].min(),processed['date'].max()).astype(str)) #전체 데이터 날짜 날짜 리스트\n",
    "combination = list(itertools.product(list_date,list_ticker)) #date, ticker 의 combination\n",
    "\n",
    "processed_full = pd.DataFrame(combination,columns=[\"date\",\"tic\"]).merge(processed,on=[\"date\",\"tic\"],how=\"left\") #date, tic combination 기준으로 우측에 해당되는 정보들 정리\n",
    "processed_full = processed_full[processed_full['date'].isin(processed['date'])]\n",
    "processed_full = processed_full.sort_values(['date','tic'])\n",
    "\n",
    "processed_full = processed_full.fillna(0)\n",
    "processed_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction 업로드시 확인 필요부분\n",
    "prediction_5 = pd.read_csv('prediction_5.csv').set_index('Unnamed: 0')\n",
    "prediction_10 = pd.read_csv('prediction_10.csv').set_index('Unnamed: 0')\n",
    "prediction_30 = pd.read_csv('prediction_30.csv').set_index('Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30204\n"
     ]
    }
   ],
   "source": [
    "#prediction 업로드시 확인 필요부분\n",
    "\n",
    "\n",
    "prediction_5_list = []\n",
    "for i in range(len(prediction_5)):\n",
    "    for j in range(len(tickers)):\n",
    "        prediction_5_list.append(prediction_5.iloc[i,j])\n",
    "        \n",
    "for i in range((len(processed_full)-len(prediction_5_list))//len(tickers)):\n",
    "    for j in range(len(tickers)):\n",
    "        prediction_5_list.append(prediction_5.iloc[-1,j])\n",
    "\n",
    "prediction_10_list = []\n",
    "for i in range(len(prediction_10)):\n",
    "    for j in range(len(tickers)):\n",
    "        prediction_10_list.append(prediction_10.iloc[i,j])\n",
    "\n",
    "for i in range((len(processed_full)-len(prediction_10_list))//len(tickers)):\n",
    "    for j in range(len(tickers)):\n",
    "        prediction_10_list.append(prediction_10.iloc[-1,j])\n",
    "    \n",
    "prediction_30_list = []\n",
    "for i in range(len(prediction_30)):\n",
    "    for j in range(len(tickers)):\n",
    "        prediction_30_list.append(prediction_30.iloc[i,j])\n",
    "for i in range((len(processed_full)-len(prediction_30_list))//len(tickers)):\n",
    "    for j in range(len(tickers)):\n",
    "        prediction_30_list.append(prediction_30.iloc[-1,j])\n",
    "        \n",
    "print(len(prediction_5_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>tic</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>day</th>\n",
       "      <th>macd</th>\n",
       "      <th>boll_ub</th>\n",
       "      <th>...</th>\n",
       "      <th>rsi_30</th>\n",
       "      <th>cci_30</th>\n",
       "      <th>dx_30</th>\n",
       "      <th>close_30_sma</th>\n",
       "      <th>close_60_sma</th>\n",
       "      <th>vix</th>\n",
       "      <th>turbulence</th>\n",
       "      <th>prediction_5</th>\n",
       "      <th>prediction_10</th>\n",
       "      <th>prediction_30</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>XLB</td>\n",
       "      <td>33.580002</td>\n",
       "      <td>34.020000</td>\n",
       "      <td>33.450001</td>\n",
       "      <td>25.364519</td>\n",
       "      <td>7567500.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>25.521512</td>\n",
       "      <td>...</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>25.364519</td>\n",
       "      <td>25.364519</td>\n",
       "      <td>20.040001</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>25.001299</td>\n",
       "      <td>25.008287</td>\n",
       "      <td>28.824316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>XLE</td>\n",
       "      <td>57.919998</td>\n",
       "      <td>58.810001</td>\n",
       "      <td>57.790001</td>\n",
       "      <td>37.747696</td>\n",
       "      <td>16928400.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>25.521512</td>\n",
       "      <td>...</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>37.747696</td>\n",
       "      <td>37.747696</td>\n",
       "      <td>20.040001</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>43.503914</td>\n",
       "      <td>40.740055</td>\n",
       "      <td>41.319206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>XLF</td>\n",
       "      <td>11.795288</td>\n",
       "      <td>11.965881</td>\n",
       "      <td>11.770918</td>\n",
       "      <td>9.344018</td>\n",
       "      <td>92942347.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>25.521512</td>\n",
       "      <td>...</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>9.344018</td>\n",
       "      <td>9.344018</td>\n",
       "      <td>20.040001</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.368776</td>\n",
       "      <td>8.849185</td>\n",
       "      <td>10.953069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>XLI</td>\n",
       "      <td>28.090000</td>\n",
       "      <td>28.320000</td>\n",
       "      <td>27.959999</td>\n",
       "      <td>21.793045</td>\n",
       "      <td>7471500.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>25.521512</td>\n",
       "      <td>...</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>21.793045</td>\n",
       "      <td>21.793045</td>\n",
       "      <td>20.040001</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>25.687847</td>\n",
       "      <td>26.830729</td>\n",
       "      <td>29.474950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>XLK</td>\n",
       "      <td>23.139999</td>\n",
       "      <td>23.290001</td>\n",
       "      <td>23.100000</td>\n",
       "      <td>19.110191</td>\n",
       "      <td>8449400.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>25.521512</td>\n",
       "      <td>...</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>19.110191</td>\n",
       "      <td>19.110191</td>\n",
       "      <td>20.040001</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>23.967155</td>\n",
       "      <td>25.295761</td>\n",
       "      <td>25.559303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43807</th>\n",
       "      <td>2023-05-03</td>\n",
       "      <td>XLK</td>\n",
       "      <td>149.690002</td>\n",
       "      <td>150.660004</td>\n",
       "      <td>148.179993</td>\n",
       "      <td>147.940125</td>\n",
       "      <td>5815100.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.101265</td>\n",
       "      <td>151.344400</td>\n",
       "      <td>...</td>\n",
       "      <td>55.182961</td>\n",
       "      <td>46.442993</td>\n",
       "      <td>6.225843</td>\n",
       "      <td>147.699630</td>\n",
       "      <td>143.517544</td>\n",
       "      <td>18.340000</td>\n",
       "      <td>4.219994</td>\n",
       "      <td>159.765030</td>\n",
       "      <td>199.236300</td>\n",
       "      <td>229.869460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43808</th>\n",
       "      <td>2023-05-03</td>\n",
       "      <td>XLP</td>\n",
       "      <td>77.059998</td>\n",
       "      <td>77.379997</td>\n",
       "      <td>76.639999</td>\n",
       "      <td>76.119987</td>\n",
       "      <td>11656600.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.875702</td>\n",
       "      <td>77.189108</td>\n",
       "      <td>...</td>\n",
       "      <td>58.998054</td>\n",
       "      <td>93.547472</td>\n",
       "      <td>22.508083</td>\n",
       "      <td>74.795711</td>\n",
       "      <td>73.269686</td>\n",
       "      <td>18.340000</td>\n",
       "      <td>4.219994</td>\n",
       "      <td>77.748860</td>\n",
       "      <td>81.883970</td>\n",
       "      <td>84.795660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43809</th>\n",
       "      <td>2023-05-03</td>\n",
       "      <td>XLU</td>\n",
       "      <td>68.639999</td>\n",
       "      <td>69.050003</td>\n",
       "      <td>67.989998</td>\n",
       "      <td>67.526855</td>\n",
       "      <td>11857200.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.319781</td>\n",
       "      <td>69.807814</td>\n",
       "      <td>...</td>\n",
       "      <td>50.161291</td>\n",
       "      <td>8.126302</td>\n",
       "      <td>8.132944</td>\n",
       "      <td>67.692193</td>\n",
       "      <td>66.656867</td>\n",
       "      <td>18.340000</td>\n",
       "      <td>4.219994</td>\n",
       "      <td>72.211586</td>\n",
       "      <td>72.498500</td>\n",
       "      <td>82.259070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43810</th>\n",
       "      <td>2023-05-03</td>\n",
       "      <td>XLV</td>\n",
       "      <td>134.259995</td>\n",
       "      <td>134.899994</td>\n",
       "      <td>133.270004</td>\n",
       "      <td>132.912888</td>\n",
       "      <td>8961500.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.915705</td>\n",
       "      <td>134.927106</td>\n",
       "      <td>...</td>\n",
       "      <td>53.906948</td>\n",
       "      <td>54.096702</td>\n",
       "      <td>6.687990</td>\n",
       "      <td>131.318106</td>\n",
       "      <td>129.596837</td>\n",
       "      <td>18.340000</td>\n",
       "      <td>4.219994</td>\n",
       "      <td>131.804820</td>\n",
       "      <td>142.887970</td>\n",
       "      <td>185.065430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43811</th>\n",
       "      <td>2023-05-03</td>\n",
       "      <td>XLY</td>\n",
       "      <td>145.940002</td>\n",
       "      <td>147.889999</td>\n",
       "      <td>145.279999</td>\n",
       "      <td>145.098145</td>\n",
       "      <td>4256000.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.418781</td>\n",
       "      <td>148.824533</td>\n",
       "      <td>...</td>\n",
       "      <td>50.679820</td>\n",
       "      <td>23.335247</td>\n",
       "      <td>0.925005</td>\n",
       "      <td>145.369260</td>\n",
       "      <td>145.068299</td>\n",
       "      <td>18.340000</td>\n",
       "      <td>4.219994</td>\n",
       "      <td>158.683320</td>\n",
       "      <td>165.199040</td>\n",
       "      <td>216.651820</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30204 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             date  tic        open        high         low       close  \\\n",
       "0      2010-01-04  XLB   33.580002   34.020000   33.450001   25.364519   \n",
       "1      2010-01-04  XLE   57.919998   58.810001   57.790001   37.747696   \n",
       "2      2010-01-04  XLF   11.795288   11.965881   11.770918    9.344018   \n",
       "3      2010-01-04  XLI   28.090000   28.320000   27.959999   21.793045   \n",
       "4      2010-01-04  XLK   23.139999   23.290001   23.100000   19.110191   \n",
       "...           ...  ...         ...         ...         ...         ...   \n",
       "43807  2023-05-03  XLK  149.690002  150.660004  148.179993  147.940125   \n",
       "43808  2023-05-03  XLP   77.059998   77.379997   76.639999   76.119987   \n",
       "43809  2023-05-03  XLU   68.639999   69.050003   67.989998   67.526855   \n",
       "43810  2023-05-03  XLV  134.259995  134.899994  133.270004  132.912888   \n",
       "43811  2023-05-03  XLY  145.940002  147.889999  145.279999  145.098145   \n",
       "\n",
       "           volume  day      macd     boll_ub  ...      rsi_30     cci_30  \\\n",
       "0       7567500.0  0.0  0.000000   25.521512  ...  100.000000  66.666667   \n",
       "1      16928400.0  0.0  0.000000   25.521512  ...  100.000000  66.666667   \n",
       "2      92942347.0  0.0  0.000000   25.521512  ...  100.000000  66.666667   \n",
       "3       7471500.0  0.0  0.000000   25.521512  ...  100.000000  66.666667   \n",
       "4       8449400.0  0.0  0.000000   25.521512  ...  100.000000  66.666667   \n",
       "...           ...  ...       ...         ...  ...         ...        ...   \n",
       "43807   5815100.0  2.0  1.101265  151.344400  ...   55.182961  46.442993   \n",
       "43808  11656600.0  2.0  0.875702   77.189108  ...   58.998054  93.547472   \n",
       "43809  11857200.0  2.0  0.319781   69.807814  ...   50.161291   8.126302   \n",
       "43810   8961500.0  2.0  0.915705  134.927106  ...   53.906948  54.096702   \n",
       "43811   4256000.0  2.0  0.418781  148.824533  ...   50.679820  23.335247   \n",
       "\n",
       "            dx_30  close_30_sma  close_60_sma        vix  turbulence  \\\n",
       "0      100.000000     25.364519     25.364519  20.040001    0.000000   \n",
       "1      100.000000     37.747696     37.747696  20.040001    0.000000   \n",
       "2      100.000000      9.344018      9.344018  20.040001    0.000000   \n",
       "3      100.000000     21.793045     21.793045  20.040001    0.000000   \n",
       "4      100.000000     19.110191     19.110191  20.040001    0.000000   \n",
       "...           ...           ...           ...        ...         ...   \n",
       "43807    6.225843    147.699630    143.517544  18.340000    4.219994   \n",
       "43808   22.508083     74.795711     73.269686  18.340000    4.219994   \n",
       "43809    8.132944     67.692193     66.656867  18.340000    4.219994   \n",
       "43810    6.687990    131.318106    129.596837  18.340000    4.219994   \n",
       "43811    0.925005    145.369260    145.068299  18.340000    4.219994   \n",
       "\n",
       "       prediction_5  prediction_10  prediction_30  \n",
       "0         25.001299      25.008287      28.824316  \n",
       "1         43.503914      40.740055      41.319206  \n",
       "2          9.368776       8.849185      10.953069  \n",
       "3         25.687847      26.830729      29.474950  \n",
       "4         23.967155      25.295761      25.559303  \n",
       "...             ...            ...            ...  \n",
       "43807    159.765030     199.236300     229.869460  \n",
       "43808     77.748860      81.883970      84.795660  \n",
       "43809     72.211586      72.498500      82.259070  \n",
       "43810    131.804820     142.887970     185.065430  \n",
       "43811    158.683320     165.199040     216.651820  \n",
       "\n",
       "[30204 rows x 21 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#prediction 업로드시 확인 필요부분\n",
    "\n",
    "processed_full['prediction_5']=prediction_5_list\n",
    "processed_full['prediction_10']=prediction_10_list\n",
    "processed_full['prediction_30']=prediction_30_list\n",
    "\n",
    "processed_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction 업로드시 확인 필요부분\n",
    "\n",
    "\n",
    "INDICATORS += ['prediction_5','prediction_10','prediction_30']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26613\n",
      "3591\n"
     ]
    }
   ],
   "source": [
    "train = data_split(processed_full, TRAIN_START_DATE,TRAIN_END_DATE)\n",
    "trade = data_split(processed_full, TRADE_START_DATE,TRADE_END_DATE)\n",
    "print(len(train))\n",
    "print(len(trade))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stock Dimension: 9, State Space: 118\n"
     ]
    }
   ],
   "source": [
    "stock_dimension = len(train.tic.unique())\n",
    "state_space = 1 + 2*stock_dimension + len(INDICATORS)*stock_dimension #현재 상태를 나타내는 state를 정의. 1+ ticker 개수 x2 + 인디케이터 개수 x ticker개수\n",
    "print(f\"Stock Dimension: {stock_dimension}, State Space: {state_space}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_steps': 5, 'ent_coef': 0.01, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to results/a2c\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 114        |\n",
      "|    iterations         | 100        |\n",
      "|    time_elapsed       | 4          |\n",
      "|    total_timesteps    | 500        |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -12.9      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 99         |\n",
      "|    policy_loss        | -6.47      |\n",
      "|    reward             | 0.24104658 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 0.375      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 114       |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 8         |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -12.9     |\n",
      "|    explained_variance | -0.0633   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | -9.69     |\n",
      "|    reward             | 0.6381139 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 2.88      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 115        |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 12         |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -12.9      |\n",
      "|    explained_variance | 0.0163     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | -2.97      |\n",
      "|    reward             | -2.4972622 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 2.66       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 113       |\n",
      "|    iterations         | 400       |\n",
      "|    time_elapsed       | 17        |\n",
      "|    total_timesteps    | 2000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -12.9     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 399       |\n",
      "|    policy_loss        | -25.7     |\n",
      "|    reward             | 0.9031801 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 5.98      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 109        |\n",
      "|    iterations         | 500        |\n",
      "|    time_elapsed       | 22         |\n",
      "|    total_timesteps    | 2500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -12.9      |\n",
      "|    explained_variance | 0.11       |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 499        |\n",
      "|    policy_loss        | -1.5       |\n",
      "|    reward             | -1.5519694 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 1.77       |\n",
      "--------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 109           |\n",
      "|    iterations         | 600           |\n",
      "|    time_elapsed       | 27            |\n",
      "|    total_timesteps    | 3000          |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -12.9         |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 599           |\n",
      "|    policy_loss        | -3.8          |\n",
      "|    reward             | -0.0022997877 |\n",
      "|    std                | 1.02          |\n",
      "|    value_loss         | 0.0766        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 109           |\n",
      "|    iterations         | 700           |\n",
      "|    time_elapsed       | 32            |\n",
      "|    total_timesteps    | 3500          |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -13           |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 699           |\n",
      "|    policy_loss        | -3.63         |\n",
      "|    reward             | -0.0040774476 |\n",
      "|    std                | 1.02          |\n",
      "|    value_loss         | 0.12          |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 109         |\n",
      "|    iterations         | 800         |\n",
      "|    time_elapsed       | 36          |\n",
      "|    total_timesteps    | 4000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13         |\n",
      "|    explained_variance | -0.583      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 799         |\n",
      "|    policy_loss        | 5.12        |\n",
      "|    reward             | -0.21127297 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 1.04        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 109       |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 41        |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -12.9     |\n",
      "|    explained_variance | 0.111     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | -8.12     |\n",
      "|    reward             | 1.6944656 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 1.23      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 105         |\n",
      "|    iterations         | 1000        |\n",
      "|    time_elapsed       | 47          |\n",
      "|    total_timesteps    | 5000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13         |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 999         |\n",
      "|    policy_loss        | -88.3       |\n",
      "|    reward             | -0.31241164 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 62.3        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 100        |\n",
      "|    iterations         | 1100       |\n",
      "|    time_elapsed       | 54         |\n",
      "|    total_timesteps    | 5500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -12.9      |\n",
      "|    explained_variance | 0.0801     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1099       |\n",
      "|    policy_loss        | -4.45      |\n",
      "|    reward             | 0.37505782 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 0.837      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 96          |\n",
      "|    iterations         | 1200        |\n",
      "|    time_elapsed       | 62          |\n",
      "|    total_timesteps    | 6000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -12.9       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1199        |\n",
      "|    policy_loss        | -1.54       |\n",
      "|    reward             | -0.23833899 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 0.063       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 93         |\n",
      "|    iterations         | 1300       |\n",
      "|    time_elapsed       | 69         |\n",
      "|    total_timesteps    | 6500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -12.9      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1299       |\n",
      "|    policy_loss        | -3.6       |\n",
      "|    reward             | 0.47471365 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 0.286      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 90          |\n",
      "|    iterations         | 1400        |\n",
      "|    time_elapsed       | 77          |\n",
      "|    total_timesteps    | 7000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -12.9       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1399        |\n",
      "|    policy_loss        | 12.8        |\n",
      "|    reward             | -0.25508577 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 1.93        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 88        |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 84        |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -12.9     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | 2.64      |\n",
      "|    reward             | 1.4140931 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 0.507     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 84         |\n",
      "|    iterations         | 1600       |\n",
      "|    time_elapsed       | 94         |\n",
      "|    total_timesteps    | 8000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -12.9      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1599       |\n",
      "|    policy_loss        | -26.7      |\n",
      "|    reward             | -1.0100919 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 5.32       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 83       |\n",
      "|    iterations         | 1700     |\n",
      "|    time_elapsed       | 102      |\n",
      "|    total_timesteps    | 8500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -12.9    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1699     |\n",
      "|    policy_loss        | 97.9     |\n",
      "|    reward             | 8.825979 |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 195      |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 82         |\n",
      "|    iterations         | 1800       |\n",
      "|    time_elapsed       | 109        |\n",
      "|    total_timesteps    | 9000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -12.9      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1799       |\n",
      "|    policy_loss        | -7.34      |\n",
      "|    reward             | 0.16356294 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 0.404      |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 81       |\n",
      "|    iterations         | 1900     |\n",
      "|    time_elapsed       | 116      |\n",
      "|    total_timesteps    | 9500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -12.9    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1899     |\n",
      "|    policy_loss        | -20.9    |\n",
      "|    reward             | 0.720791 |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 4.57     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 81        |\n",
      "|    iterations         | 2000      |\n",
      "|    time_elapsed       | 122       |\n",
      "|    total_timesteps    | 10000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -12.9     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1999      |\n",
      "|    policy_loss        | 7.7       |\n",
      "|    reward             | 1.3033842 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 4         |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 81         |\n",
      "|    iterations         | 2100       |\n",
      "|    time_elapsed       | 129        |\n",
      "|    total_timesteps    | 10500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -12.9      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2099       |\n",
      "|    policy_loss        | 6.47       |\n",
      "|    reward             | 0.56090313 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 0.447      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 80        |\n",
      "|    iterations         | 2200      |\n",
      "|    time_elapsed       | 136       |\n",
      "|    total_timesteps    | 11000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -12.9     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2199      |\n",
      "|    policy_loss        | 14.2      |\n",
      "|    reward             | 2.096011  |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 4.78      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 79        |\n",
      "|    iterations         | 2300      |\n",
      "|    time_elapsed       | 144       |\n",
      "|    total_timesteps    | 11500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13       |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2299      |\n",
      "|    policy_loss        | 113       |\n",
      "|    reward             | 1.8163033 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 75.3      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 78         |\n",
      "|    iterations         | 2400       |\n",
      "|    time_elapsed       | 151        |\n",
      "|    total_timesteps    | 12000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13        |\n",
      "|    explained_variance | 1.79e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2399       |\n",
      "|    policy_loss        | 9.06       |\n",
      "|    reward             | 0.35844046 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 0.537      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 78          |\n",
      "|    iterations         | 2500        |\n",
      "|    time_elapsed       | 159         |\n",
      "|    total_timesteps    | 12500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13         |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 2499        |\n",
      "|    policy_loss        | -14.7       |\n",
      "|    reward             | -0.13262065 |\n",
      "|    std                | 1.03        |\n",
      "|    value_loss         | 2.26        |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 77       |\n",
      "|    iterations         | 2600     |\n",
      "|    time_elapsed       | 167      |\n",
      "|    total_timesteps    | 13000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13      |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2599     |\n",
      "|    policy_loss        | 4.49     |\n",
      "|    reward             | 1.003378 |\n",
      "|    std                | 1.03     |\n",
      "|    value_loss         | 0.949    |\n",
      "------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 76           |\n",
      "|    iterations         | 2700         |\n",
      "|    time_elapsed       | 175          |\n",
      "|    total_timesteps    | 13500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -13          |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 2699         |\n",
      "|    policy_loss        | -8.77        |\n",
      "|    reward             | -0.076787494 |\n",
      "|    std                | 1.03         |\n",
      "|    value_loss         | 5.36         |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 76          |\n",
      "|    iterations         | 2800        |\n",
      "|    time_elapsed       | 183         |\n",
      "|    total_timesteps    | 14000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13         |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 2799        |\n",
      "|    policy_loss        | -9.15       |\n",
      "|    reward             | -0.78111315 |\n",
      "|    std                | 1.03        |\n",
      "|    value_loss         | 1           |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 2900      |\n",
      "|    time_elapsed       | 190       |\n",
      "|    total_timesteps    | 14500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13       |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2899      |\n",
      "|    policy_loss        | 44.8      |\n",
      "|    reward             | -2.328488 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 19.4      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 3000       |\n",
      "|    time_elapsed       | 198        |\n",
      "|    total_timesteps    | 15000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13        |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2999       |\n",
      "|    policy_loss        | 6.42       |\n",
      "|    reward             | -0.6604264 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 0.303      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 3100       |\n",
      "|    time_elapsed       | 206        |\n",
      "|    total_timesteps    | 15500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13        |\n",
      "|    explained_variance | 0.0234     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3099       |\n",
      "|    policy_loss        | -16.4      |\n",
      "|    reward             | -1.7227465 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 3.88       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 74       |\n",
      "|    iterations         | 3200     |\n",
      "|    time_elapsed       | 213      |\n",
      "|    total_timesteps    | 16000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13.1    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3199     |\n",
      "|    policy_loss        | 28.8     |\n",
      "|    reward             | 3.930173 |\n",
      "|    std                | 1.04     |\n",
      "|    value_loss         | 8.59     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 74        |\n",
      "|    iterations         | 3300      |\n",
      "|    time_elapsed       | 220       |\n",
      "|    total_timesteps    | 16500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.1     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3299      |\n",
      "|    policy_loss        | 37.1      |\n",
      "|    reward             | 1.2896336 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 11.5      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 74        |\n",
      "|    iterations         | 3400      |\n",
      "|    time_elapsed       | 228       |\n",
      "|    total_timesteps    | 17000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3399      |\n",
      "|    policy_loss        | -54.2     |\n",
      "|    reward             | 3.8640082 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 22.1      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 74          |\n",
      "|    iterations         | 3500        |\n",
      "|    time_elapsed       | 235         |\n",
      "|    total_timesteps    | 17500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.1       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 3499        |\n",
      "|    policy_loss        | 139         |\n",
      "|    reward             | -0.19661114 |\n",
      "|    std                | 1.04        |\n",
      "|    value_loss         | 122         |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 73         |\n",
      "|    iterations         | 3600       |\n",
      "|    time_elapsed       | 243        |\n",
      "|    total_timesteps    | 18000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.2      |\n",
      "|    explained_variance | -4.26      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3599       |\n",
      "|    policy_loss        | 16.2       |\n",
      "|    reward             | 0.27478966 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 1.67       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 73          |\n",
      "|    iterations         | 3700        |\n",
      "|    time_elapsed       | 250         |\n",
      "|    total_timesteps    | 18500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.1       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 3699        |\n",
      "|    policy_loss        | -4.79       |\n",
      "|    reward             | -0.40549174 |\n",
      "|    std                | 1.04        |\n",
      "|    value_loss         | 1.61        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 73        |\n",
      "|    iterations         | 3800      |\n",
      "|    time_elapsed       | 258       |\n",
      "|    total_timesteps    | 19000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.1     |\n",
      "|    explained_variance | 1.79e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3799      |\n",
      "|    policy_loss        | 58.7      |\n",
      "|    reward             | 0.8897564 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 24        |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 73         |\n",
      "|    iterations         | 3900       |\n",
      "|    time_elapsed       | 265        |\n",
      "|    total_timesteps    | 19500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3899       |\n",
      "|    policy_loss        | 26.9       |\n",
      "|    reward             | 0.17359288 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 6.58       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 73         |\n",
      "|    iterations         | 4000       |\n",
      "|    time_elapsed       | 273        |\n",
      "|    total_timesteps    | 20000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.2      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3999       |\n",
      "|    policy_loss        | 6.55       |\n",
      "|    reward             | -1.0713556 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 0.874      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 73         |\n",
      "|    iterations         | 4100       |\n",
      "|    time_elapsed       | 280        |\n",
      "|    total_timesteps    | 20500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.1      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4099       |\n",
      "|    policy_loss        | -37.6      |\n",
      "|    reward             | -3.5866892 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 11.8       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 72         |\n",
      "|    iterations         | 4200       |\n",
      "|    time_elapsed       | 288        |\n",
      "|    total_timesteps    | 21000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4199       |\n",
      "|    policy_loss        | 9.72       |\n",
      "|    reward             | -1.5419708 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 0.908      |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 72           |\n",
      "|    iterations         | 4300         |\n",
      "|    time_elapsed       | 296          |\n",
      "|    total_timesteps    | 21500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -13.2        |\n",
      "|    explained_variance | 5.96e-08     |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 4299         |\n",
      "|    policy_loss        | 30.2         |\n",
      "|    reward             | -0.105805285 |\n",
      "|    std                | 1.05         |\n",
      "|    value_loss         | 4.98         |\n",
      "----------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 72        |\n",
      "|    iterations         | 4400      |\n",
      "|    time_elapsed       | 302       |\n",
      "|    total_timesteps    | 22000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4399      |\n",
      "|    policy_loss        | -23.9     |\n",
      "|    reward             | -5.197419 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 4.25      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 72         |\n",
      "|    iterations         | 4500       |\n",
      "|    time_elapsed       | 309        |\n",
      "|    total_timesteps    | 22500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4499       |\n",
      "|    policy_loss        | 39.1       |\n",
      "|    reward             | -1.4462633 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 11.3       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 72          |\n",
      "|    iterations         | 4600        |\n",
      "|    time_elapsed       | 317         |\n",
      "|    total_timesteps    | 23000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.2       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 4599        |\n",
      "|    policy_loss        | 22.1        |\n",
      "|    reward             | -0.34739923 |\n",
      "|    std                | 1.05        |\n",
      "|    value_loss         | 2.33        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 72        |\n",
      "|    iterations         | 4700      |\n",
      "|    time_elapsed       | 325       |\n",
      "|    total_timesteps    | 23500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.2     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4699      |\n",
      "|    policy_loss        | -20.6     |\n",
      "|    reward             | -4.907592 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 3.45      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 71          |\n",
      "|    iterations         | 4800        |\n",
      "|    time_elapsed       | 334         |\n",
      "|    total_timesteps    | 24000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.2       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 4799        |\n",
      "|    policy_loss        | -0.867      |\n",
      "|    reward             | 0.089724824 |\n",
      "|    std                | 1.05        |\n",
      "|    value_loss         | 0.22        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 71          |\n",
      "|    iterations         | 4900        |\n",
      "|    time_elapsed       | 343         |\n",
      "|    total_timesteps    | 24500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.2       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 4899        |\n",
      "|    policy_loss        | -35.6       |\n",
      "|    reward             | -0.35093746 |\n",
      "|    std                | 1.05        |\n",
      "|    value_loss         | 5.31        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 71        |\n",
      "|    iterations         | 5000      |\n",
      "|    time_elapsed       | 351       |\n",
      "|    total_timesteps    | 25000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4999      |\n",
      "|    policy_loss        | -31.4     |\n",
      "|    reward             | 0.5820694 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 7.98      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 71          |\n",
      "|    iterations         | 5100        |\n",
      "|    time_elapsed       | 359         |\n",
      "|    total_timesteps    | 25500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.2       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 5099        |\n",
      "|    policy_loss        | 8.34        |\n",
      "|    reward             | -0.24447758 |\n",
      "|    std                | 1.06        |\n",
      "|    value_loss         | 1.13        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 70         |\n",
      "|    iterations         | 5200       |\n",
      "|    time_elapsed       | 366        |\n",
      "|    total_timesteps    | 26000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5199       |\n",
      "|    policy_loss        | -22        |\n",
      "|    reward             | -1.3171439 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 6.45       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 70         |\n",
      "|    iterations         | 5300       |\n",
      "|    time_elapsed       | 374        |\n",
      "|    total_timesteps    | 26500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5299       |\n",
      "|    policy_loss        | 73.9       |\n",
      "|    reward             | -2.3359153 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 43.6       |\n",
      "--------------------------------------\n",
      "day: 2956, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3126382.18\n",
      "total_reward: 2126382.18\n",
      "total_cost: 7066.88\n",
      "total_trades: 12162\n",
      "Sharpe: 0.692\n",
      "=================================\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 70         |\n",
      "|    iterations         | 5400       |\n",
      "|    time_elapsed       | 383        |\n",
      "|    total_timesteps    | 27000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5399       |\n",
      "|    policy_loss        | -17.3      |\n",
      "|    reward             | -0.9416072 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 1.79       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 70        |\n",
      "|    iterations         | 5500      |\n",
      "|    time_elapsed       | 392       |\n",
      "|    total_timesteps    | 27500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5499      |\n",
      "|    policy_loss        | 13.2      |\n",
      "|    reward             | 1.4766513 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 1.32      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 70         |\n",
      "|    iterations         | 5600       |\n",
      "|    time_elapsed       | 399        |\n",
      "|    total_timesteps    | 28000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5599       |\n",
      "|    policy_loss        | 0.145      |\n",
      "|    reward             | -1.0715047 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 1.16       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 69          |\n",
      "|    iterations         | 5700        |\n",
      "|    time_elapsed       | 407         |\n",
      "|    total_timesteps    | 28500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.3       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 5699        |\n",
      "|    policy_loss        | -9.86       |\n",
      "|    reward             | -0.70134217 |\n",
      "|    std                | 1.06        |\n",
      "|    value_loss         | 2.48        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 69         |\n",
      "|    iterations         | 5800       |\n",
      "|    time_elapsed       | 414        |\n",
      "|    total_timesteps    | 29000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5799       |\n",
      "|    policy_loss        | 23.9       |\n",
      "|    reward             | 0.54630584 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 4.79       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 69         |\n",
      "|    iterations         | 5900       |\n",
      "|    time_elapsed       | 422        |\n",
      "|    total_timesteps    | 29500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.3      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5899       |\n",
      "|    policy_loss        | -58.7      |\n",
      "|    reward             | -2.8342135 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 17.5       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 69        |\n",
      "|    iterations         | 6000      |\n",
      "|    time_elapsed       | 429       |\n",
      "|    total_timesteps    | 30000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5999      |\n",
      "|    policy_loss        | -34.2     |\n",
      "|    reward             | 0.9336502 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 8.07      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 69        |\n",
      "|    iterations         | 6100      |\n",
      "|    time_elapsed       | 436       |\n",
      "|    total_timesteps    | 30500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.3     |\n",
      "|    explained_variance | 0.0274    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6099      |\n",
      "|    policy_loss        | 2.23      |\n",
      "|    reward             | 1.0807527 |\n",
      "|    std                | 1.07      |\n",
      "|    value_loss         | 0.136     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 69         |\n",
      "|    iterations         | 6200       |\n",
      "|    time_elapsed       | 444        |\n",
      "|    total_timesteps    | 31000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.3      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6199       |\n",
      "|    policy_loss        | -79.7      |\n",
      "|    reward             | -2.7214856 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 32.6       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 69          |\n",
      "|    iterations         | 6300        |\n",
      "|    time_elapsed       | 452         |\n",
      "|    total_timesteps    | 31500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.3       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 6299        |\n",
      "|    policy_loss        | -4.72       |\n",
      "|    reward             | 0.049280934 |\n",
      "|    std                | 1.06        |\n",
      "|    value_loss         | 0.578       |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 69        |\n",
      "|    iterations         | 6400      |\n",
      "|    time_elapsed       | 460       |\n",
      "|    total_timesteps    | 32000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.3     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6399      |\n",
      "|    policy_loss        | 25.3      |\n",
      "|    reward             | 1.2794654 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 6.51      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 69         |\n",
      "|    iterations         | 6500       |\n",
      "|    time_elapsed       | 468        |\n",
      "|    total_timesteps    | 32500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6499       |\n",
      "|    policy_loss        | 44.2       |\n",
      "|    reward             | -2.5623007 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 19.2       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 69         |\n",
      "|    iterations         | 6600       |\n",
      "|    time_elapsed       | 476        |\n",
      "|    total_timesteps    | 33000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6599       |\n",
      "|    policy_loss        | -27.3      |\n",
      "|    reward             | -1.3739542 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 3.88       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 69        |\n",
      "|    iterations         | 6700      |\n",
      "|    time_elapsed       | 485       |\n",
      "|    total_timesteps    | 33500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6699      |\n",
      "|    policy_loss        | 19.7      |\n",
      "|    reward             | 1.1148195 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 2.65      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 68          |\n",
      "|    iterations         | 6800        |\n",
      "|    time_elapsed       | 493         |\n",
      "|    total_timesteps    | 34000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.3       |\n",
      "|    explained_variance | -0.0051     |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 6799        |\n",
      "|    policy_loss        | -17.6       |\n",
      "|    reward             | -0.62201273 |\n",
      "|    std                | 1.06        |\n",
      "|    value_loss         | 3.08        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 68          |\n",
      "|    iterations         | 6900        |\n",
      "|    time_elapsed       | 502         |\n",
      "|    total_timesteps    | 34500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.3       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 6899        |\n",
      "|    policy_loss        | -17.5       |\n",
      "|    reward             | 0.008763997 |\n",
      "|    std                | 1.06        |\n",
      "|    value_loss         | 1.95        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 68        |\n",
      "|    iterations         | 7000      |\n",
      "|    time_elapsed       | 511       |\n",
      "|    total_timesteps    | 35000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6999      |\n",
      "|    policy_loss        | 29.4      |\n",
      "|    reward             | 1.7379477 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 6.45      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 68          |\n",
      "|    iterations         | 7100        |\n",
      "|    time_elapsed       | 520         |\n",
      "|    total_timesteps    | 35500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.2       |\n",
      "|    explained_variance | 1.19e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 7099        |\n",
      "|    policy_loss        | -0.428      |\n",
      "|    reward             | 0.007987627 |\n",
      "|    std                | 1.06        |\n",
      "|    value_loss         | 0.00165     |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 68         |\n",
      "|    iterations         | 7200       |\n",
      "|    time_elapsed       | 528        |\n",
      "|    total_timesteps    | 36000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7199       |\n",
      "|    policy_loss        | -5.72      |\n",
      "|    reward             | 0.05363819 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 0.3        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 68         |\n",
      "|    iterations         | 7300       |\n",
      "|    time_elapsed       | 535        |\n",
      "|    total_timesteps    | 36500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.2      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7299       |\n",
      "|    policy_loss        | 0.727      |\n",
      "|    reward             | 0.32266924 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 0.244      |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 68       |\n",
      "|    iterations         | 7400     |\n",
      "|    time_elapsed       | 543      |\n",
      "|    total_timesteps    | 37000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13.2    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7399     |\n",
      "|    policy_loss        | -25      |\n",
      "|    reward             | 0.585237 |\n",
      "|    std                | 1.06     |\n",
      "|    value_loss         | 7.04     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 67         |\n",
      "|    iterations         | 7500       |\n",
      "|    time_elapsed       | 552        |\n",
      "|    total_timesteps    | 37500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.3      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7499       |\n",
      "|    policy_loss        | -0.571     |\n",
      "|    reward             | 0.72304803 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 0.173      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 67         |\n",
      "|    iterations         | 7600       |\n",
      "|    time_elapsed       | 560        |\n",
      "|    total_timesteps    | 38000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.3      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7599       |\n",
      "|    policy_loss        | 4.53       |\n",
      "|    reward             | -2.8338797 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 1.38       |\n",
      "--------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 67            |\n",
      "|    iterations         | 7700          |\n",
      "|    time_elapsed       | 568           |\n",
      "|    total_timesteps    | 38500         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -13.3         |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 7699          |\n",
      "|    policy_loss        | 2.77          |\n",
      "|    reward             | -0.0058397427 |\n",
      "|    std                | 1.06          |\n",
      "|    value_loss         | 0.0337        |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 67          |\n",
      "|    iterations         | 7800        |\n",
      "|    time_elapsed       | 575         |\n",
      "|    total_timesteps    | 39000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.3       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 7799        |\n",
      "|    policy_loss        | 1.25        |\n",
      "|    reward             | 0.014702935 |\n",
      "|    std                | 1.06        |\n",
      "|    value_loss         | 0.372       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 67         |\n",
      "|    iterations         | 7900       |\n",
      "|    time_elapsed       | 583        |\n",
      "|    total_timesteps    | 39500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7899       |\n",
      "|    policy_loss        | -7.05      |\n",
      "|    reward             | 0.34672847 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 0.399      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 67          |\n",
      "|    iterations         | 8000        |\n",
      "|    time_elapsed       | 591         |\n",
      "|    total_timesteps    | 40000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.3       |\n",
      "|    explained_variance | 0.026       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 7999        |\n",
      "|    policy_loss        | 6.7         |\n",
      "|    reward             | -0.88967735 |\n",
      "|    std                | 1.07        |\n",
      "|    value_loss         | 0.721       |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 67        |\n",
      "|    iterations         | 8100      |\n",
      "|    time_elapsed       | 599       |\n",
      "|    total_timesteps    | 40500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8099      |\n",
      "|    policy_loss        | -32.7     |\n",
      "|    reward             | 2.3282135 |\n",
      "|    std                | 1.07      |\n",
      "|    value_loss         | 18.6      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 67         |\n",
      "|    iterations         | 8200       |\n",
      "|    time_elapsed       | 608        |\n",
      "|    total_timesteps    | 41000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.3      |\n",
      "|    explained_variance | -0.00648   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8199       |\n",
      "|    policy_loss        | -271       |\n",
      "|    reward             | -6.6073294 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 522        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 67         |\n",
      "|    iterations         | 8300       |\n",
      "|    time_elapsed       | 618        |\n",
      "|    total_timesteps    | 41500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.4      |\n",
      "|    explained_variance | -2.38e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8299       |\n",
      "|    policy_loss        | -16.5      |\n",
      "|    reward             | -0.5095184 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 1.94       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 66          |\n",
      "|    iterations         | 8400        |\n",
      "|    time_elapsed       | 627         |\n",
      "|    total_timesteps    | 42000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.4       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 8399        |\n",
      "|    policy_loss        | -8.45       |\n",
      "|    reward             | -0.47812775 |\n",
      "|    std                | 1.07        |\n",
      "|    value_loss         | 0.647       |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 66        |\n",
      "|    iterations         | 8500      |\n",
      "|    time_elapsed       | 640       |\n",
      "|    total_timesteps    | 42500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8499      |\n",
      "|    policy_loss        | -10.8     |\n",
      "|    reward             | 0.6431323 |\n",
      "|    std                | 1.07      |\n",
      "|    value_loss         | 1.51      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 66        |\n",
      "|    iterations         | 8600      |\n",
      "|    time_elapsed       | 649       |\n",
      "|    total_timesteps    | 43000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.4     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8599      |\n",
      "|    policy_loss        | 15.4      |\n",
      "|    reward             | 0.9931124 |\n",
      "|    std                | 1.08      |\n",
      "|    value_loss         | 1.38      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 66        |\n",
      "|    iterations         | 8700      |\n",
      "|    time_elapsed       | 658       |\n",
      "|    total_timesteps    | 43500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.5     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8699      |\n",
      "|    policy_loss        | -50.6     |\n",
      "|    reward             | 1.8849932 |\n",
      "|    std                | 1.08      |\n",
      "|    value_loss         | 19.1      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 65         |\n",
      "|    iterations         | 8800       |\n",
      "|    time_elapsed       | 668        |\n",
      "|    total_timesteps    | 44000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.5      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8799       |\n",
      "|    policy_loss        | 16         |\n",
      "|    reward             | -4.2590322 |\n",
      "|    std                | 1.09       |\n",
      "|    value_loss         | 3.86       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 65        |\n",
      "|    iterations         | 8900      |\n",
      "|    time_elapsed       | 677       |\n",
      "|    total_timesteps    | 44500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.5     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8899      |\n",
      "|    policy_loss        | 11.4      |\n",
      "|    reward             | 1.1490666 |\n",
      "|    std                | 1.09      |\n",
      "|    value_loss         | 0.882     |\n",
      "-------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 65           |\n",
      "|    iterations         | 9000         |\n",
      "|    time_elapsed       | 686          |\n",
      "|    total_timesteps    | 45000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -13.5        |\n",
      "|    explained_variance | 2.38e-07     |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 8999         |\n",
      "|    policy_loss        | 8.45         |\n",
      "|    reward             | -0.027046429 |\n",
      "|    std                | 1.09         |\n",
      "|    value_loss         | 0.666        |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 65           |\n",
      "|    iterations         | 9100         |\n",
      "|    time_elapsed       | 695          |\n",
      "|    total_timesteps    | 45500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -13.5        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 9099         |\n",
      "|    policy_loss        | -10.7        |\n",
      "|    reward             | -0.030867292 |\n",
      "|    std                | 1.09         |\n",
      "|    value_loss         | 0.955        |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 65         |\n",
      "|    iterations         | 9200       |\n",
      "|    time_elapsed       | 703        |\n",
      "|    total_timesteps    | 46000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.6      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9199       |\n",
      "|    policy_loss        | 8.76       |\n",
      "|    reward             | 0.28657192 |\n",
      "|    std                | 1.09       |\n",
      "|    value_loss         | 1.28       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 65        |\n",
      "|    iterations         | 9300      |\n",
      "|    time_elapsed       | 711       |\n",
      "|    total_timesteps    | 46500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.6     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9299      |\n",
      "|    policy_loss        | 32.4      |\n",
      "|    reward             | 0.4345243 |\n",
      "|    std                | 1.1       |\n",
      "|    value_loss         | 6.5       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 65        |\n",
      "|    iterations         | 9400      |\n",
      "|    time_elapsed       | 719       |\n",
      "|    total_timesteps    | 47000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9399      |\n",
      "|    policy_loss        | 28.3      |\n",
      "|    reward             | 1.7866365 |\n",
      "|    std                | 1.1       |\n",
      "|    value_loss         | 14        |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 65         |\n",
      "|    iterations         | 9500       |\n",
      "|    time_elapsed       | 727        |\n",
      "|    total_timesteps    | 47500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.6      |\n",
      "|    explained_variance | -4.33      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9499       |\n",
      "|    policy_loss        | 18.1       |\n",
      "|    reward             | 0.15894139 |\n",
      "|    std                | 1.1        |\n",
      "|    value_loss         | 2.14       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 65         |\n",
      "|    iterations         | 9600       |\n",
      "|    time_elapsed       | 735        |\n",
      "|    total_timesteps    | 48000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.6      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9599       |\n",
      "|    policy_loss        | 4.99       |\n",
      "|    reward             | -1.0153557 |\n",
      "|    std                | 1.1        |\n",
      "|    value_loss         | 0.575      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 65        |\n",
      "|    iterations         | 9700      |\n",
      "|    time_elapsed       | 743       |\n",
      "|    total_timesteps    | 48500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9699      |\n",
      "|    policy_loss        | 12.3      |\n",
      "|    reward             | -1.157452 |\n",
      "|    std                | 1.11      |\n",
      "|    value_loss         | 1.03      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 65          |\n",
      "|    iterations         | 9800        |\n",
      "|    time_elapsed       | 750         |\n",
      "|    total_timesteps    | 49000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.7       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 9799        |\n",
      "|    policy_loss        | -97.9       |\n",
      "|    reward             | -0.45801044 |\n",
      "|    std                | 1.11        |\n",
      "|    value_loss         | 52.7        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 65        |\n",
      "|    iterations         | 9900      |\n",
      "|    time_elapsed       | 756       |\n",
      "|    total_timesteps    | 49500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9899      |\n",
      "|    policy_loss        | 4.07      |\n",
      "|    reward             | 1.4139644 |\n",
      "|    std                | 1.11      |\n",
      "|    value_loss         | 0.428     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 65        |\n",
      "|    iterations         | 10000     |\n",
      "|    time_elapsed       | 764       |\n",
      "|    total_timesteps    | 50000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9999      |\n",
      "|    policy_loss        | 45.7      |\n",
      "|    reward             | -9.917589 |\n",
      "|    std                | 1.11      |\n",
      "|    value_loss         | 16.4      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 65        |\n",
      "|    iterations         | 10100     |\n",
      "|    time_elapsed       | 772       |\n",
      "|    total_timesteps    | 50500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.7     |\n",
      "|    explained_variance | 0.122     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 10099     |\n",
      "|    policy_loss        | 2.35      |\n",
      "|    reward             | 0.4974093 |\n",
      "|    std                | 1.11      |\n",
      "|    value_loss         | 0.375     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 65         |\n",
      "|    iterations         | 10200      |\n",
      "|    time_elapsed       | 779        |\n",
      "|    total_timesteps    | 51000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 10199      |\n",
      "|    policy_loss        | 9.77       |\n",
      "|    reward             | 0.82194114 |\n",
      "|    std                | 1.11       |\n",
      "|    value_loss         | 0.868      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 65        |\n",
      "|    iterations         | 10300     |\n",
      "|    time_elapsed       | 786       |\n",
      "|    total_timesteps    | 51500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.8     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 10299     |\n",
      "|    policy_loss        | -36.5     |\n",
      "|    reward             | 0.7684428 |\n",
      "|    std                | 1.12      |\n",
      "|    value_loss         | 8.15      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 65          |\n",
      "|    iterations         | 10400       |\n",
      "|    time_elapsed       | 793         |\n",
      "|    total_timesteps    | 52000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.8       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 10399       |\n",
      "|    policy_loss        | -36         |\n",
      "|    reward             | -0.56297797 |\n",
      "|    std                | 1.12        |\n",
      "|    value_loss         | 8.92        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 65         |\n",
      "|    iterations         | 10500      |\n",
      "|    time_elapsed       | 800        |\n",
      "|    total_timesteps    | 52500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.8      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 10499      |\n",
      "|    policy_loss        | 18.3       |\n",
      "|    reward             | -1.9487938 |\n",
      "|    std                | 1.12       |\n",
      "|    value_loss         | 2.94       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 65        |\n",
      "|    iterations         | 10600     |\n",
      "|    time_elapsed       | 809       |\n",
      "|    total_timesteps    | 53000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 10599     |\n",
      "|    policy_loss        | -155      |\n",
      "|    reward             | 0.5382856 |\n",
      "|    std                | 1.11      |\n",
      "|    value_loss         | 121       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 65         |\n",
      "|    iterations         | 10700      |\n",
      "|    time_elapsed       | 817        |\n",
      "|    total_timesteps    | 53500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 10699      |\n",
      "|    policy_loss        | -8.52      |\n",
      "|    reward             | 0.34769568 |\n",
      "|    std                | 1.12       |\n",
      "|    value_loss         | 0.339      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 65          |\n",
      "|    iterations         | 10800       |\n",
      "|    time_elapsed       | 826         |\n",
      "|    total_timesteps    | 54000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.8       |\n",
      "|    explained_variance | 0.00314     |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 10799       |\n",
      "|    policy_loss        | 7.69        |\n",
      "|    reward             | 0.033028383 |\n",
      "|    std                | 1.12        |\n",
      "|    value_loss         | 0.573       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 65         |\n",
      "|    iterations         | 10900      |\n",
      "|    time_elapsed       | 834        |\n",
      "|    total_timesteps    | 54500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.8      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 10899      |\n",
      "|    policy_loss        | 27.6       |\n",
      "|    reward             | -2.1046612 |\n",
      "|    std                | 1.12       |\n",
      "|    value_loss         | 3.44       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 65        |\n",
      "|    iterations         | 11000     |\n",
      "|    time_elapsed       | 842       |\n",
      "|    total_timesteps    | 55000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.8     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 10999     |\n",
      "|    policy_loss        | -16.5     |\n",
      "|    reward             | 1.3070372 |\n",
      "|    std                | 1.12      |\n",
      "|    value_loss         | 2.34      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 65        |\n",
      "|    iterations         | 11100     |\n",
      "|    time_elapsed       | 850       |\n",
      "|    total_timesteps    | 55500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.8     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 11099     |\n",
      "|    policy_loss        | 30        |\n",
      "|    reward             | -0.918385 |\n",
      "|    std                | 1.12      |\n",
      "|    value_loss         | 12.3      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 65         |\n",
      "|    iterations         | 11200      |\n",
      "|    time_elapsed       | 857        |\n",
      "|    total_timesteps    | 56000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 11199      |\n",
      "|    policy_loss        | -48.3      |\n",
      "|    reward             | -3.0126991 |\n",
      "|    std                | 1.12       |\n",
      "|    value_loss         | 14.2       |\n",
      "--------------------------------------\n",
      "day: 2956, episode: 20\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3952373.45\n",
      "total_reward: 2952373.45\n",
      "total_cost: 5367.29\n",
      "total_trades: 16219\n",
      "Sharpe: 0.915\n",
      "=================================\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 65         |\n",
      "|    iterations         | 11300      |\n",
      "|    time_elapsed       | 866        |\n",
      "|    total_timesteps    | 56500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.8      |\n",
      "|    explained_variance | -0.0288    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 11299      |\n",
      "|    policy_loss        | 13.3       |\n",
      "|    reward             | 0.49393436 |\n",
      "|    std                | 1.12       |\n",
      "|    value_loss         | 1.27       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 65        |\n",
      "|    iterations         | 11400     |\n",
      "|    time_elapsed       | 874       |\n",
      "|    total_timesteps    | 57000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.8     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 11399     |\n",
      "|    policy_loss        | 8.58      |\n",
      "|    reward             | 0.7554575 |\n",
      "|    std                | 1.12      |\n",
      "|    value_loss         | 0.729     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 65        |\n",
      "|    iterations         | 11500     |\n",
      "|    time_elapsed       | 883       |\n",
      "|    total_timesteps    | 57500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.9     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 11499     |\n",
      "|    policy_loss        | 18.3      |\n",
      "|    reward             | 2.1011963 |\n",
      "|    std                | 1.13      |\n",
      "|    value_loss         | 2.3       |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 65          |\n",
      "|    iterations         | 11600       |\n",
      "|    time_elapsed       | 892         |\n",
      "|    total_timesteps    | 58000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.9       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 11599       |\n",
      "|    policy_loss        | 13.3        |\n",
      "|    reward             | -0.52357566 |\n",
      "|    std                | 1.13        |\n",
      "|    value_loss         | 1.43        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 64        |\n",
      "|    iterations         | 11700     |\n",
      "|    time_elapsed       | 901       |\n",
      "|    total_timesteps    | 58500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.9     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 11699     |\n",
      "|    policy_loss        | 48.5      |\n",
      "|    reward             | -1.104122 |\n",
      "|    std                | 1.13      |\n",
      "|    value_loss         | 12.9      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 64        |\n",
      "|    iterations         | 11800     |\n",
      "|    time_elapsed       | 911       |\n",
      "|    total_timesteps    | 59000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.9     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 11799     |\n",
      "|    policy_loss        | 11.2      |\n",
      "|    reward             | 2.3647172 |\n",
      "|    std                | 1.14      |\n",
      "|    value_loss         | 12.2      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 64          |\n",
      "|    iterations         | 11900       |\n",
      "|    time_elapsed       | 920         |\n",
      "|    total_timesteps    | 59500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.9       |\n",
      "|    explained_variance | -0.0142     |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 11899       |\n",
      "|    policy_loss        | -14.2       |\n",
      "|    reward             | -0.06357787 |\n",
      "|    std                | 1.14        |\n",
      "|    value_loss         | 1.09        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 64         |\n",
      "|    iterations         | 12000      |\n",
      "|    time_elapsed       | 929        |\n",
      "|    total_timesteps    | 60000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -14        |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 11999      |\n",
      "|    policy_loss        | -30.7      |\n",
      "|    reward             | -1.8611778 |\n",
      "|    std                | 1.14       |\n",
      "|    value_loss         | 4.83       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 64         |\n",
      "|    iterations         | 12100      |\n",
      "|    time_elapsed       | 940        |\n",
      "|    total_timesteps    | 60500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -14        |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 12099      |\n",
      "|    policy_loss        | -17.8      |\n",
      "|    reward             | 0.18907903 |\n",
      "|    std                | 1.14       |\n",
      "|    value_loss         | 1.61       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 64        |\n",
      "|    iterations         | 12200     |\n",
      "|    time_elapsed       | 949       |\n",
      "|    total_timesteps    | 61000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -14       |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 12199     |\n",
      "|    policy_loss        | -31.2     |\n",
      "|    reward             | 0.9259375 |\n",
      "|    std                | 1.15      |\n",
      "|    value_loss         | 3.86      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 64        |\n",
      "|    iterations         | 12300     |\n",
      "|    time_elapsed       | 957       |\n",
      "|    total_timesteps    | 61500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -14       |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 12299     |\n",
      "|    policy_loss        | -15.6     |\n",
      "|    reward             | 0.6409748 |\n",
      "|    std                | 1.15      |\n",
      "|    value_loss         | 4.02      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 64       |\n",
      "|    iterations         | 12400    |\n",
      "|    time_elapsed       | 965      |\n",
      "|    total_timesteps    | 62000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -14      |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 12399    |\n",
      "|    policy_loss        | 49.8     |\n",
      "|    reward             | 4.076384 |\n",
      "|    std                | 1.15     |\n",
      "|    value_loss         | 14.9     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 64       |\n",
      "|    iterations         | 12500    |\n",
      "|    time_elapsed       | 973      |\n",
      "|    total_timesteps    | 62500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -14      |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 12499    |\n",
      "|    policy_loss        | -65.3    |\n",
      "|    reward             | 4.004997 |\n",
      "|    std                | 1.15     |\n",
      "|    value_loss         | 23       |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 64         |\n",
      "|    iterations         | 12600      |\n",
      "|    time_elapsed       | 980        |\n",
      "|    total_timesteps    | 63000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -14        |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 12599      |\n",
      "|    policy_loss        | -15.4      |\n",
      "|    reward             | -0.5405309 |\n",
      "|    std                | 1.15       |\n",
      "|    value_loss         | 1.31       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 64          |\n",
      "|    iterations         | 12700       |\n",
      "|    time_elapsed       | 989         |\n",
      "|    total_timesteps    | 63500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -14         |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 12699       |\n",
      "|    policy_loss        | -38.5       |\n",
      "|    reward             | -0.38125882 |\n",
      "|    std                | 1.15        |\n",
      "|    value_loss         | 7.43        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 64         |\n",
      "|    iterations         | 12800      |\n",
      "|    time_elapsed       | 997        |\n",
      "|    total_timesteps    | 64000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -14        |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 12799      |\n",
      "|    policy_loss        | 23.8       |\n",
      "|    reward             | 0.07140383 |\n",
      "|    std                | 1.15       |\n",
      "|    value_loss         | 3.16       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 64        |\n",
      "|    iterations         | 12900     |\n",
      "|    time_elapsed       | 1006      |\n",
      "|    total_timesteps    | 64500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -14       |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 12899     |\n",
      "|    policy_loss        | 13.1      |\n",
      "|    reward             | 1.8605254 |\n",
      "|    std                | 1.15      |\n",
      "|    value_loss         | 2.86      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 64         |\n",
      "|    iterations         | 13000      |\n",
      "|    time_elapsed       | 1014       |\n",
      "|    total_timesteps    | 65000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -14        |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 12999      |\n",
      "|    policy_loss        | 20.2       |\n",
      "|    reward             | -1.8461179 |\n",
      "|    std                | 1.15       |\n",
      "|    value_loss         | 2.76       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 64        |\n",
      "|    iterations         | 13100     |\n",
      "|    time_elapsed       | 1021      |\n",
      "|    total_timesteps    | 65500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -14.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 13099     |\n",
      "|    policy_loss        | -74.4     |\n",
      "|    reward             | 3.3717198 |\n",
      "|    std                | 1.16      |\n",
      "|    value_loss         | 34.8      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 64        |\n",
      "|    iterations         | 13200     |\n",
      "|    time_elapsed       | 1027      |\n",
      "|    total_timesteps    | 66000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -14.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 13199     |\n",
      "|    policy_loss        | -25.4     |\n",
      "|    reward             | -1.539504 |\n",
      "|    std                | 1.15      |\n",
      "|    value_loss         | 3.27      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 64        |\n",
      "|    iterations         | 13300     |\n",
      "|    time_elapsed       | 1034      |\n",
      "|    total_timesteps    | 66500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -14.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 13299     |\n",
      "|    policy_loss        | -38.1     |\n",
      "|    reward             | 0.7518549 |\n",
      "|    std                | 1.16      |\n",
      "|    value_loss         | 8.88      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 64         |\n",
      "|    iterations         | 13400      |\n",
      "|    time_elapsed       | 1042       |\n",
      "|    total_timesteps    | 67000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -14.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 13399      |\n",
      "|    policy_loss        | -10.7      |\n",
      "|    reward             | 0.19260015 |\n",
      "|    std                | 1.16       |\n",
      "|    value_loss         | 0.841      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 64        |\n",
      "|    iterations         | 13500     |\n",
      "|    time_elapsed       | 1049      |\n",
      "|    total_timesteps    | 67500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -14.1     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 13499     |\n",
      "|    policy_loss        | -32.3     |\n",
      "|    reward             | 0.9528956 |\n",
      "|    std                | 1.16      |\n",
      "|    value_loss         | 5.35      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 64        |\n",
      "|    iterations         | 13600     |\n",
      "|    time_elapsed       | 1056      |\n",
      "|    total_timesteps    | 68000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -14.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 13599     |\n",
      "|    policy_loss        | -18.2     |\n",
      "|    reward             | 0.4285768 |\n",
      "|    std                | 1.17      |\n",
      "|    value_loss         | 1.69      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 64        |\n",
      "|    iterations         | 13700     |\n",
      "|    time_elapsed       | 1063      |\n",
      "|    total_timesteps    | 68500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -14.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 13699     |\n",
      "|    policy_loss        | 43.5      |\n",
      "|    reward             | 2.0030792 |\n",
      "|    std                | 1.16      |\n",
      "|    value_loss         | 18.5      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 64        |\n",
      "|    iterations         | 13800     |\n",
      "|    time_elapsed       | 1070      |\n",
      "|    total_timesteps    | 69000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -14.2     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 13799     |\n",
      "|    policy_loss        | -2.95     |\n",
      "|    reward             | 1.5416461 |\n",
      "|    std                | 1.17      |\n",
      "|    value_loss         | 0.116     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 64         |\n",
      "|    iterations         | 13900      |\n",
      "|    time_elapsed       | 1076       |\n",
      "|    total_timesteps    | 69500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -14.2      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 13899      |\n",
      "|    policy_loss        | 30.3       |\n",
      "|    reward             | -2.5186768 |\n",
      "|    std                | 1.17       |\n",
      "|    value_loss         | 7.22       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 64          |\n",
      "|    iterations         | 14000       |\n",
      "|    time_elapsed       | 1084        |\n",
      "|    total_timesteps    | 70000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -14.2       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 13999       |\n",
      "|    policy_loss        | 15.9        |\n",
      "|    reward             | -0.20302962 |\n",
      "|    std                | 1.17        |\n",
      "|    value_loss         | 1.77        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 64         |\n",
      "|    iterations         | 14100      |\n",
      "|    time_elapsed       | 1092       |\n",
      "|    total_timesteps    | 70500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -14.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 14099      |\n",
      "|    policy_loss        | -7.53      |\n",
      "|    reward             | -1.3555235 |\n",
      "|    std                | 1.17       |\n",
      "|    value_loss         | 1.09       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 64         |\n",
      "|    iterations         | 14200      |\n",
      "|    time_elapsed       | 1101       |\n",
      "|    total_timesteps    | 71000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -14.2      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 14199      |\n",
      "|    policy_loss        | 3.35       |\n",
      "|    reward             | 0.06315063 |\n",
      "|    std                | 1.17       |\n",
      "|    value_loss         | 0.0585     |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 64        |\n",
      "|    iterations         | 14300     |\n",
      "|    time_elapsed       | 1109      |\n",
      "|    total_timesteps    | 71500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -14.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 14299     |\n",
      "|    policy_loss        | 29.7      |\n",
      "|    reward             | 1.0036587 |\n",
      "|    std                | 1.17      |\n",
      "|    value_loss         | 4.29      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 64         |\n",
      "|    iterations         | 14400      |\n",
      "|    time_elapsed       | 1116       |\n",
      "|    total_timesteps    | 72000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -14.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 14399      |\n",
      "|    policy_loss        | -94.3      |\n",
      "|    reward             | -0.3076441 |\n",
      "|    std                | 1.17       |\n",
      "|    value_loss         | 39.2       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 64        |\n",
      "|    iterations         | 14500     |\n",
      "|    time_elapsed       | 1122      |\n",
      "|    total_timesteps    | 72500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -14.2     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 14499     |\n",
      "|    policy_loss        | 27.6      |\n",
      "|    reward             | -0.874479 |\n",
      "|    std                | 1.17      |\n",
      "|    value_loss         | 6.68      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 64        |\n",
      "|    iterations         | 14600     |\n",
      "|    time_elapsed       | 1129      |\n",
      "|    total_timesteps    | 73000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -14.2     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 14599     |\n",
      "|    policy_loss        | 75        |\n",
      "|    reward             | -2.286194 |\n",
      "|    std                | 1.17      |\n",
      "|    value_loss         | 37.1      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 64         |\n",
      "|    iterations         | 14700      |\n",
      "|    time_elapsed       | 1137       |\n",
      "|    total_timesteps    | 73500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -14.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 14699      |\n",
      "|    policy_loss        | 32.3       |\n",
      "|    reward             | -5.9620004 |\n",
      "|    std                | 1.17       |\n",
      "|    value_loss         | 6.65       |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 64           |\n",
      "|    iterations         | 14800        |\n",
      "|    time_elapsed       | 1144         |\n",
      "|    total_timesteps    | 74000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -14.2        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 14799        |\n",
      "|    policy_loss        | -0.178       |\n",
      "|    reward             | -0.053425413 |\n",
      "|    std                | 1.17         |\n",
      "|    value_loss         | 0.00685      |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 64          |\n",
      "|    iterations         | 14900       |\n",
      "|    time_elapsed       | 1150        |\n",
      "|    total_timesteps    | 74500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -14.2       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 14899       |\n",
      "|    policy_loss        | -34.9       |\n",
      "|    reward             | -0.19431047 |\n",
      "|    std                | 1.17        |\n",
      "|    value_loss         | 6.26        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 64         |\n",
      "|    iterations         | 15000      |\n",
      "|    time_elapsed       | 1158       |\n",
      "|    total_timesteps    | 75000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -14.2      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 14999      |\n",
      "|    policy_loss        | -16.7      |\n",
      "|    reward             | -1.5884941 |\n",
      "|    std                | 1.17       |\n",
      "|    value_loss         | 2.49       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 64         |\n",
      "|    iterations         | 15100      |\n",
      "|    time_elapsed       | 1164       |\n",
      "|    total_timesteps    | 75500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -14.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 15099      |\n",
      "|    policy_loss        | 18.9       |\n",
      "|    reward             | -2.2757847 |\n",
      "|    std                | 1.18       |\n",
      "|    value_loss         | 3.45       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 64        |\n",
      "|    iterations         | 15200     |\n",
      "|    time_elapsed       | 1171      |\n",
      "|    total_timesteps    | 76000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -14.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15199     |\n",
      "|    policy_loss        | -4.7      |\n",
      "|    reward             | 3.5150526 |\n",
      "|    std                | 1.18      |\n",
      "|    value_loss         | 15.2      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 64         |\n",
      "|    iterations         | 15300      |\n",
      "|    time_elapsed       | 1178       |\n",
      "|    total_timesteps    | 76500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -14.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 15299      |\n",
      "|    policy_loss        | -215       |\n",
      "|    reward             | -7.1854286 |\n",
      "|    std                | 1.17       |\n",
      "|    value_loss         | 387        |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 64          |\n",
      "|    iterations         | 15400       |\n",
      "|    time_elapsed       | 1184        |\n",
      "|    total_timesteps    | 77000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -14.2       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 15399       |\n",
      "|    policy_loss        | 14.9        |\n",
      "|    reward             | -0.11902517 |\n",
      "|    std                | 1.17        |\n",
      "|    value_loss         | 2.02        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 65        |\n",
      "|    iterations         | 15500     |\n",
      "|    time_elapsed       | 1191      |\n",
      "|    total_timesteps    | 77500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -14.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15499     |\n",
      "|    policy_loss        | 29.2      |\n",
      "|    reward             | 1.1091894 |\n",
      "|    std                | 1.17      |\n",
      "|    value_loss         | 6.71      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 65         |\n",
      "|    iterations         | 15600      |\n",
      "|    time_elapsed       | 1198       |\n",
      "|    total_timesteps    | 78000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -14.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 15599      |\n",
      "|    policy_loss        | 23.2       |\n",
      "|    reward             | 0.42568415 |\n",
      "|    std                | 1.17       |\n",
      "|    value_loss         | 2.91       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 65        |\n",
      "|    iterations         | 15700     |\n",
      "|    time_elapsed       | 1204      |\n",
      "|    total_timesteps    | 78500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -14.2     |\n",
      "|    explained_variance | -0.000233 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15699     |\n",
      "|    policy_loss        | 3.48      |\n",
      "|    reward             | 0.3836379 |\n",
      "|    std                | 1.17      |\n",
      "|    value_loss         | 0.423     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 65       |\n",
      "|    iterations         | 15800    |\n",
      "|    time_elapsed       | 1210     |\n",
      "|    total_timesteps    | 79000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -14.2    |\n",
      "|    explained_variance | -0.00387 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 15799    |\n",
      "|    policy_loss        | -4.63    |\n",
      "|    reward             | 2.48119  |\n",
      "|    std                | 1.17     |\n",
      "|    value_loss         | 0.646    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 65        |\n",
      "|    iterations         | 15900     |\n",
      "|    time_elapsed       | 1217      |\n",
      "|    total_timesteps    | 79500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -14.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15899     |\n",
      "|    policy_loss        | 123       |\n",
      "|    reward             | 1.1537123 |\n",
      "|    std                | 1.17      |\n",
      "|    value_loss         | 115       |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 65          |\n",
      "|    iterations         | 16000       |\n",
      "|    time_elapsed       | 1223        |\n",
      "|    total_timesteps    | 80000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -14.2       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 15999       |\n",
      "|    policy_loss        | 46.9        |\n",
      "|    reward             | -0.88140976 |\n",
      "|    std                | 1.18        |\n",
      "|    value_loss         | 10.5        |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 65       |\n",
      "|    iterations         | 16100    |\n",
      "|    time_elapsed       | 1229     |\n",
      "|    total_timesteps    | 80500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -14.2    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 16099    |\n",
      "|    policy_loss        | 4.18     |\n",
      "|    reward             | 1.021368 |\n",
      "|    std                | 1.18     |\n",
      "|    value_loss         | 1.32     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 65        |\n",
      "|    iterations         | 16200     |\n",
      "|    time_elapsed       | 1236      |\n",
      "|    total_timesteps    | 81000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -14.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16199     |\n",
      "|    policy_loss        | -30.3     |\n",
      "|    reward             | 1.4090564 |\n",
      "|    std                | 1.17      |\n",
      "|    value_loss         | 5.28      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 65         |\n",
      "|    iterations         | 16300      |\n",
      "|    time_elapsed       | 1242       |\n",
      "|    total_timesteps    | 81500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -14.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 16299      |\n",
      "|    policy_loss        | -51.3      |\n",
      "|    reward             | -0.6289209 |\n",
      "|    std                | 1.18       |\n",
      "|    value_loss         | 12.3       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 65        |\n",
      "|    iterations         | 16400     |\n",
      "|    time_elapsed       | 1249      |\n",
      "|    total_timesteps    | 82000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -14.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16399     |\n",
      "|    policy_loss        | -40.9     |\n",
      "|    reward             | 1.1753328 |\n",
      "|    std                | 1.18      |\n",
      "|    value_loss         | 6.47      |\n",
      "-------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 65           |\n",
      "|    iterations         | 16500        |\n",
      "|    time_elapsed       | 1255         |\n",
      "|    total_timesteps    | 82500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -14.2        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 16499        |\n",
      "|    policy_loss        | -34.8        |\n",
      "|    reward             | -0.022953585 |\n",
      "|    std                | 1.18         |\n",
      "|    value_loss         | 21.9         |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 65         |\n",
      "|    iterations         | 16600      |\n",
      "|    time_elapsed       | 1261       |\n",
      "|    total_timesteps    | 83000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -14.3      |\n",
      "|    explained_variance | -0.0243    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 16599      |\n",
      "|    policy_loss        | 6.37       |\n",
      "|    reward             | 0.35947683 |\n",
      "|    std                | 1.18       |\n",
      "|    value_loss         | 0.312      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 65         |\n",
      "|    iterations         | 16700      |\n",
      "|    time_elapsed       | 1268       |\n",
      "|    total_timesteps    | 83500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -14.3      |\n",
      "|    explained_variance | 0.0216     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 16699      |\n",
      "|    policy_loss        | -37.5      |\n",
      "|    reward             | 0.75422555 |\n",
      "|    std                | 1.18       |\n",
      "|    value_loss         | 8.71       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 65         |\n",
      "|    iterations         | 16800      |\n",
      "|    time_elapsed       | 1274       |\n",
      "|    total_timesteps    | 84000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -14.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 16799      |\n",
      "|    policy_loss        | 10.2       |\n",
      "|    reward             | -1.3887022 |\n",
      "|    std                | 1.18       |\n",
      "|    value_loss         | 2.52       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 65        |\n",
      "|    iterations         | 16900     |\n",
      "|    time_elapsed       | 1281      |\n",
      "|    total_timesteps    | 84500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -14.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16899     |\n",
      "|    policy_loss        | 12.1      |\n",
      "|    reward             | 0.6161375 |\n",
      "|    std                | 1.18      |\n",
      "|    value_loss         | 2.17      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 66        |\n",
      "|    iterations         | 17000     |\n",
      "|    time_elapsed       | 1287      |\n",
      "|    total_timesteps    | 85000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -14.2     |\n",
      "|    explained_variance | 1.79e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16999     |\n",
      "|    policy_loss        | 11.1      |\n",
      "|    reward             | -5.635724 |\n",
      "|    std                | 1.18      |\n",
      "|    value_loss         | 2.49      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 66        |\n",
      "|    iterations         | 17100     |\n",
      "|    time_elapsed       | 1293      |\n",
      "|    total_timesteps    | 85500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -14.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17099     |\n",
      "|    policy_loss        | -128      |\n",
      "|    reward             | 3.8538125 |\n",
      "|    std                | 1.18      |\n",
      "|    value_loss         | 122       |\n",
      "-------------------------------------\n",
      "day: 2956, episode: 30\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 5943931.06\n",
      "total_reward: 4943931.06\n",
      "total_cost: 4309.26\n",
      "total_trades: 14814\n",
      "Sharpe: 0.977\n",
      "=================================\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 66          |\n",
      "|    iterations         | 17200       |\n",
      "|    time_elapsed       | 1299        |\n",
      "|    total_timesteps    | 86000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -14.2       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 17199       |\n",
      "|    policy_loss        | 1.03        |\n",
      "|    reward             | -0.34812358 |\n",
      "|    std                | 1.18        |\n",
      "|    value_loss         | 0.0561      |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 66         |\n",
      "|    iterations         | 17300      |\n",
      "|    time_elapsed       | 1306       |\n",
      "|    total_timesteps    | 86500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -14.3      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 17299      |\n",
      "|    policy_loss        | -16.8      |\n",
      "|    reward             | 0.46877423 |\n",
      "|    std                | 1.18       |\n",
      "|    value_loss         | 1.35       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 66         |\n",
      "|    iterations         | 17400      |\n",
      "|    time_elapsed       | 1312       |\n",
      "|    total_timesteps    | 87000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -14.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 17399      |\n",
      "|    policy_loss        | -39.4      |\n",
      "|    reward             | -2.0294986 |\n",
      "|    std                | 1.18       |\n",
      "|    value_loss         | 10.4       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 66         |\n",
      "|    iterations         | 17500      |\n",
      "|    time_elapsed       | 1318       |\n",
      "|    total_timesteps    | 87500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -14.3      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 17499      |\n",
      "|    policy_loss        | -23.8      |\n",
      "|    reward             | 0.69196117 |\n",
      "|    std                | 1.19       |\n",
      "|    value_loss         | 3.88       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 66         |\n",
      "|    iterations         | 17600      |\n",
      "|    time_elapsed       | 1325       |\n",
      "|    total_timesteps    | 88000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -14.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 17599      |\n",
      "|    policy_loss        | 162        |\n",
      "|    reward             | 0.47340423 |\n",
      "|    std                | 1.19       |\n",
      "|    value_loss         | 136        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 66        |\n",
      "|    iterations         | 17700     |\n",
      "|    time_elapsed       | 1331      |\n",
      "|    total_timesteps    | 88500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -14.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17699     |\n",
      "|    policy_loss        | 31.4      |\n",
      "|    reward             | 3.2496328 |\n",
      "|    std                | 1.18      |\n",
      "|    value_loss         | 11.1      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 66       |\n",
      "|    iterations         | 17800    |\n",
      "|    time_elapsed       | 1337     |\n",
      "|    total_timesteps    | 89000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -14.2    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 17799    |\n",
      "|    policy_loss        | 13       |\n",
      "|    reward             | 0.881371 |\n",
      "|    std                | 1.18     |\n",
      "|    value_loss         | 0.911    |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 66         |\n",
      "|    iterations         | 17900      |\n",
      "|    time_elapsed       | 1344       |\n",
      "|    total_timesteps    | 89500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -14.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 17899      |\n",
      "|    policy_loss        | -4.62      |\n",
      "|    reward             | -2.8925095 |\n",
      "|    std                | 1.18       |\n",
      "|    value_loss         | 0.159      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 66         |\n",
      "|    iterations         | 18000      |\n",
      "|    time_elapsed       | 1349       |\n",
      "|    total_timesteps    | 90000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -14.3      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 17999      |\n",
      "|    policy_loss        | 6.6        |\n",
      "|    reward             | 0.28368288 |\n",
      "|    std                | 1.19       |\n",
      "|    value_loss         | 0.786      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 66         |\n",
      "|    iterations         | 18100      |\n",
      "|    time_elapsed       | 1356       |\n",
      "|    total_timesteps    | 90500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -14.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 18099      |\n",
      "|    policy_loss        | -17.1      |\n",
      "|    reward             | 0.76542056 |\n",
      "|    std                | 1.18       |\n",
      "|    value_loss         | 1.66       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 66         |\n",
      "|    iterations         | 18200      |\n",
      "|    time_elapsed       | 1362       |\n",
      "|    total_timesteps    | 91000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -14.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 18199      |\n",
      "|    policy_loss        | 23.9       |\n",
      "|    reward             | -0.6759463 |\n",
      "|    std                | 1.18       |\n",
      "|    value_loss         | 10.1       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 66         |\n",
      "|    iterations         | 18300      |\n",
      "|    time_elapsed       | 1368       |\n",
      "|    total_timesteps    | 91500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -14.3      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 18299      |\n",
      "|    policy_loss        | -192       |\n",
      "|    reward             | -1.0112451 |\n",
      "|    std                | 1.18       |\n",
      "|    value_loss         | 180        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 66        |\n",
      "|    iterations         | 18400     |\n",
      "|    time_elapsed       | 1375      |\n",
      "|    total_timesteps    | 92000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -14.3     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18399     |\n",
      "|    policy_loss        | 26.3      |\n",
      "|    reward             | 0.707528  |\n",
      "|    std                | 1.18      |\n",
      "|    value_loss         | 4.82      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 66          |\n",
      "|    iterations         | 18500       |\n",
      "|    time_elapsed       | 1382        |\n",
      "|    total_timesteps    | 92500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -14.3       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 18499       |\n",
      "|    policy_loss        | -26.2       |\n",
      "|    reward             | -0.45479608 |\n",
      "|    std                | 1.18        |\n",
      "|    value_loss         | 8.84        |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 67       |\n",
      "|    iterations         | 18600    |\n",
      "|    time_elapsed       | 1387     |\n",
      "|    total_timesteps    | 93000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -14.3    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 18599    |\n",
      "|    policy_loss        | 16.6     |\n",
      "|    reward             | 0.82743  |\n",
      "|    std                | 1.18     |\n",
      "|    value_loss         | 1.94     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 67        |\n",
      "|    iterations         | 18700     |\n",
      "|    time_elapsed       | 1394      |\n",
      "|    total_timesteps    | 93500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -14.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18699     |\n",
      "|    policy_loss        | -12.7     |\n",
      "|    reward             | 2.0625052 |\n",
      "|    std                | 1.19      |\n",
      "|    value_loss         | 0.932     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 67         |\n",
      "|    iterations         | 18800      |\n",
      "|    time_elapsed       | 1400       |\n",
      "|    total_timesteps    | 94000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -14.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 18799      |\n",
      "|    policy_loss        | 51.8       |\n",
      "|    reward             | 0.70196986 |\n",
      "|    std                | 1.19       |\n",
      "|    value_loss         | 18.8       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 67          |\n",
      "|    iterations         | 18900       |\n",
      "|    time_elapsed       | 1406        |\n",
      "|    total_timesteps    | 94500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -14.3       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 18899       |\n",
      "|    policy_loss        | 53.8        |\n",
      "|    reward             | -0.51015824 |\n",
      "|    std                | 1.19        |\n",
      "|    value_loss         | 23          |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 67        |\n",
      "|    iterations         | 19000     |\n",
      "|    time_elapsed       | 1412      |\n",
      "|    total_timesteps    | 95000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -14.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18999     |\n",
      "|    policy_loss        | 6.69      |\n",
      "|    reward             | 1.5356218 |\n",
      "|    std                | 1.18      |\n",
      "|    value_loss         | 1.28      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 67        |\n",
      "|    iterations         | 19100     |\n",
      "|    time_elapsed       | 1418      |\n",
      "|    total_timesteps    | 95500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -14.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19099     |\n",
      "|    policy_loss        | -71.4     |\n",
      "|    reward             | 1.0624901 |\n",
      "|    std                | 1.18      |\n",
      "|    value_loss         | 25.4      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 67         |\n",
      "|    iterations         | 19200      |\n",
      "|    time_elapsed       | 1425       |\n",
      "|    total_timesteps    | 96000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -14.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 19199      |\n",
      "|    policy_loss        | -10        |\n",
      "|    reward             | 0.45244434 |\n",
      "|    std                | 1.19       |\n",
      "|    value_loss         | 1.59       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 67        |\n",
      "|    iterations         | 19300     |\n",
      "|    time_elapsed       | 1431      |\n",
      "|    total_timesteps    | 96500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -14.3     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19299     |\n",
      "|    policy_loss        | -32.3     |\n",
      "|    reward             | 0.6718939 |\n",
      "|    std                | 1.19      |\n",
      "|    value_loss         | 5.83      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 67        |\n",
      "|    iterations         | 19400     |\n",
      "|    time_elapsed       | 1437      |\n",
      "|    total_timesteps    | 97000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -14.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19399     |\n",
      "|    policy_loss        | 83.5      |\n",
      "|    reward             | 2.3084705 |\n",
      "|    std                | 1.19      |\n",
      "|    value_loss         | 48.3      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 67       |\n",
      "|    iterations         | 19500    |\n",
      "|    time_elapsed       | 1443     |\n",
      "|    total_timesteps    | 97500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -14.3    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 19499    |\n",
      "|    policy_loss        | 33.9     |\n",
      "|    reward             | 2.463351 |\n",
      "|    std                | 1.19     |\n",
      "|    value_loss         | 7.7      |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 67         |\n",
      "|    iterations         | 19600      |\n",
      "|    time_elapsed       | 1449       |\n",
      "|    total_timesteps    | 98000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -14.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 19599      |\n",
      "|    policy_loss        | 31.5       |\n",
      "|    reward             | 0.64701563 |\n",
      "|    std                | 1.19       |\n",
      "|    value_loss         | 15         |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 67        |\n",
      "|    iterations         | 19700     |\n",
      "|    time_elapsed       | 1455      |\n",
      "|    total_timesteps    | 98500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -14.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19699     |\n",
      "|    policy_loss        | -29.1     |\n",
      "|    reward             | 1.1543411 |\n",
      "|    std                | 1.19      |\n",
      "|    value_loss         | 6.9       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 67        |\n",
      "|    iterations         | 19800     |\n",
      "|    time_elapsed       | 1461      |\n",
      "|    total_timesteps    | 99000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -14.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19799     |\n",
      "|    policy_loss        | 11.7      |\n",
      "|    reward             | -8.018154 |\n",
      "|    std                | 1.19      |\n",
      "|    value_loss         | 1.23      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 67         |\n",
      "|    iterations         | 19900      |\n",
      "|    time_elapsed       | 1467       |\n",
      "|    total_timesteps    | 99500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -14.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 19899      |\n",
      "|    policy_loss        | -67.4      |\n",
      "|    reward             | -4.0773454 |\n",
      "|    std                | 1.19       |\n",
      "|    value_loss         | 21.5       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 67        |\n",
      "|    iterations         | 20000     |\n",
      "|    time_elapsed       | 1473      |\n",
      "|    total_timesteps    | 100000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -14.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19999     |\n",
      "|    policy_loss        | -59.5     |\n",
      "|    reward             | -9.939007 |\n",
      "|    std                | 1.19      |\n",
      "|    value_loss         | 59.7      |\n",
      "-------------------------------------\n",
      "hit end!\n",
      "==============Get Baseline Stats===========\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Shape of DataFrame:  (400, 8)\n",
      "Annual return         -0.022145\n",
      "Cumulative returns    -0.034921\n",
      "Annual volatility      0.176822\n",
      "Sharpe ratio          -0.038668\n",
      "Calmar ratio          -0.100930\n",
      "Stability              0.215672\n",
      "Max drawdown          -0.219408\n",
      "Omega ratio            0.993578\n",
      "Sortino ratio         -0.053987\n",
      "Skew                        NaN\n",
      "Kurtosis                    NaN\n",
      "Tail ratio             1.003436\n",
      "Daily value at risk   -0.022305\n",
      "dtype: float64\n",
      "result:                       a2c           dji\n",
      "date                                  \n",
      "2021-10-01  1.000000e+06  1.000000e+06\n",
      "2021-10-04  9.994680e+05  9.905746e+05\n",
      "2021-10-05  1.000312e+06  9.996566e+05\n",
      "2021-10-06  1.000874e+06  1.002637e+06\n",
      "2021-10-07  1.003048e+06  1.012483e+06\n",
      "...                  ...           ...\n",
      "2023-04-27  9.509908e+05  9.854252e+05\n",
      "2023-04-28  9.588728e+05  9.933491e+05\n",
      "2023-05-01  9.547838e+05  9.919956e+05\n",
      "2023-05-02  9.441706e+05  9.812993e+05\n",
      "2023-05-03  9.357140e+05  9.734251e+05\n",
      "\n",
      "[399 rows x 2 columns]\n",
      "A2C로 얻은 투자 수익률>> -6.43 %\n",
      "{'n_steps': 5, 'ent_coef': 0.01, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to results/a2c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\se99a\\anaconda3\\lib\\site-packages\\pyfolio\\timeseries.py:724: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  stats = pd.Series()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 81        |\n",
      "|    iterations         | 100       |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 500       |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -12.8     |\n",
      "|    explained_variance | -0.197    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 99        |\n",
      "|    policy_loss        | -11.9     |\n",
      "|    reward             | 0.3162262 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 1.39      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 81         |\n",
      "|    iterations         | 200        |\n",
      "|    time_elapsed       | 12         |\n",
      "|    total_timesteps    | 1000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -12.9      |\n",
      "|    explained_variance | -0.159     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 199        |\n",
      "|    policy_loss        | -9.25      |\n",
      "|    reward             | 0.49540532 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 3.36       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 81        |\n",
      "|    iterations         | 300       |\n",
      "|    time_elapsed       | 18        |\n",
      "|    total_timesteps    | 1500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -12.9     |\n",
      "|    explained_variance | -0.0999   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 299       |\n",
      "|    policy_loss        | -13.4     |\n",
      "|    reward             | -2.366373 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 3.32      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 81         |\n",
      "|    iterations         | 400        |\n",
      "|    time_elapsed       | 24         |\n",
      "|    total_timesteps    | 2000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -12.8      |\n",
      "|    explained_variance | -0.766     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 399        |\n",
      "|    policy_loss        | -23.1      |\n",
      "|    reward             | -1.3556383 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 4.49       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 82        |\n",
      "|    iterations         | 500       |\n",
      "|    time_elapsed       | 30        |\n",
      "|    total_timesteps    | 2500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -12.8     |\n",
      "|    explained_variance | 0.14      |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 499       |\n",
      "|    policy_loss        | -32.6     |\n",
      "|    reward             | -1.240193 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 6.19      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 77          |\n",
      "|    iterations         | 600         |\n",
      "|    time_elapsed       | 38          |\n",
      "|    total_timesteps    | 3000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -12.8       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 599         |\n",
      "|    policy_loss        | 4.06        |\n",
      "|    reward             | -0.06652672 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 0.104       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 72         |\n",
      "|    iterations         | 700        |\n",
      "|    time_elapsed       | 48         |\n",
      "|    total_timesteps    | 3500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -12.9      |\n",
      "|    explained_variance | -0.191     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 699        |\n",
      "|    policy_loss        | 3.72       |\n",
      "|    reward             | -0.7146044 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 0.542      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 69          |\n",
      "|    iterations         | 800         |\n",
      "|    time_elapsed       | 57          |\n",
      "|    total_timesteps    | 4000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -12.9       |\n",
      "|    explained_variance | 1.77e-05    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 799         |\n",
      "|    policy_loss        | 8.2         |\n",
      "|    reward             | -0.37684795 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 1.49        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 67        |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 66        |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -12.9     |\n",
      "|    explained_variance | 0.00995   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | -27.8     |\n",
      "|    reward             | 2.4179075 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 6.27      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 66        |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 75        |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -12.9     |\n",
      "|    explained_variance | -0.0234   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | -99.7     |\n",
      "|    reward             | -0.556737 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 70.6      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 65         |\n",
      "|    iterations         | 1100       |\n",
      "|    time_elapsed       | 83         |\n",
      "|    total_timesteps    | 5500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -12.9      |\n",
      "|    explained_variance | -7.95e-05  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1099       |\n",
      "|    policy_loss        | -10.6      |\n",
      "|    reward             | 0.63616794 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 2.04       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 65         |\n",
      "|    iterations         | 1200       |\n",
      "|    time_elapsed       | 91         |\n",
      "|    total_timesteps    | 6000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -12.9      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1199       |\n",
      "|    policy_loss        | -9.95      |\n",
      "|    reward             | -0.9600917 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 0.904      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 66        |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 98        |\n",
      "|    total_timesteps    | 6500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -12.9     |\n",
      "|    explained_variance | -0.0216   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1299      |\n",
      "|    policy_loss        | -5.92     |\n",
      "|    reward             | 1.2816793 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 0.409     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 66         |\n",
      "|    iterations         | 1400       |\n",
      "|    time_elapsed       | 104        |\n",
      "|    total_timesteps    | 7000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -12.9      |\n",
      "|    explained_variance | 0.0858     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1399       |\n",
      "|    policy_loss        | 20.1       |\n",
      "|    reward             | 0.45745444 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 4.54       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 67        |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 111       |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -12.9     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | 12.5      |\n",
      "|    reward             | 1.5468206 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 2.24      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 68        |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 117       |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -12.9     |\n",
      "|    explained_variance | -0.148    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | 9.76      |\n",
      "|    reward             | 0.2596536 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 2.38      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 68        |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 123       |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -12.9     |\n",
      "|    explained_variance | 0.000596  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | 88.7      |\n",
      "|    reward             | 5.3785663 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 119       |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 69       |\n",
      "|    iterations         | 1800     |\n",
      "|    time_elapsed       | 130      |\n",
      "|    total_timesteps    | 9000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -12.9    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1799     |\n",
      "|    policy_loss        | -21.7    |\n",
      "|    reward             | 0.451654 |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 4.29     |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 69          |\n",
      "|    iterations         | 1900        |\n",
      "|    time_elapsed       | 137         |\n",
      "|    total_timesteps    | 9500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -12.9       |\n",
      "|    explained_variance | 0.158       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1899        |\n",
      "|    policy_loss        | -44.5       |\n",
      "|    reward             | -0.26701912 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 12.8        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 69          |\n",
      "|    iterations         | 2000        |\n",
      "|    time_elapsed       | 143         |\n",
      "|    total_timesteps    | 10000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -12.9       |\n",
      "|    explained_variance | 0.0322      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1999        |\n",
      "|    policy_loss        | -0.0986     |\n",
      "|    reward             | -0.58377534 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 0.635       |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 70        |\n",
      "|    iterations         | 2100      |\n",
      "|    time_elapsed       | 149       |\n",
      "|    total_timesteps    | 10500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -12.9     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2099      |\n",
      "|    policy_loss        | -17.4     |\n",
      "|    reward             | 2.6172214 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 5.94      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 70         |\n",
      "|    iterations         | 2200       |\n",
      "|    time_elapsed       | 155        |\n",
      "|    total_timesteps    | 11000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -12.9      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2199       |\n",
      "|    policy_loss        | 19.2       |\n",
      "|    reward             | -6.7128744 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 5.72       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 70        |\n",
      "|    iterations         | 2300      |\n",
      "|    time_elapsed       | 162       |\n",
      "|    total_timesteps    | 11500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -12.9     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2299      |\n",
      "|    policy_loss        | 325       |\n",
      "|    reward             | 3.0392241 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 639       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 70         |\n",
      "|    iterations         | 2400       |\n",
      "|    time_elapsed       | 169        |\n",
      "|    total_timesteps    | 12000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13        |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2399       |\n",
      "|    policy_loss        | 23.7       |\n",
      "|    reward             | 0.23386289 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 3.78       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 71         |\n",
      "|    iterations         | 2500       |\n",
      "|    time_elapsed       | 175        |\n",
      "|    total_timesteps    | 12500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13        |\n",
      "|    explained_variance | 0.0467     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2499       |\n",
      "|    policy_loss        | -7.45      |\n",
      "|    reward             | 0.66829544 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 0.88       |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 71           |\n",
      "|    iterations         | 2600         |\n",
      "|    time_elapsed       | 181          |\n",
      "|    total_timesteps    | 13000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -13          |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 2599         |\n",
      "|    policy_loss        | 9.14         |\n",
      "|    reward             | -0.049798902 |\n",
      "|    std                | 1.02         |\n",
      "|    value_loss         | 3.78         |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 71         |\n",
      "|    iterations         | 2700       |\n",
      "|    time_elapsed       | 187        |\n",
      "|    total_timesteps    | 13500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13        |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2699       |\n",
      "|    policy_loss        | 0.444      |\n",
      "|    reward             | -0.6333268 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 0.475      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 72        |\n",
      "|    iterations         | 2800      |\n",
      "|    time_elapsed       | 193       |\n",
      "|    total_timesteps    | 14000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13       |\n",
      "|    explained_variance | 0.0231    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2799      |\n",
      "|    policy_loss        | -43       |\n",
      "|    reward             | 1.1703646 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 14.7      |\n",
      "-------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 72           |\n",
      "|    iterations         | 2900         |\n",
      "|    time_elapsed       | 199          |\n",
      "|    total_timesteps    | 14500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -13          |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 2899         |\n",
      "|    policy_loss        | 98.9         |\n",
      "|    reward             | -0.026595993 |\n",
      "|    std                | 1.03         |\n",
      "|    value_loss         | 62.2         |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 72         |\n",
      "|    iterations         | 3000       |\n",
      "|    time_elapsed       | 206        |\n",
      "|    total_timesteps    | 15000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13        |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2999       |\n",
      "|    policy_loss        | 7.95       |\n",
      "|    reward             | -0.7281587 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 0.441      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 72         |\n",
      "|    iterations         | 3100       |\n",
      "|    time_elapsed       | 212        |\n",
      "|    total_timesteps    | 15500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13        |\n",
      "|    explained_variance | -0.0736    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3099       |\n",
      "|    policy_loss        | -23.8      |\n",
      "|    reward             | 0.55299556 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 7.66       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 73        |\n",
      "|    iterations         | 3200      |\n",
      "|    time_elapsed       | 218       |\n",
      "|    total_timesteps    | 16000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13       |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3199      |\n",
      "|    policy_loss        | 63        |\n",
      "|    reward             | 1.3132727 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 28.5      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 73          |\n",
      "|    iterations         | 3300        |\n",
      "|    time_elapsed       | 225         |\n",
      "|    total_timesteps    | 16500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13         |\n",
      "|    explained_variance | -0.0375     |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 3299        |\n",
      "|    policy_loss        | 16.1        |\n",
      "|    reward             | -0.24900278 |\n",
      "|    std                | 1.03        |\n",
      "|    value_loss         | 1.46        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 73        |\n",
      "|    iterations         | 3400      |\n",
      "|    time_elapsed       | 232       |\n",
      "|    total_timesteps    | 17000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13       |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3399      |\n",
      "|    policy_loss        | -86.7     |\n",
      "|    reward             | 1.6834834 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 76.9      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 73         |\n",
      "|    iterations         | 3500       |\n",
      "|    time_elapsed       | 237        |\n",
      "|    total_timesteps    | 17500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13        |\n",
      "|    explained_variance | 0.000396   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3499       |\n",
      "|    policy_loss        | 160        |\n",
      "|    reward             | 0.21491148 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 153        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 73         |\n",
      "|    iterations         | 3600       |\n",
      "|    time_elapsed       | 243        |\n",
      "|    total_timesteps    | 18000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13        |\n",
      "|    explained_variance | 0.0904     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3599       |\n",
      "|    policy_loss        | -8.3       |\n",
      "|    reward             | 0.42452165 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 0.463      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 73          |\n",
      "|    iterations         | 3700        |\n",
      "|    time_elapsed       | 250         |\n",
      "|    total_timesteps    | 18500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13         |\n",
      "|    explained_variance | 1.19e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 3699        |\n",
      "|    policy_loss        | 6.03        |\n",
      "|    reward             | -0.66354686 |\n",
      "|    std                | 1.03        |\n",
      "|    value_loss         | 1.45        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 74         |\n",
      "|    iterations         | 3800       |\n",
      "|    time_elapsed       | 256        |\n",
      "|    total_timesteps    | 19000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13        |\n",
      "|    explained_variance | -0.0646    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3799       |\n",
      "|    policy_loss        | 43.2       |\n",
      "|    reward             | 0.03288919 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 19         |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 74        |\n",
      "|    iterations         | 3900      |\n",
      "|    time_elapsed       | 263       |\n",
      "|    total_timesteps    | 19500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13       |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3899      |\n",
      "|    policy_loss        | 14.4      |\n",
      "|    reward             | 0.5237961 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 2.15      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 71        |\n",
      "|    iterations         | 4000      |\n",
      "|    time_elapsed       | 281       |\n",
      "|    total_timesteps    | 20000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13       |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3999      |\n",
      "|    policy_loss        | -46.2     |\n",
      "|    reward             | -5.088266 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 10.3      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 71         |\n",
      "|    iterations         | 4100       |\n",
      "|    time_elapsed       | 288        |\n",
      "|    total_timesteps    | 20500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13        |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4099       |\n",
      "|    policy_loss        | -49.2      |\n",
      "|    reward             | 0.45913658 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 20.3       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 71         |\n",
      "|    iterations         | 4200       |\n",
      "|    time_elapsed       | 294        |\n",
      "|    total_timesteps    | 21000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13        |\n",
      "|    explained_variance | 0.259      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4199       |\n",
      "|    policy_loss        | 1.19       |\n",
      "|    reward             | -1.1817889 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 0.683      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 71          |\n",
      "|    iterations         | 4300        |\n",
      "|    time_elapsed       | 300         |\n",
      "|    total_timesteps    | 21500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13         |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 4299        |\n",
      "|    policy_loss        | 23.1        |\n",
      "|    reward             | -0.57679254 |\n",
      "|    std                | 1.03        |\n",
      "|    value_loss         | 3.48        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 71         |\n",
      "|    iterations         | 4400       |\n",
      "|    time_elapsed       | 306        |\n",
      "|    total_timesteps    | 22000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13        |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4399       |\n",
      "|    policy_loss        | -23.1      |\n",
      "|    reward             | -3.3500009 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 3.23       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 71       |\n",
      "|    iterations         | 4500     |\n",
      "|    time_elapsed       | 312      |\n",
      "|    total_timesteps    | 22500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13      |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4499     |\n",
      "|    policy_loss        | 21.3     |\n",
      "|    reward             | 1.829025 |\n",
      "|    std                | 1.03     |\n",
      "|    value_loss         | 2.27     |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 71          |\n",
      "|    iterations         | 4600        |\n",
      "|    time_elapsed       | 319         |\n",
      "|    total_timesteps    | 23000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13         |\n",
      "|    explained_variance | 5.96e-08    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 4599        |\n",
      "|    policy_loss        | 37.8        |\n",
      "|    reward             | -0.16533454 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 8.94        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 72         |\n",
      "|    iterations         | 4700       |\n",
      "|    time_elapsed       | 325        |\n",
      "|    total_timesteps    | 23500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13        |\n",
      "|    explained_variance | 0.000152   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4699       |\n",
      "|    policy_loss        | 22.7       |\n",
      "|    reward             | -1.4130263 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 3.87       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 72         |\n",
      "|    iterations         | 4800       |\n",
      "|    time_elapsed       | 331        |\n",
      "|    total_timesteps    | 24000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13        |\n",
      "|    explained_variance | 0.0709     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4799       |\n",
      "|    policy_loss        | 1.41       |\n",
      "|    reward             | -0.8645081 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 0.677      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 72          |\n",
      "|    iterations         | 4900        |\n",
      "|    time_elapsed       | 337         |\n",
      "|    total_timesteps    | 24500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13         |\n",
      "|    explained_variance | 1.19e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 4899        |\n",
      "|    policy_loss        | 14.7        |\n",
      "|    reward             | -0.28661415 |\n",
      "|    std                | 1.03        |\n",
      "|    value_loss         | 1.73        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 72        |\n",
      "|    iterations         | 5000      |\n",
      "|    time_elapsed       | 343       |\n",
      "|    total_timesteps    | 25000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13       |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4999      |\n",
      "|    policy_loss        | -33.5     |\n",
      "|    reward             | 1.0013536 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 5.85      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 72         |\n",
      "|    iterations         | 5100       |\n",
      "|    time_elapsed       | 350        |\n",
      "|    total_timesteps    | 25500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13        |\n",
      "|    explained_variance | 0.231      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5099       |\n",
      "|    policy_loss        | 49.7       |\n",
      "|    reward             | 0.13557048 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 15.7       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 72        |\n",
      "|    iterations         | 5200      |\n",
      "|    time_elapsed       | 356       |\n",
      "|    total_timesteps    | 26000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13       |\n",
      "|    explained_variance | 0.156     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5199      |\n",
      "|    policy_loss        | 22.8      |\n",
      "|    reward             | 0.6733781 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 5.4       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 73        |\n",
      "|    iterations         | 5300      |\n",
      "|    time_elapsed       | 362       |\n",
      "|    total_timesteps    | 26500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13       |\n",
      "|    explained_variance | -0.818    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5299      |\n",
      "|    policy_loss        | 48.7      |\n",
      "|    reward             | -5.778872 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 21.2      |\n",
      "-------------------------------------\n",
      "day: 2956, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 5468045.10\n",
      "total_reward: 4468045.10\n",
      "total_cost: 42360.50\n",
      "total_trades: 19007\n",
      "Sharpe: 0.879\n",
      "=================================\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 73         |\n",
      "|    iterations         | 5400       |\n",
      "|    time_elapsed       | 368        |\n",
      "|    total_timesteps    | 27000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13        |\n",
      "|    explained_variance | 0.0607     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5399       |\n",
      "|    policy_loss        | -33        |\n",
      "|    reward             | -1.0020543 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 5.91       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 73        |\n",
      "|    iterations         | 5500      |\n",
      "|    time_elapsed       | 374       |\n",
      "|    total_timesteps    | 27500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13       |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5499      |\n",
      "|    policy_loss        | 16.7      |\n",
      "|    reward             | 0.5006968 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 2.12      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 73          |\n",
      "|    iterations         | 5600        |\n",
      "|    time_elapsed       | 382         |\n",
      "|    total_timesteps    | 28000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.1       |\n",
      "|    explained_variance | 0.21        |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 5599        |\n",
      "|    policy_loss        | -19.1       |\n",
      "|    reward             | -0.11048869 |\n",
      "|    std                | 1.04        |\n",
      "|    value_loss         | 8.43        |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 73           |\n",
      "|    iterations         | 5700         |\n",
      "|    time_elapsed       | 388          |\n",
      "|    total_timesteps    | 28500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -13.2        |\n",
      "|    explained_variance | -0.659       |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 5699         |\n",
      "|    policy_loss        | -2.57        |\n",
      "|    reward             | -0.031228244 |\n",
      "|    std                | 1.05         |\n",
      "|    value_loss         | 1.43         |\n",
      "----------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 73       |\n",
      "|    iterations         | 5800     |\n",
      "|    time_elapsed       | 394      |\n",
      "|    total_timesteps    | 29000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13.2    |\n",
      "|    explained_variance | -0.0923  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5799     |\n",
      "|    policy_loss        | 75.8     |\n",
      "|    reward             | 1.055681 |\n",
      "|    std                | 1.05     |\n",
      "|    value_loss         | 36       |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 73          |\n",
      "|    iterations         | 5900        |\n",
      "|    time_elapsed       | 400         |\n",
      "|    total_timesteps    | 29500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.2       |\n",
      "|    explained_variance | -0.19       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 5899        |\n",
      "|    policy_loss        | -10.2       |\n",
      "|    reward             | -0.97425014 |\n",
      "|    std                | 1.05        |\n",
      "|    value_loss         | 4.97        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 73        |\n",
      "|    iterations         | 6000      |\n",
      "|    time_elapsed       | 407       |\n",
      "|    total_timesteps    | 30000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.2     |\n",
      "|    explained_variance | -0.023    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5999      |\n",
      "|    policy_loss        | -41.2     |\n",
      "|    reward             | 0.5575017 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 13.9      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 73        |\n",
      "|    iterations         | 6100      |\n",
      "|    time_elapsed       | 413       |\n",
      "|    total_timesteps    | 30500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.2     |\n",
      "|    explained_variance | 0.362     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6099      |\n",
      "|    policy_loss        | 7.99      |\n",
      "|    reward             | 0.2786446 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 0.802     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 73         |\n",
      "|    iterations         | 6200       |\n",
      "|    time_elapsed       | 419        |\n",
      "|    total_timesteps    | 31000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.2      |\n",
      "|    explained_variance | 0.0102     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6199       |\n",
      "|    policy_loss        | -92.8      |\n",
      "|    reward             | -3.0249734 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 67.9       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 74          |\n",
      "|    iterations         | 6300        |\n",
      "|    time_elapsed       | 425         |\n",
      "|    total_timesteps    | 31500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.2       |\n",
      "|    explained_variance | -0.293      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 6299        |\n",
      "|    policy_loss        | 7.29        |\n",
      "|    reward             | 0.111356996 |\n",
      "|    std                | 1.05        |\n",
      "|    value_loss         | 2.11        |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 74       |\n",
      "|    iterations         | 6400     |\n",
      "|    time_elapsed       | 431      |\n",
      "|    total_timesteps    | 32000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13.2    |\n",
      "|    explained_variance | 0.00236  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6399     |\n",
      "|    policy_loss        | 16.8     |\n",
      "|    reward             | 3.724288 |\n",
      "|    std                | 1.05     |\n",
      "|    value_loss         | 9.62     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 74        |\n",
      "|    iterations         | 6500      |\n",
      "|    time_elapsed       | 437       |\n",
      "|    total_timesteps    | 32500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6499      |\n",
      "|    policy_loss        | 40.5      |\n",
      "|    reward             | 0.4451007 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 11.5      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 74         |\n",
      "|    iterations         | 6600       |\n",
      "|    time_elapsed       | 445        |\n",
      "|    total_timesteps    | 33000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.1      |\n",
      "|    explained_variance | -0.00529   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6599       |\n",
      "|    policy_loss        | -41.8      |\n",
      "|    reward             | -1.8544033 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 11.9       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 74        |\n",
      "|    iterations         | 6700      |\n",
      "|    time_elapsed       | 451       |\n",
      "|    total_timesteps    | 33500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.1     |\n",
      "|    explained_variance | -0.0227   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6699      |\n",
      "|    policy_loss        | 8.17      |\n",
      "|    reward             | 1.0850563 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 0.523     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 74         |\n",
      "|    iterations         | 6800       |\n",
      "|    time_elapsed       | 458        |\n",
      "|    total_timesteps    | 34000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.1      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6799       |\n",
      "|    policy_loss        | 5.85       |\n",
      "|    reward             | -1.2595433 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 0.759      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 74        |\n",
      "|    iterations         | 6900      |\n",
      "|    time_elapsed       | 464       |\n",
      "|    total_timesteps    | 34500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.1     |\n",
      "|    explained_variance | -0.0229   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6899      |\n",
      "|    policy_loss        | 0.0688    |\n",
      "|    reward             | 1.1887996 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 0.629     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 74         |\n",
      "|    iterations         | 7000       |\n",
      "|    time_elapsed       | 470        |\n",
      "|    total_timesteps    | 35000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6999       |\n",
      "|    policy_loss        | 20.5       |\n",
      "|    reward             | 0.24672818 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 4.22       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 74          |\n",
      "|    iterations         | 7100        |\n",
      "|    time_elapsed       | 477         |\n",
      "|    total_timesteps    | 35500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.2       |\n",
      "|    explained_variance | 5.96e-08    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 7099        |\n",
      "|    policy_loss        | -1.15       |\n",
      "|    reward             | 0.010336651 |\n",
      "|    std                | 1.05        |\n",
      "|    value_loss         | 0.00798     |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 74           |\n",
      "|    iterations         | 7200         |\n",
      "|    time_elapsed       | 483          |\n",
      "|    total_timesteps    | 36000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -13.2        |\n",
      "|    explained_variance | -0.329       |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 7199         |\n",
      "|    policy_loss        | 2.13         |\n",
      "|    reward             | -0.093657985 |\n",
      "|    std                | 1.05         |\n",
      "|    value_loss         | 0.294        |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 74         |\n",
      "|    iterations         | 7300       |\n",
      "|    time_elapsed       | 489        |\n",
      "|    total_timesteps    | 36500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.1      |\n",
      "|    explained_variance | -0.0048    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7299       |\n",
      "|    policy_loss        | -3.79      |\n",
      "|    reward             | 0.04760448 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 0.141      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 74         |\n",
      "|    iterations         | 7400       |\n",
      "|    time_elapsed       | 495        |\n",
      "|    total_timesteps    | 37000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.1      |\n",
      "|    explained_variance | -0.0837    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7399       |\n",
      "|    policy_loss        | -33        |\n",
      "|    reward             | 0.53063565 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 7.81       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 74        |\n",
      "|    iterations         | 7500      |\n",
      "|    time_elapsed       | 501       |\n",
      "|    total_timesteps    | 37500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.2     |\n",
      "|    explained_variance | -0.0656   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7499      |\n",
      "|    policy_loss        | 7.66      |\n",
      "|    reward             | 0.5570429 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 0.757     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 74        |\n",
      "|    iterations         | 7600      |\n",
      "|    time_elapsed       | 507       |\n",
      "|    total_timesteps    | 38000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7599      |\n",
      "|    policy_loss        | -0.702    |\n",
      "|    reward             | 1.1993946 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 0.111     |\n",
      "-------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 74           |\n",
      "|    iterations         | 7700         |\n",
      "|    time_elapsed       | 513          |\n",
      "|    total_timesteps    | 38500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -13.2        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 7699         |\n",
      "|    policy_loss        | 0.875        |\n",
      "|    reward             | -0.010698829 |\n",
      "|    std                | 1.05         |\n",
      "|    value_loss         | 0.0168       |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 7800       |\n",
      "|    time_elapsed       | 519        |\n",
      "|    total_timesteps    | 39000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.1      |\n",
      "|    explained_variance | 0.0341     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7799       |\n",
      "|    policy_loss        | 18.2       |\n",
      "|    reward             | -1.5604169 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 2.42       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 75        |\n",
      "|    iterations         | 7900      |\n",
      "|    time_elapsed       | 525       |\n",
      "|    total_timesteps    | 39500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.1     |\n",
      "|    explained_variance | -0.0156   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7899      |\n",
      "|    policy_loss        | -40.5     |\n",
      "|    reward             | 2.0190706 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 10.3      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 8000       |\n",
      "|    time_elapsed       | 531        |\n",
      "|    total_timesteps    | 40000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7999       |\n",
      "|    policy_loss        | -25.8      |\n",
      "|    reward             | -1.3485199 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 4.83       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 75       |\n",
      "|    iterations         | 8100     |\n",
      "|    time_elapsed       | 538      |\n",
      "|    total_timesteps    | 40500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13.1    |\n",
      "|    explained_variance | -0.0641  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8099     |\n",
      "|    policy_loss        | -65.6    |\n",
      "|    reward             | 5.454387 |\n",
      "|    std                | 1.04     |\n",
      "|    value_loss         | 42.8     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 8200       |\n",
      "|    time_elapsed       | 544        |\n",
      "|    total_timesteps    | 41000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.2      |\n",
      "|    explained_variance | -0.0022    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8199       |\n",
      "|    policy_loss        | -351       |\n",
      "|    reward             | -11.817917 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 905        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 8300       |\n",
      "|    time_elapsed       | 550        |\n",
      "|    total_timesteps    | 41500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8299       |\n",
      "|    policy_loss        | -27.1      |\n",
      "|    reward             | -1.6882619 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 4.82       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 8400       |\n",
      "|    time_elapsed       | 556        |\n",
      "|    total_timesteps    | 42000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8399       |\n",
      "|    policy_loss        | -49.1      |\n",
      "|    reward             | 0.46806332 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 21         |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 75        |\n",
      "|    iterations         | 8500      |\n",
      "|    time_elapsed       | 562       |\n",
      "|    total_timesteps    | 42500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.2     |\n",
      "|    explained_variance | 0.0992    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8499      |\n",
      "|    policy_loss        | 0.0366    |\n",
      "|    reward             | 1.6918793 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 0.61      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 75        |\n",
      "|    iterations         | 8600      |\n",
      "|    time_elapsed       | 568       |\n",
      "|    total_timesteps    | 43000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.2     |\n",
      "|    explained_variance | -0.0783   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8599      |\n",
      "|    policy_loss        | -31.6     |\n",
      "|    reward             | 1.5744715 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 10.4      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 75        |\n",
      "|    iterations         | 8700      |\n",
      "|    time_elapsed       | 574       |\n",
      "|    total_timesteps    | 43500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.2     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8699      |\n",
      "|    policy_loss        | -38.8     |\n",
      "|    reward             | 1.5819206 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 12.2      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 8800       |\n",
      "|    time_elapsed       | 580        |\n",
      "|    total_timesteps    | 44000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.2      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8799       |\n",
      "|    policy_loss        | 117        |\n",
      "|    reward             | -3.3188276 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 117        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 75        |\n",
      "|    iterations         | 8900      |\n",
      "|    time_elapsed       | 587       |\n",
      "|    total_timesteps    | 44500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.2     |\n",
      "|    explained_variance | -5.19     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8899      |\n",
      "|    policy_loss        | 35.9      |\n",
      "|    reward             | 2.1970038 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 14.3      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 75        |\n",
      "|    iterations         | 9000      |\n",
      "|    time_elapsed       | 593       |\n",
      "|    total_timesteps    | 45000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.2     |\n",
      "|    explained_variance | 0.0726    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8999      |\n",
      "|    policy_loss        | 17.6      |\n",
      "|    reward             | 0.2795435 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 3.06      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 75        |\n",
      "|    iterations         | 9100      |\n",
      "|    time_elapsed       | 599       |\n",
      "|    total_timesteps    | 45500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.2     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9099      |\n",
      "|    policy_loss        | -3.37     |\n",
      "|    reward             | 0.9214082 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 0.181     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 9200       |\n",
      "|    time_elapsed       | 605        |\n",
      "|    total_timesteps    | 46000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9199       |\n",
      "|    policy_loss        | 43.4       |\n",
      "|    reward             | 0.69018614 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 12.3       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 9300      |\n",
      "|    time_elapsed       | 611       |\n",
      "|    total_timesteps    | 46500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9299      |\n",
      "|    policy_loss        | 19.7      |\n",
      "|    reward             | 1.5708824 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 2.59      |\n",
      "-------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 76           |\n",
      "|    iterations         | 9400         |\n",
      "|    time_elapsed       | 617          |\n",
      "|    total_timesteps    | 47000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -13.2        |\n",
      "|    explained_variance | 0.023        |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 9399         |\n",
      "|    policy_loss        | 41.7         |\n",
      "|    reward             | -0.024290608 |\n",
      "|    std                | 1.05         |\n",
      "|    value_loss         | 24.9         |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 9500       |\n",
      "|    time_elapsed       | 624        |\n",
      "|    total_timesteps    | 47500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9499       |\n",
      "|    policy_loss        | 10.9       |\n",
      "|    reward             | 0.70214844 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 1.15       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 9600       |\n",
      "|    time_elapsed       | 630        |\n",
      "|    total_timesteps    | 48000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.2      |\n",
      "|    explained_variance | 0.24       |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9599       |\n",
      "|    policy_loss        | 5.73       |\n",
      "|    reward             | -1.2443671 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 1.12       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 9700       |\n",
      "|    time_elapsed       | 636        |\n",
      "|    total_timesteps    | 48500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9699       |\n",
      "|    policy_loss        | 10.3       |\n",
      "|    reward             | -1.2407321 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 0.999      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 76          |\n",
      "|    iterations         | 9800        |\n",
      "|    time_elapsed       | 643         |\n",
      "|    total_timesteps    | 49000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.2       |\n",
      "|    explained_variance | -0.353      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 9799        |\n",
      "|    policy_loss        | -60         |\n",
      "|    reward             | -0.48191452 |\n",
      "|    std                | 1.05        |\n",
      "|    value_loss         | 22          |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 9900       |\n",
      "|    time_elapsed       | 649        |\n",
      "|    total_timesteps    | 49500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.2      |\n",
      "|    explained_variance | -0.182     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9899       |\n",
      "|    policy_loss        | -2.56      |\n",
      "|    reward             | -0.0729871 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 1.26       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 10000     |\n",
      "|    time_elapsed       | 654       |\n",
      "|    total_timesteps    | 50000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.2     |\n",
      "|    explained_variance | -0.075    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9999      |\n",
      "|    policy_loss        | 0.735     |\n",
      "|    reward             | -5.542665 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 1.65      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 10100     |\n",
      "|    time_elapsed       | 661       |\n",
      "|    total_timesteps    | 50500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.2     |\n",
      "|    explained_variance | 0.0359    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 10099     |\n",
      "|    policy_loss        | 4.61      |\n",
      "|    reward             | 1.5915169 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 0.943     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 10200     |\n",
      "|    time_elapsed       | 670       |\n",
      "|    total_timesteps    | 51000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.2     |\n",
      "|    explained_variance | 0.0269    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 10199     |\n",
      "|    policy_loss        | 20.1      |\n",
      "|    reward             | 0.5230249 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 3.83      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 76       |\n",
      "|    iterations         | 10300    |\n",
      "|    time_elapsed       | 676      |\n",
      "|    total_timesteps    | 51500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13.2    |\n",
      "|    explained_variance | 0.0554   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 10299    |\n",
      "|    policy_loss        | -20.3    |\n",
      "|    reward             | 1.183543 |\n",
      "|    std                | 1.06     |\n",
      "|    value_loss         | 2.99     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 10400      |\n",
      "|    time_elapsed       | 682        |\n",
      "|    total_timesteps    | 52000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.2      |\n",
      "|    explained_variance | -0.00719   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 10399      |\n",
      "|    policy_loss        | 91.6       |\n",
      "|    reward             | -1.4333048 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 50.3       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 10500      |\n",
      "|    time_elapsed       | 688        |\n",
      "|    total_timesteps    | 52500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.2      |\n",
      "|    explained_variance | -8.34e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 10499      |\n",
      "|    policy_loss        | 40.6       |\n",
      "|    reward             | -3.7756493 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 14.6       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 76          |\n",
      "|    iterations         | 10600       |\n",
      "|    time_elapsed       | 694         |\n",
      "|    total_timesteps    | 53000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.2       |\n",
      "|    explained_variance | -0.0191     |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 10599       |\n",
      "|    policy_loss        | -119        |\n",
      "|    reward             | 0.011418285 |\n",
      "|    std                | 1.06        |\n",
      "|    value_loss         | 158         |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 10700      |\n",
      "|    time_elapsed       | 700        |\n",
      "|    total_timesteps    | 53500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.2      |\n",
      "|    explained_variance | -0.0289    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 10699      |\n",
      "|    policy_loss        | 8.26       |\n",
      "|    reward             | 0.33318558 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 0.571      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 10800      |\n",
      "|    time_elapsed       | 706        |\n",
      "|    total_timesteps    | 54000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 10799      |\n",
      "|    policy_loss        | 12.5       |\n",
      "|    reward             | -0.2455679 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 1.09       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 10900      |\n",
      "|    time_elapsed       | 712        |\n",
      "|    total_timesteps    | 54500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.3      |\n",
      "|    explained_variance | -0.0244    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 10899      |\n",
      "|    policy_loss        | 8.18       |\n",
      "|    reward             | -1.7909051 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 3.31       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 76          |\n",
      "|    iterations         | 11000       |\n",
      "|    time_elapsed       | 718         |\n",
      "|    total_timesteps    | 55000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.3       |\n",
      "|    explained_variance | 0.12        |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 10999       |\n",
      "|    policy_loss        | -19.3       |\n",
      "|    reward             | -0.40534797 |\n",
      "|    std                | 1.07        |\n",
      "|    value_loss         | 2.48        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 11100      |\n",
      "|    time_elapsed       | 724        |\n",
      "|    total_timesteps    | 55500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.3      |\n",
      "|    explained_variance | -0.0104    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 11099      |\n",
      "|    policy_loss        | 68.3       |\n",
      "|    reward             | 0.37180954 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 45.6       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 11200     |\n",
      "|    time_elapsed       | 730       |\n",
      "|    total_timesteps    | 56000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.3     |\n",
      "|    explained_variance | -0.0405   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 11199     |\n",
      "|    policy_loss        | 21.5      |\n",
      "|    reward             | 1.8709099 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 7.7       |\n",
      "-------------------------------------\n",
      "day: 2956, episode: 20\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4770552.75\n",
      "total_reward: 3770552.75\n",
      "total_cost: 31306.30\n",
      "total_trades: 12696\n",
      "Sharpe: 0.860\n",
      "=================================\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 76          |\n",
      "|    iterations         | 11300       |\n",
      "|    time_elapsed       | 736         |\n",
      "|    total_timesteps    | 56500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.2       |\n",
      "|    explained_variance | 0.0208      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 11299       |\n",
      "|    policy_loss        | 17.8        |\n",
      "|    reward             | -0.29421088 |\n",
      "|    std                | 1.06        |\n",
      "|    value_loss         | 1.88        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 11400     |\n",
      "|    time_elapsed       | 742       |\n",
      "|    total_timesteps    | 57000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.3     |\n",
      "|    explained_variance | -0.0827   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 11399     |\n",
      "|    policy_loss        | -0.893    |\n",
      "|    reward             | 1.0560887 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 0.376     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 76       |\n",
      "|    iterations         | 11500    |\n",
      "|    time_elapsed       | 748      |\n",
      "|    total_timesteps    | 57500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13.3    |\n",
      "|    explained_variance | 0.0221   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 11499    |\n",
      "|    policy_loss        | 7.85     |\n",
      "|    reward             | 2.673363 |\n",
      "|    std                | 1.06     |\n",
      "|    value_loss         | 1.16     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 11600      |\n",
      "|    time_elapsed       | 754        |\n",
      "|    total_timesteps    | 58000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 11599      |\n",
      "|    policy_loss        | -10.6      |\n",
      "|    reward             | -0.3240054 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 1.05       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 11700      |\n",
      "|    time_elapsed       | 760        |\n",
      "|    total_timesteps    | 58500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.3      |\n",
      "|    explained_variance | 0.156      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 11699      |\n",
      "|    policy_loss        | 61.6       |\n",
      "|    reward             | -3.0792108 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 21.9       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 76          |\n",
      "|    iterations         | 11800       |\n",
      "|    time_elapsed       | 766         |\n",
      "|    total_timesteps    | 59000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.3       |\n",
      "|    explained_variance | -0.00256    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 11799       |\n",
      "|    policy_loss        | 60.2        |\n",
      "|    reward             | -0.15675366 |\n",
      "|    std                | 1.07        |\n",
      "|    value_loss         | 25.7        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 77          |\n",
      "|    iterations         | 11900       |\n",
      "|    time_elapsed       | 772         |\n",
      "|    total_timesteps    | 59500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.3       |\n",
      "|    explained_variance | -0.222      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 11899       |\n",
      "|    policy_loss        | -24.8       |\n",
      "|    reward             | -0.45577008 |\n",
      "|    std                | 1.07        |\n",
      "|    value_loss         | 3.97        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 77         |\n",
      "|    iterations         | 12000      |\n",
      "|    time_elapsed       | 778        |\n",
      "|    total_timesteps    | 60000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.3      |\n",
      "|    explained_variance | -0.00136   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 11999      |\n",
      "|    policy_loss        | -20.4      |\n",
      "|    reward             | -2.1067014 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 4.29       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 77         |\n",
      "|    iterations         | 12100      |\n",
      "|    time_elapsed       | 784        |\n",
      "|    total_timesteps    | 60500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.3      |\n",
      "|    explained_variance | 0.0571     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 12099      |\n",
      "|    policy_loss        | -6.19      |\n",
      "|    reward             | 0.50400954 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 0.52       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 77         |\n",
      "|    iterations         | 12200      |\n",
      "|    time_elapsed       | 790        |\n",
      "|    total_timesteps    | 61000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.3      |\n",
      "|    explained_variance | -0.104     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 12199      |\n",
      "|    policy_loss        | -74.5      |\n",
      "|    reward             | 0.30541435 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 30.6       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 77        |\n",
      "|    iterations         | 12300     |\n",
      "|    time_elapsed       | 796       |\n",
      "|    total_timesteps    | 61500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.3     |\n",
      "|    explained_variance | 0.00282   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 12299     |\n",
      "|    policy_loss        | -38.3     |\n",
      "|    reward             | 2.4618053 |\n",
      "|    std                | 1.07      |\n",
      "|    value_loss         | 12.2      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 77        |\n",
      "|    iterations         | 12400     |\n",
      "|    time_elapsed       | 802       |\n",
      "|    total_timesteps    | 62000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.3     |\n",
      "|    explained_variance | -0.0138   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 12399     |\n",
      "|    policy_loss        | 70.4      |\n",
      "|    reward             | 4.9061456 |\n",
      "|    std                | 1.07      |\n",
      "|    value_loss         | 42.6      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 77        |\n",
      "|    iterations         | 12500     |\n",
      "|    time_elapsed       | 809       |\n",
      "|    total_timesteps    | 62500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.3     |\n",
      "|    explained_variance | -0.131    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 12499     |\n",
      "|    policy_loss        | -65.5     |\n",
      "|    reward             | 4.9222755 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 21.4      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 77         |\n",
      "|    iterations         | 12600      |\n",
      "|    time_elapsed       | 815        |\n",
      "|    total_timesteps    | 63000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.3      |\n",
      "|    explained_variance | -0.0895    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 12599      |\n",
      "|    policy_loss        | -2.06      |\n",
      "|    reward             | -1.0091106 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 0.536      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 77         |\n",
      "|    iterations         | 12700      |\n",
      "|    time_elapsed       | 821        |\n",
      "|    total_timesteps    | 63500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.3      |\n",
      "|    explained_variance | -0.26      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 12699      |\n",
      "|    policy_loss        | -43.2      |\n",
      "|    reward             | 0.95055896 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 13.1       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 77         |\n",
      "|    iterations         | 12800      |\n",
      "|    time_elapsed       | 828        |\n",
      "|    total_timesteps    | 64000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 12799      |\n",
      "|    policy_loss        | 8.23       |\n",
      "|    reward             | 0.30120644 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 0.544      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 77        |\n",
      "|    iterations         | 12900     |\n",
      "|    time_elapsed       | 834       |\n",
      "|    total_timesteps    | 64500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.3     |\n",
      "|    explained_variance | -0.0625   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 12899     |\n",
      "|    policy_loss        | -27.8     |\n",
      "|    reward             | 1.3236394 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 5.07      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 77         |\n",
      "|    iterations         | 13000      |\n",
      "|    time_elapsed       | 840        |\n",
      "|    total_timesteps    | 65000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.3      |\n",
      "|    explained_variance | 0.00613    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 12999      |\n",
      "|    policy_loss        | 16.3       |\n",
      "|    reward             | -0.7705507 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 4.88       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 77        |\n",
      "|    iterations         | 13100     |\n",
      "|    time_elapsed       | 845       |\n",
      "|    total_timesteps    | 65500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.2     |\n",
      "|    explained_variance | 0.0092    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 13099     |\n",
      "|    policy_loss        | -63.5     |\n",
      "|    reward             | 3.1844978 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 23.4      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 77         |\n",
      "|    iterations         | 13200      |\n",
      "|    time_elapsed       | 851        |\n",
      "|    total_timesteps    | 66000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.2      |\n",
      "|    explained_variance | 5.96e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 13199      |\n",
      "|    policy_loss        | -23.2      |\n",
      "|    reward             | -1.4754494 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 3.71       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 77        |\n",
      "|    iterations         | 13300     |\n",
      "|    time_elapsed       | 858       |\n",
      "|    total_timesteps    | 66500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.2     |\n",
      "|    explained_variance | 0.0121    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 13299     |\n",
      "|    policy_loss        | -55.5     |\n",
      "|    reward             | 1.1721098 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 25.9      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 77         |\n",
      "|    iterations         | 13400      |\n",
      "|    time_elapsed       | 864        |\n",
      "|    total_timesteps    | 67000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.2      |\n",
      "|    explained_variance | -0.0905    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 13399      |\n",
      "|    policy_loss        | -28.8      |\n",
      "|    reward             | -0.3575538 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 5.21       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 77         |\n",
      "|    iterations         | 13500      |\n",
      "|    time_elapsed       | 870        |\n",
      "|    total_timesteps    | 67500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 13499      |\n",
      "|    policy_loss        | 3.21       |\n",
      "|    reward             | -1.2960054 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 2.18       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 77         |\n",
      "|    iterations         | 13600      |\n",
      "|    time_elapsed       | 876        |\n",
      "|    total_timesteps    | 68000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.2      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 13599      |\n",
      "|    policy_loss        | -68.5      |\n",
      "|    reward             | -1.1597328 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 36.4       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 77        |\n",
      "|    iterations         | 13700     |\n",
      "|    time_elapsed       | 882       |\n",
      "|    total_timesteps    | 68500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.2     |\n",
      "|    explained_variance | 0.0148    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 13699     |\n",
      "|    policy_loss        | 27.4      |\n",
      "|    reward             | 1.7770736 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 10.1      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 77        |\n",
      "|    iterations         | 13800     |\n",
      "|    time_elapsed       | 888       |\n",
      "|    total_timesteps    | 69000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.2     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 13799     |\n",
      "|    policy_loss        | -5.06     |\n",
      "|    reward             | 2.4673777 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 0.8       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 77        |\n",
      "|    iterations         | 13900     |\n",
      "|    time_elapsed       | 894       |\n",
      "|    total_timesteps    | 69500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 13899     |\n",
      "|    policy_loss        | 1.06      |\n",
      "|    reward             | -2.243015 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 2.43      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 77         |\n",
      "|    iterations         | 14000      |\n",
      "|    time_elapsed       | 900        |\n",
      "|    total_timesteps    | 70000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 13999      |\n",
      "|    policy_loss        | -5.83      |\n",
      "|    reward             | 0.26173937 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 0.639      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 77         |\n",
      "|    iterations         | 14100      |\n",
      "|    time_elapsed       | 906        |\n",
      "|    total_timesteps    | 70500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.2      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 14099      |\n",
      "|    policy_loss        | 12         |\n",
      "|    reward             | 0.38283238 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 1.42       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 77         |\n",
      "|    iterations         | 14200      |\n",
      "|    time_elapsed       | 912        |\n",
      "|    total_timesteps    | 71000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 14199      |\n",
      "|    policy_loss        | 2.38       |\n",
      "|    reward             | 0.11894945 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 0.0365     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 77         |\n",
      "|    iterations         | 14300      |\n",
      "|    time_elapsed       | 919        |\n",
      "|    total_timesteps    | 71500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.3      |\n",
      "|    explained_variance | 0.0803     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 14299      |\n",
      "|    policy_loss        | 0.724      |\n",
      "|    reward             | 0.37996796 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 0.264      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 77        |\n",
      "|    iterations         | 14400     |\n",
      "|    time_elapsed       | 925       |\n",
      "|    total_timesteps    | 72000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.3     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 14399     |\n",
      "|    policy_loss        | -29.2     |\n",
      "|    reward             | 1.1076304 |\n",
      "|    std                | 1.07      |\n",
      "|    value_loss         | 6.31      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 77         |\n",
      "|    iterations         | 14500      |\n",
      "|    time_elapsed       | 930        |\n",
      "|    total_timesteps    | 72500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 14499      |\n",
      "|    policy_loss        | -13.6      |\n",
      "|    reward             | -0.3732308 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 2.29       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 77        |\n",
      "|    iterations         | 14600     |\n",
      "|    time_elapsed       | 940       |\n",
      "|    total_timesteps    | 73000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.3     |\n",
      "|    explained_variance | -0.0332   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 14599     |\n",
      "|    policy_loss        | 26.1      |\n",
      "|    reward             | -4.176939 |\n",
      "|    std                | 1.07      |\n",
      "|    value_loss         | 5.96      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 77         |\n",
      "|    iterations         | 14700      |\n",
      "|    time_elapsed       | 948        |\n",
      "|    total_timesteps    | 73500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 14699      |\n",
      "|    policy_loss        | 44.9       |\n",
      "|    reward             | -3.4714167 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 16.8       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 77        |\n",
      "|    iterations         | 14800     |\n",
      "|    time_elapsed       | 957       |\n",
      "|    total_timesteps    | 74000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 14799     |\n",
      "|    policy_loss        | 3.99      |\n",
      "|    reward             | 0.2546564 |\n",
      "|    std                | 1.07      |\n",
      "|    value_loss         | 0.123     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 77       |\n",
      "|    iterations         | 14900    |\n",
      "|    time_elapsed       | 965      |\n",
      "|    total_timesteps    | 74500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13.3    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 14899    |\n",
      "|    policy_loss        | -32.2    |\n",
      "|    reward             | 0.440492 |\n",
      "|    std                | 1.07     |\n",
      "|    value_loss         | 7.3      |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 77         |\n",
      "|    iterations         | 15000      |\n",
      "|    time_elapsed       | 973        |\n",
      "|    total_timesteps    | 75000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 14999      |\n",
      "|    policy_loss        | -35.4      |\n",
      "|    reward             | -1.7668883 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 6.66       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 77         |\n",
      "|    iterations         | 15100      |\n",
      "|    time_elapsed       | 980        |\n",
      "|    total_timesteps    | 75500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 15099      |\n",
      "|    policy_loss        | 1.05       |\n",
      "|    reward             | -2.8839684 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 0.782      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 15200     |\n",
      "|    time_elapsed       | 989       |\n",
      "|    total_timesteps    | 76000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15199     |\n",
      "|    policy_loss        | -63.6     |\n",
      "|    reward             | 4.3814607 |\n",
      "|    std                | 1.08      |\n",
      "|    value_loss         | 74.7      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 15300      |\n",
      "|    time_elapsed       | 998        |\n",
      "|    total_timesteps    | 76500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.4      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 15299      |\n",
      "|    policy_loss        | -298       |\n",
      "|    reward             | -7.1272397 |\n",
      "|    std                | 1.08       |\n",
      "|    value_loss         | 668        |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 76          |\n",
      "|    iterations         | 15400       |\n",
      "|    time_elapsed       | 1006        |\n",
      "|    total_timesteps    | 77000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.4       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 15399       |\n",
      "|    policy_loss        | 16.4        |\n",
      "|    reward             | -0.29912883 |\n",
      "|    std                | 1.08        |\n",
      "|    value_loss         | 2           |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 15500     |\n",
      "|    time_elapsed       | 1013      |\n",
      "|    total_timesteps    | 77500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.4     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15499     |\n",
      "|    policy_loss        | 28.8      |\n",
      "|    reward             | 0.7915404 |\n",
      "|    std                | 1.08      |\n",
      "|    value_loss         | 5.87      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 76          |\n",
      "|    iterations         | 15600       |\n",
      "|    time_elapsed       | 1020        |\n",
      "|    total_timesteps    | 78000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.4       |\n",
      "|    explained_variance | 0.0545      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 15599       |\n",
      "|    policy_loss        | 3.54        |\n",
      "|    reward             | -0.14455001 |\n",
      "|    std                | 1.08        |\n",
      "|    value_loss         | 0.18        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 15700     |\n",
      "|    time_elapsed       | 1026      |\n",
      "|    total_timesteps    | 78500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.4     |\n",
      "|    explained_variance | 0.0238    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15699     |\n",
      "|    policy_loss        | 20.2      |\n",
      "|    reward             | 0.7927871 |\n",
      "|    std                | 1.08      |\n",
      "|    value_loss         | 3.3       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 15800     |\n",
      "|    time_elapsed       | 1032      |\n",
      "|    total_timesteps    | 79000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.4     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15799     |\n",
      "|    policy_loss        | -16.5     |\n",
      "|    reward             | 1.4739289 |\n",
      "|    std                | 1.08      |\n",
      "|    value_loss         | 1.53      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 15900     |\n",
      "|    time_elapsed       | 1039      |\n",
      "|    total_timesteps    | 79500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.4     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15899     |\n",
      "|    policy_loss        | -16.8     |\n",
      "|    reward             | 5.0755477 |\n",
      "|    std                | 1.08      |\n",
      "|    value_loss         | 10.6      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 76          |\n",
      "|    iterations         | 16000       |\n",
      "|    time_elapsed       | 1045        |\n",
      "|    total_timesteps    | 80000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.4       |\n",
      "|    explained_variance | 1.19e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 15999       |\n",
      "|    policy_loss        | 0.547       |\n",
      "|    reward             | -0.75487095 |\n",
      "|    std                | 1.08        |\n",
      "|    value_loss         | 1.08        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 16100      |\n",
      "|    time_elapsed       | 1051       |\n",
      "|    total_timesteps    | 80500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.4      |\n",
      "|    explained_variance | -0.14      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 16099      |\n",
      "|    policy_loss        | -0.66      |\n",
      "|    reward             | 0.16080084 |\n",
      "|    std                | 1.08       |\n",
      "|    value_loss         | 0.525      |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 76       |\n",
      "|    iterations         | 16200    |\n",
      "|    time_elapsed       | 1058     |\n",
      "|    total_timesteps    | 81000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13.4    |\n",
      "|    explained_variance | 0.00306  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 16199    |\n",
      "|    policy_loss        | -52.5    |\n",
      "|    reward             | 2.187001 |\n",
      "|    std                | 1.08     |\n",
      "|    value_loss         | 16.2     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 16300     |\n",
      "|    time_elapsed       | 1064      |\n",
      "|    total_timesteps    | 81500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16299     |\n",
      "|    policy_loss        | -0.23     |\n",
      "|    reward             | 0.6329717 |\n",
      "|    std                | 1.08      |\n",
      "|    value_loss         | 0.688     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 76       |\n",
      "|    iterations         | 16400    |\n",
      "|    time_elapsed       | 1070     |\n",
      "|    total_timesteps    | 82000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13.4    |\n",
      "|    explained_variance | 0.0111   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 16399    |\n",
      "|    policy_loss        | -23.4    |\n",
      "|    reward             | 1.839602 |\n",
      "|    std                | 1.08     |\n",
      "|    value_loss         | 12       |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 16500      |\n",
      "|    time_elapsed       | 1077       |\n",
      "|    total_timesteps    | 82500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.4      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 16499      |\n",
      "|    policy_loss        | -34.5      |\n",
      "|    reward             | -0.8781769 |\n",
      "|    std                | 1.08       |\n",
      "|    value_loss         | 18.2       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 76          |\n",
      "|    iterations         | 16600       |\n",
      "|    time_elapsed       | 1083        |\n",
      "|    total_timesteps    | 83000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.4       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 16599       |\n",
      "|    policy_loss        | 1.12        |\n",
      "|    reward             | 0.004907127 |\n",
      "|    std                | 1.08        |\n",
      "|    value_loss         | 0.0483      |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 16700      |\n",
      "|    time_elapsed       | 1089       |\n",
      "|    total_timesteps    | 83500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.4      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 16699      |\n",
      "|    policy_loss        | -39.5      |\n",
      "|    reward             | 0.14656435 |\n",
      "|    std                | 1.08       |\n",
      "|    value_loss         | 9.75       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 16800     |\n",
      "|    time_elapsed       | 1095      |\n",
      "|    total_timesteps    | 84000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16799     |\n",
      "|    policy_loss        | 25.4      |\n",
      "|    reward             | -1.479451 |\n",
      "|    std                | 1.07      |\n",
      "|    value_loss         | 7.64      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 16900     |\n",
      "|    time_elapsed       | 1102      |\n",
      "|    total_timesteps    | 84500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16899     |\n",
      "|    policy_loss        | -8.52     |\n",
      "|    reward             | 1.3481288 |\n",
      "|    std                | 1.08      |\n",
      "|    value_loss         | 1.92      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 17000      |\n",
      "|    time_elapsed       | 1108       |\n",
      "|    total_timesteps    | 85000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 16999      |\n",
      "|    policy_loss        | 12.9       |\n",
      "|    reward             | -5.4909906 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 1.49       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 76       |\n",
      "|    iterations         | 17100    |\n",
      "|    time_elapsed       | 1114     |\n",
      "|    total_timesteps    | 85500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13.3    |\n",
      "|    explained_variance | 0.0117   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 17099    |\n",
      "|    policy_loss        | -177     |\n",
      "|    reward             | 6.052913 |\n",
      "|    std                | 1.07     |\n",
      "|    value_loss         | 172      |\n",
      "------------------------------------\n",
      "day: 2956, episode: 30\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 6629896.36\n",
      "total_reward: 5629896.36\n",
      "total_cost: 17418.69\n",
      "total_trades: 13283\n",
      "Sharpe: 1.038\n",
      "=================================\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 17200      |\n",
      "|    time_elapsed       | 1121       |\n",
      "|    total_timesteps    | 86000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.4      |\n",
      "|    explained_variance | 0.0828     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 17199      |\n",
      "|    policy_loss        | -1.65      |\n",
      "|    reward             | 0.04141294 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 0.179      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 17300      |\n",
      "|    time_elapsed       | 1128       |\n",
      "|    total_timesteps    | 86500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.4      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 17299      |\n",
      "|    policy_loss        | -14.2      |\n",
      "|    reward             | 0.26318628 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 1.19       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 17400     |\n",
      "|    time_elapsed       | 1135      |\n",
      "|    total_timesteps    | 87000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.3     |\n",
      "|    explained_variance | -0.186    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17399     |\n",
      "|    policy_loss        | -16.4     |\n",
      "|    reward             | -3.010532 |\n",
      "|    std                | 1.07      |\n",
      "|    value_loss         | 1.6       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 17500     |\n",
      "|    time_elapsed       | 1141      |\n",
      "|    total_timesteps    | 87500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17499     |\n",
      "|    policy_loss        | -46       |\n",
      "|    reward             | 2.0220225 |\n",
      "|    std                | 1.07      |\n",
      "|    value_loss         | 17.2      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 17600     |\n",
      "|    time_elapsed       | 1148      |\n",
      "|    total_timesteps    | 88000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.3     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17599     |\n",
      "|    policy_loss        | 185       |\n",
      "|    reward             | 0.220832  |\n",
      "|    std                | 1.07      |\n",
      "|    value_loss         | 209       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 17700     |\n",
      "|    time_elapsed       | 1154      |\n",
      "|    total_timesteps    | 88500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17699     |\n",
      "|    policy_loss        | -43       |\n",
      "|    reward             | 5.2969947 |\n",
      "|    std                | 1.07      |\n",
      "|    value_loss         | 31.3      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 17800      |\n",
      "|    time_elapsed       | 1160       |\n",
      "|    total_timesteps    | 89000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.3      |\n",
      "|    explained_variance | 0.209      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 17799      |\n",
      "|    policy_loss        | 1.69       |\n",
      "|    reward             | 0.95967525 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 0.0414     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 17900      |\n",
      "|    time_elapsed       | 1166       |\n",
      "|    total_timesteps    | 89500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.3      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 17899      |\n",
      "|    policy_loss        | -5.11      |\n",
      "|    reward             | -2.0973399 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 0.227      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 18000     |\n",
      "|    time_elapsed       | 1173      |\n",
      "|    total_timesteps    | 90000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17999     |\n",
      "|    policy_loss        | 21        |\n",
      "|    reward             | 0.6596565 |\n",
      "|    std                | 1.07      |\n",
      "|    value_loss         | 3.7       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 18100      |\n",
      "|    time_elapsed       | 1180       |\n",
      "|    total_timesteps    | 90500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 18099      |\n",
      "|    policy_loss        | -5.28      |\n",
      "|    reward             | 0.85610867 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 0.191      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 18200      |\n",
      "|    time_elapsed       | 1186       |\n",
      "|    total_timesteps    | 91000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 18199      |\n",
      "|    policy_loss        | 24.1       |\n",
      "|    reward             | -0.3730288 |\n",
      "|    std                | 1.08       |\n",
      "|    value_loss         | 14.6       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 18300     |\n",
      "|    time_elapsed       | 1193      |\n",
      "|    total_timesteps    | 91500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18299     |\n",
      "|    policy_loss        | -209      |\n",
      "|    reward             | -3.035855 |\n",
      "|    std                | 1.08      |\n",
      "|    value_loss         | 232       |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 76          |\n",
      "|    iterations         | 18400       |\n",
      "|    time_elapsed       | 1199        |\n",
      "|    total_timesteps    | 92000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.4       |\n",
      "|    explained_variance | 5.96e-08    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 18399       |\n",
      "|    policy_loss        | 12          |\n",
      "|    reward             | 0.005297815 |\n",
      "|    std                | 1.08        |\n",
      "|    value_loss         | 1.2         |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 18500      |\n",
      "|    time_elapsed       | 1205       |\n",
      "|    total_timesteps    | 92500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.4      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 18499      |\n",
      "|    policy_loss        | -14.2      |\n",
      "|    reward             | -0.5087795 |\n",
      "|    std                | 1.08       |\n",
      "|    value_loss         | 5.15       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 18600      |\n",
      "|    time_elapsed       | 1212       |\n",
      "|    total_timesteps    | 93000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 18599      |\n",
      "|    policy_loss        | -2.53      |\n",
      "|    reward             | 0.64332736 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 0.409      |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 76       |\n",
      "|    iterations         | 18700    |\n",
      "|    time_elapsed       | 1218     |\n",
      "|    total_timesteps    | 93500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13.4    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 18699    |\n",
      "|    policy_loss        | -18.2    |\n",
      "|    reward             | 2.514338 |\n",
      "|    std                | 1.08     |\n",
      "|    value_loss         | 2.45     |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 76          |\n",
      "|    iterations         | 18800       |\n",
      "|    time_elapsed       | 1224        |\n",
      "|    total_timesteps    | 94000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.4       |\n",
      "|    explained_variance | 1.19e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 18799       |\n",
      "|    policy_loss        | 65.8        |\n",
      "|    reward             | -0.24979892 |\n",
      "|    std                | 1.08        |\n",
      "|    value_loss         | 35.1        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 76          |\n",
      "|    iterations         | 18900       |\n",
      "|    time_elapsed       | 1231        |\n",
      "|    total_timesteps    | 94500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.4       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 18899       |\n",
      "|    policy_loss        | 49          |\n",
      "|    reward             | -0.36247918 |\n",
      "|    std                | 1.08        |\n",
      "|    value_loss         | 27.1        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 19000     |\n",
      "|    time_elapsed       | 1238      |\n",
      "|    total_timesteps    | 95000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18999     |\n",
      "|    policy_loss        | 3.39      |\n",
      "|    reward             | 0.8643089 |\n",
      "|    std                | 1.08      |\n",
      "|    value_loss         | 0.781     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 19100     |\n",
      "|    time_elapsed       | 1244      |\n",
      "|    total_timesteps    | 95500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19099     |\n",
      "|    policy_loss        | -70.9     |\n",
      "|    reward             | 0.9218334 |\n",
      "|    std                | 1.08      |\n",
      "|    value_loss         | 29.4      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 19200      |\n",
      "|    time_elapsed       | 1251       |\n",
      "|    total_timesteps    | 96000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 19199      |\n",
      "|    policy_loss        | 1.21       |\n",
      "|    reward             | 0.57125866 |\n",
      "|    std                | 1.09       |\n",
      "|    value_loss         | 1.83       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 19300      |\n",
      "|    time_elapsed       | 1257       |\n",
      "|    total_timesteps    | 96500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.5      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 19299      |\n",
      "|    policy_loss        | -32.4      |\n",
      "|    reward             | 0.11719703 |\n",
      "|    std                | 1.09       |\n",
      "|    value_loss         | 6.93       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 19400     |\n",
      "|    time_elapsed       | 1263      |\n",
      "|    total_timesteps    | 97000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.4     |\n",
      "|    explained_variance | -0.00246  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19399     |\n",
      "|    policy_loss        | 83        |\n",
      "|    reward             | 1.4603084 |\n",
      "|    std                | 1.09      |\n",
      "|    value_loss         | 66.8      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 19500     |\n",
      "|    time_elapsed       | 1269      |\n",
      "|    total_timesteps    | 97500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19499     |\n",
      "|    policy_loss        | -36       |\n",
      "|    reward             | 1.6905117 |\n",
      "|    std                | 1.09      |\n",
      "|    value_loss         | 7.91      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 19600     |\n",
      "|    time_elapsed       | 1275      |\n",
      "|    total_timesteps    | 98000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19599     |\n",
      "|    policy_loss        | 32        |\n",
      "|    reward             | 0.5678949 |\n",
      "|    std                | 1.08      |\n",
      "|    value_loss         | 12.5      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 19700      |\n",
      "|    time_elapsed       | 1281       |\n",
      "|    total_timesteps    | 98500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.5      |\n",
      "|    explained_variance | 0.0204     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 19699      |\n",
      "|    policy_loss        | -26.3      |\n",
      "|    reward             | 0.70587856 |\n",
      "|    std                | 1.09       |\n",
      "|    value_loss         | 7.03       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 19800     |\n",
      "|    time_elapsed       | 1287      |\n",
      "|    total_timesteps    | 99000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.5     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19799     |\n",
      "|    policy_loss        | 20.3      |\n",
      "|    reward             | -9.223257 |\n",
      "|    std                | 1.09      |\n",
      "|    value_loss         | 3.82      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 19900     |\n",
      "|    time_elapsed       | 1293      |\n",
      "|    total_timesteps    | 99500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19899     |\n",
      "|    policy_loss        | -68.2     |\n",
      "|    reward             | -4.578126 |\n",
      "|    std                | 1.09      |\n",
      "|    value_loss         | 31.3      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 20000      |\n",
      "|    time_elapsed       | 1299       |\n",
      "|    total_timesteps    | 100000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.6      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 19999      |\n",
      "|    policy_loss        | -44.7      |\n",
      "|    reward             | -11.545456 |\n",
      "|    std                | 1.1        |\n",
      "|    value_loss         | 55.1       |\n",
      "--------------------------------------\n",
      "hit end!\n",
      "==============Get Baseline Stats===========\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Shape of DataFrame:  (400, 8)\n",
      "Annual return         -0.022145\n",
      "Cumulative returns    -0.034921\n",
      "Annual volatility      0.176822\n",
      "Sharpe ratio          -0.038668\n",
      "Calmar ratio          -0.100930\n",
      "Stability              0.215672\n",
      "Max drawdown          -0.219408\n",
      "Omega ratio            0.993578\n",
      "Sortino ratio         -0.053987\n",
      "Skew                        NaN\n",
      "Kurtosis                    NaN\n",
      "Tail ratio             1.003436\n",
      "Daily value at risk   -0.022305\n",
      "dtype: float64\n",
      "result:                       a2c           dji\n",
      "date                                  \n",
      "2021-10-01  1.000000e+06  1.000000e+06\n",
      "2021-10-04  1.000010e+06  9.905746e+05\n",
      "2021-10-05  1.000180e+06  9.996566e+05\n",
      "2021-10-06  1.000407e+06  1.002637e+06\n",
      "2021-10-07  1.000710e+06  1.012483e+06\n",
      "...                  ...           ...\n",
      "2023-04-27  8.978109e+05  9.854252e+05\n",
      "2023-04-28  9.029916e+05  9.933491e+05\n",
      "2023-05-01  9.011997e+05  9.919956e+05\n",
      "2023-05-02  8.994640e+05  9.812993e+05\n",
      "2023-05-03  8.953710e+05  9.734251e+05\n",
      "\n",
      "[399 rows x 2 columns]\n",
      "A2C로 얻은 투자 수익률>> -10.46 %\n",
      "{'n_steps': 5, 'ent_coef': 0.01, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to results/a2c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\se99a\\anaconda3\\lib\\site-packages\\pyfolio\\timeseries.py:724: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  stats = pd.Series()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 75           |\n",
      "|    iterations         | 100          |\n",
      "|    time_elapsed       | 6            |\n",
      "|    total_timesteps    | 500          |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -12.8        |\n",
      "|    explained_variance | 0.502        |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 99           |\n",
      "|    policy_loss        | -17.6        |\n",
      "|    reward             | -0.001757413 |\n",
      "|    std                | 1.01         |\n",
      "|    value_loss         | 1.95         |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 77         |\n",
      "|    iterations         | 200        |\n",
      "|    time_elapsed       | 12         |\n",
      "|    total_timesteps    | 1000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -12.9      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 199        |\n",
      "|    policy_loss        | 4.56       |\n",
      "|    reward             | 0.65731883 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 1.4        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 78        |\n",
      "|    iterations         | 300       |\n",
      "|    time_elapsed       | 19        |\n",
      "|    total_timesteps    | 1500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -12.8     |\n",
      "|    explained_variance | 0.0282    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 299       |\n",
      "|    policy_loss        | -10.1     |\n",
      "|    reward             | -3.025514 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 2.5       |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 79          |\n",
      "|    iterations         | 400         |\n",
      "|    time_elapsed       | 25          |\n",
      "|    total_timesteps    | 2000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -12.9       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 399         |\n",
      "|    policy_loss        | -0.152      |\n",
      "|    reward             | 0.063827716 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 1.26        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 78          |\n",
      "|    iterations         | 500         |\n",
      "|    time_elapsed       | 31          |\n",
      "|    total_timesteps    | 2500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -12.9       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 499         |\n",
      "|    policy_loss        | -30         |\n",
      "|    reward             | -0.23412517 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 5.39        |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 79           |\n",
      "|    iterations         | 600          |\n",
      "|    time_elapsed       | 37           |\n",
      "|    total_timesteps    | 3000         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -12.9        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 599          |\n",
      "|    policy_loss        | 1.61         |\n",
      "|    reward             | -0.016546337 |\n",
      "|    std                | 1.01         |\n",
      "|    value_loss         | 0.0249       |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 79         |\n",
      "|    iterations         | 700        |\n",
      "|    time_elapsed       | 43         |\n",
      "|    total_timesteps    | 3500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -12.9      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 699        |\n",
      "|    policy_loss        | 3.88       |\n",
      "|    reward             | -1.0717508 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 0.53       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 80        |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 49        |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -12.9     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | -5.5      |\n",
      "|    reward             | 1.1477439 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 0.975     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 78        |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 57        |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -12.9     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | 23.2      |\n",
      "|    reward             | 3.9798017 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 7.47      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 78        |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 63        |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -12.9     |\n",
      "|    explained_variance | -2.38e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | -61.4     |\n",
      "|    reward             | 4.547108  |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 58.5      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 79       |\n",
      "|    iterations         | 1100     |\n",
      "|    time_elapsed       | 69       |\n",
      "|    total_timesteps    | 5500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -12.9    |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1099     |\n",
      "|    policy_loss        | 93.8     |\n",
      "|    reward             | 2.699386 |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 61.3     |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 79          |\n",
      "|    iterations         | 1200        |\n",
      "|    time_elapsed       | 75          |\n",
      "|    total_timesteps    | 6000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -12.9       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1199        |\n",
      "|    policy_loss        | -10.5       |\n",
      "|    reward             | -0.77563256 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 1.25        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 79        |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 81        |\n",
      "|    total_timesteps    | 6500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -12.9     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1299      |\n",
      "|    policy_loss        | -24.1     |\n",
      "|    reward             | 0.8334572 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 3.99      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 79         |\n",
      "|    iterations         | 1400       |\n",
      "|    time_elapsed       | 87         |\n",
      "|    total_timesteps    | 7000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -12.9      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1399       |\n",
      "|    policy_loss        | 31.2       |\n",
      "|    reward             | 0.91823125 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 8.12       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 80         |\n",
      "|    iterations         | 1500       |\n",
      "|    time_elapsed       | 93         |\n",
      "|    total_timesteps    | 7500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -12.9      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1499       |\n",
      "|    policy_loss        | 36.6       |\n",
      "|    reward             | 0.30438337 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 10.7       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 80        |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 99        |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -12.9     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | -4.01     |\n",
      "|    reward             | 1.4991869 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 6.18      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 80        |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 105       |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -12.9     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | 146       |\n",
      "|    reward             | 9.926856  |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 204       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 80        |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 111       |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -12.9     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | -29.7     |\n",
      "|    reward             | 0.8682135 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 6.93      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 80         |\n",
      "|    iterations         | 1900       |\n",
      "|    time_elapsed       | 117        |\n",
      "|    total_timesteps    | 9500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13        |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1899       |\n",
      "|    policy_loss        | -42.6      |\n",
      "|    reward             | 0.13399921 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 13.2       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 80        |\n",
      "|    iterations         | 2000      |\n",
      "|    time_elapsed       | 123       |\n",
      "|    total_timesteps    | 10000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13       |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1999      |\n",
      "|    policy_loss        | 5.34      |\n",
      "|    reward             | 0.3548313 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 0.98      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 80        |\n",
      "|    iterations         | 2100      |\n",
      "|    time_elapsed       | 130       |\n",
      "|    total_timesteps    | 10500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13       |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2099      |\n",
      "|    policy_loss        | -8.96     |\n",
      "|    reward             | 2.4914005 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 3.49      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 80         |\n",
      "|    iterations         | 2200       |\n",
      "|    time_elapsed       | 136        |\n",
      "|    total_timesteps    | 11000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13        |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2199       |\n",
      "|    policy_loss        | 35.7       |\n",
      "|    reward             | -1.9539584 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 10.9       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 80        |\n",
      "|    iterations         | 2300      |\n",
      "|    time_elapsed       | 142       |\n",
      "|    total_timesteps    | 11500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -12.9     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2299      |\n",
      "|    policy_loss        | 177       |\n",
      "|    reward             | 3.4058933 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 224       |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 80          |\n",
      "|    iterations         | 2400        |\n",
      "|    time_elapsed       | 148         |\n",
      "|    total_timesteps    | 12000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -12.9       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 2399        |\n",
      "|    policy_loss        | 30          |\n",
      "|    reward             | 0.001830444 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 7.21        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 80        |\n",
      "|    iterations         | 2500      |\n",
      "|    time_elapsed       | 155       |\n",
      "|    total_timesteps    | 12500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -12.9     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2499      |\n",
      "|    policy_loss        | -6.98     |\n",
      "|    reward             | 0.7886806 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 0.734     |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 80          |\n",
      "|    iterations         | 2600        |\n",
      "|    time_elapsed       | 161         |\n",
      "|    total_timesteps    | 13000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -12.9       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 2599        |\n",
      "|    policy_loss        | 11.7        |\n",
      "|    reward             | 0.076392226 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 2.18        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 80         |\n",
      "|    iterations         | 2700       |\n",
      "|    time_elapsed       | 167        |\n",
      "|    total_timesteps    | 13500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -12.9      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2699       |\n",
      "|    policy_loss        | -16.5      |\n",
      "|    reward             | -1.5801984 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 2.25       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 80        |\n",
      "|    iterations         | 2800      |\n",
      "|    time_elapsed       | 173       |\n",
      "|    total_timesteps    | 14000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -12.9     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2799      |\n",
      "|    policy_loss        | -44.2     |\n",
      "|    reward             | 2.3459444 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 17.2      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 80         |\n",
      "|    iterations         | 2900       |\n",
      "|    time_elapsed       | 180        |\n",
      "|    total_timesteps    | 14500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13        |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2899       |\n",
      "|    policy_loss        | 70.2       |\n",
      "|    reward             | 0.07346331 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 45.4       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 80         |\n",
      "|    iterations         | 3000       |\n",
      "|    time_elapsed       | 186        |\n",
      "|    total_timesteps    | 15000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13        |\n",
      "|    explained_variance | -5.17      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2999       |\n",
      "|    policy_loss        | 22.4       |\n",
      "|    reward             | -1.3734864 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 4.16       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 80        |\n",
      "|    iterations         | 3100      |\n",
      "|    time_elapsed       | 192       |\n",
      "|    total_timesteps    | 15500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -12.9     |\n",
      "|    explained_variance | 0.00618   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3099      |\n",
      "|    policy_loss        | -32.6     |\n",
      "|    reward             | 0.4944389 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 11.5      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 80        |\n",
      "|    iterations         | 3200      |\n",
      "|    time_elapsed       | 198       |\n",
      "|    total_timesteps    | 16000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13       |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3199      |\n",
      "|    policy_loss        | 52.5      |\n",
      "|    reward             | 1.3484528 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 24.4      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 80         |\n",
      "|    iterations         | 3300       |\n",
      "|    time_elapsed       | 204        |\n",
      "|    total_timesteps    | 16500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13        |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3299       |\n",
      "|    policy_loss        | -3.37      |\n",
      "|    reward             | -2.6384313 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 1.03       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 80         |\n",
      "|    iterations         | 3400       |\n",
      "|    time_elapsed       | 210        |\n",
      "|    total_timesteps    | 17000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13        |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3399       |\n",
      "|    policy_loss        | -99.3      |\n",
      "|    reward             | -2.9302745 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 109        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 80        |\n",
      "|    iterations         | 3500      |\n",
      "|    time_elapsed       | 216       |\n",
      "|    total_timesteps    | 17500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13       |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3499      |\n",
      "|    policy_loss        | 101       |\n",
      "|    reward             | 1.2082165 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 82.1      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 80         |\n",
      "|    iterations         | 3600       |\n",
      "|    time_elapsed       | 222        |\n",
      "|    total_timesteps    | 18000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13        |\n",
      "|    explained_variance | 0.000632   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3599       |\n",
      "|    policy_loss        | -2         |\n",
      "|    reward             | 0.33932915 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 0.17       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 80          |\n",
      "|    iterations         | 3700        |\n",
      "|    time_elapsed       | 228         |\n",
      "|    total_timesteps    | 18500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13         |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 3699        |\n",
      "|    policy_loss        | 11.8        |\n",
      "|    reward             | -0.22058578 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 1.68        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 80         |\n",
      "|    iterations         | 3800       |\n",
      "|    time_elapsed       | 235        |\n",
      "|    total_timesteps    | 19000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13        |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3799       |\n",
      "|    policy_loss        | 27.8       |\n",
      "|    reward             | -1.0013237 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 10.1       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 80        |\n",
      "|    iterations         | 3900      |\n",
      "|    time_elapsed       | 242       |\n",
      "|    total_timesteps    | 19500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13       |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3899      |\n",
      "|    policy_loss        | -18.9     |\n",
      "|    reward             | 1.0235654 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 3.51      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 80         |\n",
      "|    iterations         | 4000       |\n",
      "|    time_elapsed       | 248        |\n",
      "|    total_timesteps    | 20000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13        |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3999       |\n",
      "|    policy_loss        | -54.3      |\n",
      "|    reward             | -5.3502584 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 20.9       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 80        |\n",
      "|    iterations         | 4100      |\n",
      "|    time_elapsed       | 254       |\n",
      "|    total_timesteps    | 20500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13       |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4099      |\n",
      "|    policy_loss        | -10.2     |\n",
      "|    reward             | 2.0621362 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 1.39      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 80          |\n",
      "|    iterations         | 4200        |\n",
      "|    time_elapsed       | 260         |\n",
      "|    total_timesteps    | 21000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.1       |\n",
      "|    explained_variance | 5.96e-08    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 4199        |\n",
      "|    policy_loss        | 3.98        |\n",
      "|    reward             | -0.88515437 |\n",
      "|    std                | 1.03        |\n",
      "|    value_loss         | 0.945       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 80          |\n",
      "|    iterations         | 4300        |\n",
      "|    time_elapsed       | 266         |\n",
      "|    total_timesteps    | 21500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13         |\n",
      "|    explained_variance | 1.19e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 4299        |\n",
      "|    policy_loss        | 28.2        |\n",
      "|    reward             | -0.32555965 |\n",
      "|    std                | 1.03        |\n",
      "|    value_loss         | 6.42        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 80         |\n",
      "|    iterations         | 4400       |\n",
      "|    time_elapsed       | 273        |\n",
      "|    total_timesteps    | 22000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13        |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4399       |\n",
      "|    policy_loss        | -5.07      |\n",
      "|    reward             | -2.5132227 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 0.521      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 80        |\n",
      "|    iterations         | 4500      |\n",
      "|    time_elapsed       | 279       |\n",
      "|    total_timesteps    | 22500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4499      |\n",
      "|    policy_loss        | 8.76      |\n",
      "|    reward             | 3.0212212 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 0.769     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 80         |\n",
      "|    iterations         | 4600       |\n",
      "|    time_elapsed       | 285        |\n",
      "|    total_timesteps    | 23000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13        |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4599       |\n",
      "|    policy_loss        | 45.5       |\n",
      "|    reward             | 0.04974291 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 18.9       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 80         |\n",
      "|    iterations         | 4700       |\n",
      "|    time_elapsed       | 291        |\n",
      "|    total_timesteps    | 23500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13        |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4699       |\n",
      "|    policy_loss        | -54.4      |\n",
      "|    reward             | -0.9630206 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 25.5       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 80         |\n",
      "|    iterations         | 4800       |\n",
      "|    time_elapsed       | 298        |\n",
      "|    total_timesteps    | 24000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.1      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4799       |\n",
      "|    policy_loss        | -3.13      |\n",
      "|    reward             | -1.1422621 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 1.89       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 80        |\n",
      "|    iterations         | 4900      |\n",
      "|    time_elapsed       | 304       |\n",
      "|    total_timesteps    | 24500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4899      |\n",
      "|    policy_loss        | 26.9      |\n",
      "|    reward             | -0.423216 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 5.13      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 80        |\n",
      "|    iterations         | 5000      |\n",
      "|    time_elapsed       | 310       |\n",
      "|    total_timesteps    | 25000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4999      |\n",
      "|    policy_loss        | -37.6     |\n",
      "|    reward             | 1.0577953 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 11.2      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 80         |\n",
      "|    iterations         | 5100       |\n",
      "|    time_elapsed       | 315        |\n",
      "|    total_timesteps    | 25500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5099       |\n",
      "|    policy_loss        | 64.5       |\n",
      "|    reward             | 0.33746532 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 23.5       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 80          |\n",
      "|    iterations         | 5200        |\n",
      "|    time_elapsed       | 321         |\n",
      "|    total_timesteps    | 26000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.1       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 5199        |\n",
      "|    policy_loss        | -20.4       |\n",
      "|    reward             | -0.99531674 |\n",
      "|    std                | 1.04        |\n",
      "|    value_loss         | 2.58        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 80         |\n",
      "|    iterations         | 5300       |\n",
      "|    time_elapsed       | 327        |\n",
      "|    total_timesteps    | 26500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5299       |\n",
      "|    policy_loss        | 75.6       |\n",
      "|    reward             | -6.3988814 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 43.1       |\n",
      "--------------------------------------\n",
      "day: 2956, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4971397.64\n",
      "total_reward: 3971397.64\n",
      "total_cost: 2690.44\n",
      "total_trades: 12960\n",
      "Sharpe: 0.846\n",
      "=================================\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 80         |\n",
      "|    iterations         | 5400       |\n",
      "|    time_elapsed       | 334        |\n",
      "|    total_timesteps    | 27000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5399       |\n",
      "|    policy_loss        | -33.4      |\n",
      "|    reward             | -1.3559102 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 7.45       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 80         |\n",
      "|    iterations         | 5500       |\n",
      "|    time_elapsed       | 340        |\n",
      "|    total_timesteps    | 27500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5499       |\n",
      "|    policy_loss        | 33.9       |\n",
      "|    reward             | -0.1812998 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 7.58       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 80       |\n",
      "|    iterations         | 5600     |\n",
      "|    time_elapsed       | 346      |\n",
      "|    total_timesteps    | 28000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13.1    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5599     |\n",
      "|    policy_loss        | -10.1    |\n",
      "|    reward             | 0.854987 |\n",
      "|    std                | 1.03     |\n",
      "|    value_loss         | 6.47     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 80         |\n",
      "|    iterations         | 5700       |\n",
      "|    time_elapsed       | 352        |\n",
      "|    total_timesteps    | 28500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13        |\n",
      "|    explained_variance | -2.38e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5699       |\n",
      "|    policy_loss        | -7.95      |\n",
      "|    reward             | 0.44608507 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 2.94       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 80        |\n",
      "|    iterations         | 5800      |\n",
      "|    time_elapsed       | 358       |\n",
      "|    total_timesteps    | 29000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13       |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5799      |\n",
      "|    policy_loss        | 12.4      |\n",
      "|    reward             | 1.4520917 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 4.34      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 80        |\n",
      "|    iterations         | 5900      |\n",
      "|    time_elapsed       | 364       |\n",
      "|    total_timesteps    | 29500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5899      |\n",
      "|    policy_loss        | -42.9     |\n",
      "|    reward             | 2.2337909 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 12.6      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 80        |\n",
      "|    iterations         | 6000      |\n",
      "|    time_elapsed       | 370       |\n",
      "|    total_timesteps    | 30000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.1     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5999      |\n",
      "|    policy_loss        | -47       |\n",
      "|    reward             | 0.8460391 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 18.1      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 81        |\n",
      "|    iterations         | 6100      |\n",
      "|    time_elapsed       | 376       |\n",
      "|    total_timesteps    | 30500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13       |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6099      |\n",
      "|    policy_loss        | 9.35      |\n",
      "|    reward             | 0.7354939 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 1.26      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 81         |\n",
      "|    iterations         | 6200       |\n",
      "|    time_elapsed       | 382        |\n",
      "|    total_timesteps    | 31000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6199       |\n",
      "|    policy_loss        | -33.4      |\n",
      "|    reward             | -1.9552306 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 31.8       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 81        |\n",
      "|    iterations         | 6300      |\n",
      "|    time_elapsed       | 388       |\n",
      "|    total_timesteps    | 31500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6299      |\n",
      "|    policy_loss        | 0.505     |\n",
      "|    reward             | 1.3676227 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 1.17      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 80        |\n",
      "|    iterations         | 6400      |\n",
      "|    time_elapsed       | 395       |\n",
      "|    total_timesteps    | 32000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6399      |\n",
      "|    policy_loss        | 49.6      |\n",
      "|    reward             | 4.5493174 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 27.6      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 80        |\n",
      "|    iterations         | 6500      |\n",
      "|    time_elapsed       | 401       |\n",
      "|    total_timesteps    | 32500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13       |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6499      |\n",
      "|    policy_loss        | -170      |\n",
      "|    reward             | 3.3698006 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 144       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 81        |\n",
      "|    iterations         | 6600      |\n",
      "|    time_elapsed       | 407       |\n",
      "|    total_timesteps    | 33000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6599      |\n",
      "|    policy_loss        | -51.8     |\n",
      "|    reward             | -2.453602 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 15.1      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 81       |\n",
      "|    iterations         | 6700     |\n",
      "|    time_elapsed       | 413      |\n",
      "|    total_timesteps    | 33500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13.1    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6699     |\n",
      "|    policy_loss        | -2.68    |\n",
      "|    reward             | 2.274271 |\n",
      "|    std                | 1.04     |\n",
      "|    value_loss         | 0.0699   |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 80         |\n",
      "|    iterations         | 6800       |\n",
      "|    time_elapsed       | 420        |\n",
      "|    total_timesteps    | 34000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6799       |\n",
      "|    policy_loss        | 27.6       |\n",
      "|    reward             | -3.0494022 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 5.08       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 80         |\n",
      "|    iterations         | 6900       |\n",
      "|    time_elapsed       | 426        |\n",
      "|    total_timesteps    | 34500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6899       |\n",
      "|    policy_loss        | -2.69      |\n",
      "|    reward             | -1.2182186 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 0.649      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 80        |\n",
      "|    iterations         | 7000      |\n",
      "|    time_elapsed       | 432       |\n",
      "|    total_timesteps    | 35000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.1     |\n",
      "|    explained_variance | -0.24     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6999      |\n",
      "|    policy_loss        | -27.7     |\n",
      "|    reward             | 1.7429006 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 6.47      |\n",
      "-------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 80           |\n",
      "|    iterations         | 7100         |\n",
      "|    time_elapsed       | 439          |\n",
      "|    total_timesteps    | 35500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -13.1        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 7099         |\n",
      "|    policy_loss        | -2.08        |\n",
      "|    reward             | 0.0114160525 |\n",
      "|    std                | 1.04         |\n",
      "|    value_loss         | 0.0354       |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 80          |\n",
      "|    iterations         | 7200        |\n",
      "|    time_elapsed       | 445         |\n",
      "|    total_timesteps    | 36000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.2       |\n",
      "|    explained_variance | -0.927      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 7199        |\n",
      "|    policy_loss        | 8.51        |\n",
      "|    reward             | -0.70100534 |\n",
      "|    std                | 1.05        |\n",
      "|    value_loss         | 0.97        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 80         |\n",
      "|    iterations         | 7300       |\n",
      "|    time_elapsed       | 451        |\n",
      "|    total_timesteps    | 36500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.2      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7299       |\n",
      "|    policy_loss        | -7.66      |\n",
      "|    reward             | -1.1113698 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 0.626      |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 80       |\n",
      "|    iterations         | 7400     |\n",
      "|    time_elapsed       | 457      |\n",
      "|    total_timesteps    | 37000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13.2    |\n",
      "|    explained_variance | -0.00136 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7399     |\n",
      "|    policy_loss        | -82.2    |\n",
      "|    reward             | 2.388641 |\n",
      "|    std                | 1.05     |\n",
      "|    value_loss         | 41       |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 80        |\n",
      "|    iterations         | 7500      |\n",
      "|    time_elapsed       | 463       |\n",
      "|    total_timesteps    | 37500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7499      |\n",
      "|    policy_loss        | 7.72      |\n",
      "|    reward             | 2.1646192 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 0.551     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 80        |\n",
      "|    iterations         | 7600      |\n",
      "|    time_elapsed       | 469       |\n",
      "|    total_timesteps    | 38000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7599      |\n",
      "|    policy_loss        | 4.92      |\n",
      "|    reward             | 4.6499352 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 0.419     |\n",
      "-------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 80           |\n",
      "|    iterations         | 7700         |\n",
      "|    time_elapsed       | 475          |\n",
      "|    total_timesteps    | 38500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -13.3        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 7699         |\n",
      "|    policy_loss        | 4.11         |\n",
      "|    reward             | -0.023597026 |\n",
      "|    std                | 1.06         |\n",
      "|    value_loss         | 0.137        |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 80          |\n",
      "|    iterations         | 7800        |\n",
      "|    time_elapsed       | 481         |\n",
      "|    total_timesteps    | 39000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.3       |\n",
      "|    explained_variance | 0.291       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 7799        |\n",
      "|    policy_loss        | 18.2        |\n",
      "|    reward             | -0.65791315 |\n",
      "|    std                | 1.06        |\n",
      "|    value_loss         | 2.19        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 80        |\n",
      "|    iterations         | 7900      |\n",
      "|    time_elapsed       | 487       |\n",
      "|    total_timesteps    | 39500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7899      |\n",
      "|    policy_loss        | -43.1     |\n",
      "|    reward             | 1.1813085 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 9.57      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 81         |\n",
      "|    iterations         | 8000       |\n",
      "|    time_elapsed       | 493        |\n",
      "|    total_timesteps    | 40000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.3      |\n",
      "|    explained_variance | -0.695     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7999       |\n",
      "|    policy_loss        | -24.5      |\n",
      "|    reward             | 0.17262426 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 5.53       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 81       |\n",
      "|    iterations         | 8100     |\n",
      "|    time_elapsed       | 499      |\n",
      "|    total_timesteps    | 40500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13.3    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8099     |\n",
      "|    policy_loss        | -47.5    |\n",
      "|    reward             | 4.468175 |\n",
      "|    std                | 1.06     |\n",
      "|    value_loss         | 24.4     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 81         |\n",
      "|    iterations         | 8200       |\n",
      "|    time_elapsed       | 505        |\n",
      "|    total_timesteps    | 41000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.3      |\n",
      "|    explained_variance | 0.0524     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8199       |\n",
      "|    policy_loss        | -387       |\n",
      "|    reward             | -10.618932 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 839        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 81        |\n",
      "|    iterations         | 8300      |\n",
      "|    time_elapsed       | 511       |\n",
      "|    total_timesteps    | 41500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.3     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8299      |\n",
      "|    policy_loss        | -22.7     |\n",
      "|    reward             | -1.482109 |\n",
      "|    std                | 1.07      |\n",
      "|    value_loss         | 5.63      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 81         |\n",
      "|    iterations         | 8400       |\n",
      "|    time_elapsed       | 518        |\n",
      "|    total_timesteps    | 42000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.4      |\n",
      "|    explained_variance | -0.463     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8399       |\n",
      "|    policy_loss        | -18.6      |\n",
      "|    reward             | 0.42278016 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 3.42       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 81        |\n",
      "|    iterations         | 8500      |\n",
      "|    time_elapsed       | 523       |\n",
      "|    total_timesteps    | 42500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8499      |\n",
      "|    policy_loss        | 0.674     |\n",
      "|    reward             | 1.4080163 |\n",
      "|    std                | 1.07      |\n",
      "|    value_loss         | 0.96      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 81        |\n",
      "|    iterations         | 8600      |\n",
      "|    time_elapsed       | 530       |\n",
      "|    total_timesteps    | 43000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.4     |\n",
      "|    explained_variance | 0.182     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8599      |\n",
      "|    policy_loss        | -1.91     |\n",
      "|    reward             | 2.1838498 |\n",
      "|    std                | 1.07      |\n",
      "|    value_loss         | 0.615     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 80        |\n",
      "|    iterations         | 8700      |\n",
      "|    time_elapsed       | 537       |\n",
      "|    total_timesteps    | 43500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.4     |\n",
      "|    explained_variance | -0.192    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8699      |\n",
      "|    policy_loss        | -55.7     |\n",
      "|    reward             | 2.5564451 |\n",
      "|    std                | 1.07      |\n",
      "|    value_loss         | 17.9      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 80         |\n",
      "|    iterations         | 8800       |\n",
      "|    time_elapsed       | 546        |\n",
      "|    total_timesteps    | 44000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.4      |\n",
      "|    explained_variance | 0.137      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8799       |\n",
      "|    policy_loss        | 167        |\n",
      "|    reward             | -1.3202195 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 184        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 80        |\n",
      "|    iterations         | 8900      |\n",
      "|    time_elapsed       | 552       |\n",
      "|    total_timesteps    | 44500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.3     |\n",
      "|    explained_variance | -0.476    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8899      |\n",
      "|    policy_loss        | 60.4      |\n",
      "|    reward             | 2.2174892 |\n",
      "|    std                | 1.07      |\n",
      "|    value_loss         | 20.1      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 80         |\n",
      "|    iterations         | 9000       |\n",
      "|    time_elapsed       | 559        |\n",
      "|    total_timesteps    | 45000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.4      |\n",
      "|    explained_variance | -0.246     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8999       |\n",
      "|    policy_loss        | 21.4       |\n",
      "|    reward             | 0.12448571 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 3.46       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 80          |\n",
      "|    iterations         | 9100        |\n",
      "|    time_elapsed       | 565         |\n",
      "|    total_timesteps    | 45500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.4       |\n",
      "|    explained_variance | -0.0103     |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 9099        |\n",
      "|    policy_loss        | 2.6         |\n",
      "|    reward             | 0.021002319 |\n",
      "|    std                | 1.07        |\n",
      "|    value_loss         | 0.284       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 80         |\n",
      "|    iterations         | 9200       |\n",
      "|    time_elapsed       | 571        |\n",
      "|    total_timesteps    | 46000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.4      |\n",
      "|    explained_variance | 0.0911     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9199       |\n",
      "|    policy_loss        | 52         |\n",
      "|    reward             | 0.98086673 |\n",
      "|    std                | 1.08       |\n",
      "|    value_loss         | 20.7       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 80        |\n",
      "|    iterations         | 9300      |\n",
      "|    time_elapsed       | 578       |\n",
      "|    total_timesteps    | 46500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.4     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9299      |\n",
      "|    policy_loss        | 9.71      |\n",
      "|    reward             | 1.6656954 |\n",
      "|    std                | 1.08      |\n",
      "|    value_loss         | 1.23      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 80        |\n",
      "|    iterations         | 9400      |\n",
      "|    time_elapsed       | 584       |\n",
      "|    total_timesteps    | 47000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9399      |\n",
      "|    policy_loss        | 52.6      |\n",
      "|    reward             | 2.1578484 |\n",
      "|    std                | 1.08      |\n",
      "|    value_loss         | 31.8      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 80        |\n",
      "|    iterations         | 9500      |\n",
      "|    time_elapsed       | 591       |\n",
      "|    total_timesteps    | 47500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.5     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9499      |\n",
      "|    policy_loss        | 13.7      |\n",
      "|    reward             | 0.5469404 |\n",
      "|    std                | 1.08      |\n",
      "|    value_loss         | 1.81      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 80         |\n",
      "|    iterations         | 9600       |\n",
      "|    time_elapsed       | 597        |\n",
      "|    total_timesteps    | 48000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.5      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9599       |\n",
      "|    policy_loss        | -4.28      |\n",
      "|    reward             | -1.8183416 |\n",
      "|    std                | 1.08       |\n",
      "|    value_loss         | 1.45       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 80         |\n",
      "|    iterations         | 9700       |\n",
      "|    time_elapsed       | 604        |\n",
      "|    total_timesteps    | 48500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.4      |\n",
      "|    explained_variance | -5.72e-06  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9699       |\n",
      "|    policy_loss        | -4.31      |\n",
      "|    reward             | -1.5737907 |\n",
      "|    std                | 1.08       |\n",
      "|    value_loss         | 0.514      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 80          |\n",
      "|    iterations         | 9800        |\n",
      "|    time_elapsed       | 611         |\n",
      "|    total_timesteps    | 49000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.4       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 9799        |\n",
      "|    policy_loss        | -110        |\n",
      "|    reward             | -0.67253387 |\n",
      "|    std                | 1.08        |\n",
      "|    value_loss         | 54.5        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 80         |\n",
      "|    iterations         | 9900       |\n",
      "|    time_elapsed       | 617        |\n",
      "|    total_timesteps    | 49500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.4      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9899       |\n",
      "|    policy_loss        | -32        |\n",
      "|    reward             | 0.71488166 |\n",
      "|    std                | 1.08       |\n",
      "|    value_loss         | 6.75       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 80       |\n",
      "|    iterations         | 10000    |\n",
      "|    time_elapsed       | 623      |\n",
      "|    total_timesteps    | 50000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13.4    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9999     |\n",
      "|    policy_loss        | 8.82     |\n",
      "|    reward             | -6.06696 |\n",
      "|    std                | 1.08     |\n",
      "|    value_loss         | 4.42     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 80        |\n",
      "|    iterations         | 10100     |\n",
      "|    time_elapsed       | 630       |\n",
      "|    total_timesteps    | 50500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 10099     |\n",
      "|    policy_loss        | 12.4      |\n",
      "|    reward             | 1.7781104 |\n",
      "|    std                | 1.08      |\n",
      "|    value_loss         | 1.3       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 80        |\n",
      "|    iterations         | 10200     |\n",
      "|    time_elapsed       | 636       |\n",
      "|    total_timesteps    | 51000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.5     |\n",
      "|    explained_variance | 3.22e-06  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 10199     |\n",
      "|    policy_loss        | 24.4      |\n",
      "|    reward             | 1.3731687 |\n",
      "|    std                | 1.09      |\n",
      "|    value_loss         | 5.05      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 80        |\n",
      "|    iterations         | 10300     |\n",
      "|    time_elapsed       | 642       |\n",
      "|    total_timesteps    | 51500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.5     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 10299     |\n",
      "|    policy_loss        | 8.46      |\n",
      "|    reward             | 1.3078729 |\n",
      "|    std                | 1.09      |\n",
      "|    value_loss         | 0.605     |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 80          |\n",
      "|    iterations         | 10400       |\n",
      "|    time_elapsed       | 649         |\n",
      "|    total_timesteps    | 52000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.5       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 10399       |\n",
      "|    policy_loss        | 96.1        |\n",
      "|    reward             | -0.22159967 |\n",
      "|    std                | 1.09        |\n",
      "|    value_loss         | 56.1        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 80        |\n",
      "|    iterations         | 10500     |\n",
      "|    time_elapsed       | 655       |\n",
      "|    total_timesteps    | 52500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 10499     |\n",
      "|    policy_loss        | 95.5      |\n",
      "|    reward             | -6.531339 |\n",
      "|    std                | 1.09      |\n",
      "|    value_loss         | 75.1      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 80         |\n",
      "|    iterations         | 10600      |\n",
      "|    time_elapsed       | 661        |\n",
      "|    total_timesteps    | 53000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.5      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 10599      |\n",
      "|    policy_loss        | -176       |\n",
      "|    reward             | -0.7309319 |\n",
      "|    std                | 1.09       |\n",
      "|    value_loss         | 295        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 80        |\n",
      "|    iterations         | 10700     |\n",
      "|    time_elapsed       | 668       |\n",
      "|    total_timesteps    | 53500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.5     |\n",
      "|    explained_variance | -0.141    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 10699     |\n",
      "|    policy_loss        | 10.3      |\n",
      "|    reward             | 0.6880529 |\n",
      "|    std                | 1.08      |\n",
      "|    value_loss         | 0.835     |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 80          |\n",
      "|    iterations         | 10800       |\n",
      "|    time_elapsed       | 674         |\n",
      "|    total_timesteps    | 54000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.4       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 10799       |\n",
      "|    policy_loss        | 16.7        |\n",
      "|    reward             | -0.57398766 |\n",
      "|    std                | 1.08        |\n",
      "|    value_loss         | 2.29        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 80         |\n",
      "|    iterations         | 10900      |\n",
      "|    time_elapsed       | 680        |\n",
      "|    total_timesteps    | 54500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 10899      |\n",
      "|    policy_loss        | -14.5      |\n",
      "|    reward             | -2.3421314 |\n",
      "|    std                | 1.08       |\n",
      "|    value_loss         | 3.68       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 80         |\n",
      "|    iterations         | 11000      |\n",
      "|    time_elapsed       | 687        |\n",
      "|    total_timesteps    | 55000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.5      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 10999      |\n",
      "|    policy_loss        | -3.05      |\n",
      "|    reward             | 0.44228497 |\n",
      "|    std                | 1.08       |\n",
      "|    value_loss         | 0.492      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 80        |\n",
      "|    iterations         | 11100     |\n",
      "|    time_elapsed       | 693       |\n",
      "|    total_timesteps    | 55500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.5     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 11099     |\n",
      "|    policy_loss        | 105       |\n",
      "|    reward             | 0.5892101 |\n",
      "|    std                | 1.08      |\n",
      "|    value_loss         | 107       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 79        |\n",
      "|    iterations         | 11200     |\n",
      "|    time_elapsed       | 700       |\n",
      "|    total_timesteps    | 56000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.5     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 11199     |\n",
      "|    policy_loss        | -61.1     |\n",
      "|    reward             | -3.721753 |\n",
      "|    std                | 1.08      |\n",
      "|    value_loss         | 26        |\n",
      "-------------------------------------\n",
      "day: 2956, episode: 20\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 5003394.14\n",
      "total_reward: 4003394.14\n",
      "total_cost: 5123.12\n",
      "total_trades: 13423\n",
      "Sharpe: 0.839\n",
      "=================================\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 79         |\n",
      "|    iterations         | 11300      |\n",
      "|    time_elapsed       | 706        |\n",
      "|    total_timesteps    | 56500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.5      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 11299      |\n",
      "|    policy_loss        | 15.4       |\n",
      "|    reward             | 0.20948535 |\n",
      "|    std                | 1.09       |\n",
      "|    value_loss         | 1.35       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 79        |\n",
      "|    iterations         | 11400     |\n",
      "|    time_elapsed       | 713       |\n",
      "|    total_timesteps    | 57000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 11399     |\n",
      "|    policy_loss        | -6.9      |\n",
      "|    reward             | 0.7348412 |\n",
      "|    std                | 1.08      |\n",
      "|    value_loss         | 0.895     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 79        |\n",
      "|    iterations         | 11500     |\n",
      "|    time_elapsed       | 719       |\n",
      "|    total_timesteps    | 57500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.5     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 11499     |\n",
      "|    policy_loss        | -15.3     |\n",
      "|    reward             | 2.916278  |\n",
      "|    std                | 1.09      |\n",
      "|    value_loss         | 1.73      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 79        |\n",
      "|    iterations         | 11600     |\n",
      "|    time_elapsed       | 726       |\n",
      "|    total_timesteps    | 58000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 11599     |\n",
      "|    policy_loss        | -3.29     |\n",
      "|    reward             | -0.345784 |\n",
      "|    std                | 1.08      |\n",
      "|    value_loss         | 1.11      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 79         |\n",
      "|    iterations         | 11700      |\n",
      "|    time_elapsed       | 732        |\n",
      "|    total_timesteps    | 58500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.4      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 11699      |\n",
      "|    policy_loss        | 15         |\n",
      "|    reward             | -2.2248678 |\n",
      "|    std                | 1.08       |\n",
      "|    value_loss         | 2.92       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 79        |\n",
      "|    iterations         | 11800     |\n",
      "|    time_elapsed       | 739       |\n",
      "|    total_timesteps    | 59000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 11799     |\n",
      "|    policy_loss        | 92.6      |\n",
      "|    reward             | 3.2825973 |\n",
      "|    std                | 1.08      |\n",
      "|    value_loss         | 61.2      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 79       |\n",
      "|    iterations         | 11900    |\n",
      "|    time_elapsed       | 745      |\n",
      "|    total_timesteps    | 59500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13.5    |\n",
      "|    explained_variance | -0.0276  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 11899    |\n",
      "|    policy_loss        | -31.4    |\n",
      "|    reward             | -0.99078 |\n",
      "|    std                | 1.08     |\n",
      "|    value_loss         | 7.11     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 79        |\n",
      "|    iterations         | 12000     |\n",
      "|    time_elapsed       | 752       |\n",
      "|    total_timesteps    | 60000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.5     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 11999     |\n",
      "|    policy_loss        | -18.7     |\n",
      "|    reward             | -2.579205 |\n",
      "|    std                | 1.08      |\n",
      "|    value_loss         | 2.46      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 79        |\n",
      "|    iterations         | 12100     |\n",
      "|    time_elapsed       | 758       |\n",
      "|    total_timesteps    | 60500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 12099     |\n",
      "|    policy_loss        | -12.5     |\n",
      "|    reward             | 0.8188786 |\n",
      "|    std                | 1.08      |\n",
      "|    value_loss         | 1.33      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 79        |\n",
      "|    iterations         | 12200     |\n",
      "|    time_elapsed       | 764       |\n",
      "|    total_timesteps    | 61000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 12199     |\n",
      "|    policy_loss        | -76.5     |\n",
      "|    reward             | 0.2621056 |\n",
      "|    std                | 1.08      |\n",
      "|    value_loss         | 32.5      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 79        |\n",
      "|    iterations         | 12300     |\n",
      "|    time_elapsed       | 770       |\n",
      "|    total_timesteps    | 61500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 12299     |\n",
      "|    policy_loss        | -52.2     |\n",
      "|    reward             | 3.4335296 |\n",
      "|    std                | 1.08      |\n",
      "|    value_loss         | 25.6      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 79        |\n",
      "|    iterations         | 12400     |\n",
      "|    time_elapsed       | 777       |\n",
      "|    total_timesteps    | 62000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 12399     |\n",
      "|    policy_loss        | 106       |\n",
      "|    reward             | 5.9554167 |\n",
      "|    std                | 1.08      |\n",
      "|    value_loss         | 76.5      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 79       |\n",
      "|    iterations         | 12500    |\n",
      "|    time_elapsed       | 785      |\n",
      "|    total_timesteps    | 62500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13.5    |\n",
      "|    explained_variance | -0.00348 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 12499    |\n",
      "|    policy_loss        | -77.1    |\n",
      "|    reward             | 5.531368 |\n",
      "|    std                | 1.08     |\n",
      "|    value_loss         | 41.8     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 79        |\n",
      "|    iterations         | 12600     |\n",
      "|    time_elapsed       | 791       |\n",
      "|    total_timesteps    | 63000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.5     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 12599     |\n",
      "|    policy_loss        | -11.3     |\n",
      "|    reward             | -1.268637 |\n",
      "|    std                | 1.08      |\n",
      "|    value_loss         | 1.43      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 79          |\n",
      "|    iterations         | 12700       |\n",
      "|    time_elapsed       | 797         |\n",
      "|    total_timesteps    | 63500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.5       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 12699       |\n",
      "|    policy_loss        | -71.9       |\n",
      "|    reward             | -0.28883758 |\n",
      "|    std                | 1.09        |\n",
      "|    value_loss         | 35.9        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 79         |\n",
      "|    iterations         | 12800      |\n",
      "|    time_elapsed       | 804        |\n",
      "|    total_timesteps    | 64000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.5      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 12799      |\n",
      "|    policy_loss        | -3.16      |\n",
      "|    reward             | -0.1810942 |\n",
      "|    std                | 1.09       |\n",
      "|    value_loss         | 0.208      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 79        |\n",
      "|    iterations         | 12900     |\n",
      "|    time_elapsed       | 810       |\n",
      "|    total_timesteps    | 64500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.5     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 12899     |\n",
      "|    policy_loss        | 44.9      |\n",
      "|    reward             | 3.9071376 |\n",
      "|    std                | 1.09      |\n",
      "|    value_loss         | 16.3      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 79        |\n",
      "|    iterations         | 13000     |\n",
      "|    time_elapsed       | 817       |\n",
      "|    total_timesteps    | 65000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 12999     |\n",
      "|    policy_loss        | 29.2      |\n",
      "|    reward             | -4.129555 |\n",
      "|    std                | 1.09      |\n",
      "|    value_loss         | 13.4      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 79       |\n",
      "|    iterations         | 13100    |\n",
      "|    time_elapsed       | 823      |\n",
      "|    total_timesteps    | 65500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13.5    |\n",
      "|    explained_variance | -0.0212  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 13099    |\n",
      "|    policy_loss        | -86.7    |\n",
      "|    reward             | 3.969475 |\n",
      "|    std                | 1.09     |\n",
      "|    value_loss         | 43.5     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 79         |\n",
      "|    iterations         | 13200      |\n",
      "|    time_elapsed       | 829        |\n",
      "|    total_timesteps    | 66000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.5      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 13199      |\n",
      "|    policy_loss        | -25.6      |\n",
      "|    reward             | -1.4641087 |\n",
      "|    std                | 1.09       |\n",
      "|    value_loss         | 3.95       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 79         |\n",
      "|    iterations         | 13300      |\n",
      "|    time_elapsed       | 836        |\n",
      "|    total_timesteps    | 66500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.5      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 13299      |\n",
      "|    policy_loss        | -61.5      |\n",
      "|    reward             | 0.24420096 |\n",
      "|    std                | 1.08       |\n",
      "|    value_loss         | 27.4       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 79          |\n",
      "|    iterations         | 13400       |\n",
      "|    time_elapsed       | 842         |\n",
      "|    total_timesteps    | 67000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.4       |\n",
      "|    explained_variance | 5.96e-08    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 13399       |\n",
      "|    policy_loss        | 32          |\n",
      "|    reward             | -0.25503114 |\n",
      "|    std                | 1.08        |\n",
      "|    value_loss         | 6.55        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 79         |\n",
      "|    iterations         | 13500      |\n",
      "|    time_elapsed       | 849        |\n",
      "|    total_timesteps    | 67500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 13499      |\n",
      "|    policy_loss        | 18.5       |\n",
      "|    reward             | -0.2743116 |\n",
      "|    std                | 1.08       |\n",
      "|    value_loss         | 8.26       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 79         |\n",
      "|    iterations         | 13600      |\n",
      "|    time_elapsed       | 855        |\n",
      "|    total_timesteps    | 68000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.4      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 13599      |\n",
      "|    policy_loss        | -72.5      |\n",
      "|    reward             | -2.4042153 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 39.7       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 79        |\n",
      "|    iterations         | 13700     |\n",
      "|    time_elapsed       | 862       |\n",
      "|    total_timesteps    | 68500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 13699     |\n",
      "|    policy_loss        | 35.2      |\n",
      "|    reward             | 2.1854353 |\n",
      "|    std                | 1.08      |\n",
      "|    value_loss         | 17.8      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 79       |\n",
      "|    iterations         | 13800    |\n",
      "|    time_elapsed       | 868      |\n",
      "|    total_timesteps    | 69000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13.4    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 13799    |\n",
      "|    policy_loss        | -5.21    |\n",
      "|    reward             | 2.47322  |\n",
      "|    std                | 1.07     |\n",
      "|    value_loss         | 0.386    |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 79         |\n",
      "|    iterations         | 13900      |\n",
      "|    time_elapsed       | 875        |\n",
      "|    total_timesteps    | 69500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.4      |\n",
      "|    explained_variance | 0.0809     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 13899      |\n",
      "|    policy_loss        | 7.69       |\n",
      "|    reward             | -1.9833437 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 1.86       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 79         |\n",
      "|    iterations         | 14000      |\n",
      "|    time_elapsed       | 881        |\n",
      "|    total_timesteps    | 70000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.4      |\n",
      "|    explained_variance | -0.153     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 13999      |\n",
      "|    policy_loss        | -10.3      |\n",
      "|    reward             | 0.20885919 |\n",
      "|    std                | 1.08       |\n",
      "|    value_loss         | 1.64       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 79         |\n",
      "|    iterations         | 14100      |\n",
      "|    time_elapsed       | 888        |\n",
      "|    total_timesteps    | 70500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.4      |\n",
      "|    explained_variance | 0.414      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 14099      |\n",
      "|    policy_loss        | 2.25       |\n",
      "|    reward             | -0.6464784 |\n",
      "|    std                | 1.08       |\n",
      "|    value_loss         | 0.233      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 79         |\n",
      "|    iterations         | 14200      |\n",
      "|    time_elapsed       | 894        |\n",
      "|    total_timesteps    | 71000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 14199      |\n",
      "|    policy_loss        | 4.28       |\n",
      "|    reward             | 0.08642805 |\n",
      "|    std                | 1.08       |\n",
      "|    value_loss         | 0.118      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 79        |\n",
      "|    iterations         | 14300     |\n",
      "|    time_elapsed       | 901       |\n",
      "|    total_timesteps    | 71500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 14299     |\n",
      "|    policy_loss        | 16.3      |\n",
      "|    reward             | 1.1879456 |\n",
      "|    std                | 1.08      |\n",
      "|    value_loss         | 2.4       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 79         |\n",
      "|    iterations         | 14400      |\n",
      "|    time_elapsed       | 907        |\n",
      "|    total_timesteps    | 72000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.5      |\n",
      "|    explained_variance | -0.024     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 14399      |\n",
      "|    policy_loss        | -59.4      |\n",
      "|    reward             | -0.3921821 |\n",
      "|    std                | 1.09       |\n",
      "|    value_loss         | 22.6       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 79        |\n",
      "|    iterations         | 14500     |\n",
      "|    time_elapsed       | 914       |\n",
      "|    total_timesteps    | 72500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 14499     |\n",
      "|    policy_loss        | 7.88      |\n",
      "|    reward             | 1.1455263 |\n",
      "|    std                | 1.09      |\n",
      "|    value_loss         | 2.27      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 79         |\n",
      "|    iterations         | 14600      |\n",
      "|    time_elapsed       | 920        |\n",
      "|    total_timesteps    | 73000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.5      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 14599      |\n",
      "|    policy_loss        | 27.9       |\n",
      "|    reward             | -2.3936644 |\n",
      "|    std                | 1.08       |\n",
      "|    value_loss         | 7.41       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 79         |\n",
      "|    iterations         | 14700      |\n",
      "|    time_elapsed       | 926        |\n",
      "|    total_timesteps    | 73500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.5      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 14699      |\n",
      "|    policy_loss        | 72         |\n",
      "|    reward             | -7.9357457 |\n",
      "|    std                | 1.09       |\n",
      "|    value_loss         | 37.1       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 79        |\n",
      "|    iterations         | 14800     |\n",
      "|    time_elapsed       | 933       |\n",
      "|    total_timesteps    | 74000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 14799     |\n",
      "|    policy_loss        | 5.86      |\n",
      "|    reward             | 0.6262588 |\n",
      "|    std                | 1.09      |\n",
      "|    value_loss         | 0.3       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 79         |\n",
      "|    iterations         | 14900      |\n",
      "|    time_elapsed       | 939        |\n",
      "|    total_timesteps    | 74500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.5      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 14899      |\n",
      "|    policy_loss        | -39.6      |\n",
      "|    reward             | 0.01856722 |\n",
      "|    std                | 1.09       |\n",
      "|    value_loss         | 8.91       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 79         |\n",
      "|    iterations         | 15000      |\n",
      "|    time_elapsed       | 946        |\n",
      "|    total_timesteps    | 75000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.5      |\n",
      "|    explained_variance | -0.0508    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 14999      |\n",
      "|    policy_loss        | -18.5      |\n",
      "|    reward             | -1.5374486 |\n",
      "|    std                | 1.09       |\n",
      "|    value_loss         | 2.88       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 79        |\n",
      "|    iterations         | 15100     |\n",
      "|    time_elapsed       | 952       |\n",
      "|    total_timesteps    | 75500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15099     |\n",
      "|    policy_loss        | 13.4      |\n",
      "|    reward             | -2.398965 |\n",
      "|    std                | 1.1       |\n",
      "|    value_loss         | 2.1       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 79        |\n",
      "|    iterations         | 15200     |\n",
      "|    time_elapsed       | 958       |\n",
      "|    total_timesteps    | 76000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15199     |\n",
      "|    policy_loss        | -30.6     |\n",
      "|    reward             | 3.2972333 |\n",
      "|    std                | 1.1       |\n",
      "|    value_loss         | 33.6      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 79         |\n",
      "|    iterations         | 15300      |\n",
      "|    time_elapsed       | 965        |\n",
      "|    total_timesteps    | 76500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.6      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 15299      |\n",
      "|    policy_loss        | -315       |\n",
      "|    reward             | -11.634842 |\n",
      "|    std                | 1.1        |\n",
      "|    value_loss         | 774        |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 79          |\n",
      "|    iterations         | 15400       |\n",
      "|    time_elapsed       | 972         |\n",
      "|    total_timesteps    | 77000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.6       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 15399       |\n",
      "|    policy_loss        | 18.9        |\n",
      "|    reward             | -0.05442276 |\n",
      "|    std                | 1.1         |\n",
      "|    value_loss         | 3.35        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 79        |\n",
      "|    iterations         | 15500     |\n",
      "|    time_elapsed       | 978       |\n",
      "|    total_timesteps    | 77500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15499     |\n",
      "|    policy_loss        | 32.2      |\n",
      "|    reward             | 0.8412518 |\n",
      "|    std                | 1.1       |\n",
      "|    value_loss         | 6.53      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 79         |\n",
      "|    iterations         | 15600      |\n",
      "|    time_elapsed       | 984        |\n",
      "|    total_timesteps    | 78000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.6      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 15599      |\n",
      "|    policy_loss        | 12.2       |\n",
      "|    reward             | 0.43851507 |\n",
      "|    std                | 1.1        |\n",
      "|    value_loss         | 1.06       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 79        |\n",
      "|    iterations         | 15700     |\n",
      "|    time_elapsed       | 991       |\n",
      "|    total_timesteps    | 78500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.6     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15699     |\n",
      "|    policy_loss        | -9.34     |\n",
      "|    reward             | 0.9279849 |\n",
      "|    std                | 1.1       |\n",
      "|    value_loss         | 0.941     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 79        |\n",
      "|    iterations         | 15800     |\n",
      "|    time_elapsed       | 997       |\n",
      "|    total_timesteps    | 79000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15799     |\n",
      "|    policy_loss        | 2.47      |\n",
      "|    reward             | 2.3211708 |\n",
      "|    std                | 1.1       |\n",
      "|    value_loss         | 0.634     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 79        |\n",
      "|    iterations         | 15900     |\n",
      "|    time_elapsed       | 1004      |\n",
      "|    total_timesteps    | 79500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.6     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15899     |\n",
      "|    policy_loss        | 11.3      |\n",
      "|    reward             | 3.6897652 |\n",
      "|    std                | 1.1       |\n",
      "|    value_loss         | 22.8      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 79         |\n",
      "|    iterations         | 16000      |\n",
      "|    time_elapsed       | 1010       |\n",
      "|    total_timesteps    | 80000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.6      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 15999      |\n",
      "|    policy_loss        | -4.91      |\n",
      "|    reward             | -1.6700934 |\n",
      "|    std                | 1.1        |\n",
      "|    value_loss         | 2.47       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 79        |\n",
      "|    iterations         | 16100     |\n",
      "|    time_elapsed       | 1017      |\n",
      "|    total_timesteps    | 80500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.7     |\n",
      "|    explained_variance | -0.11     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16099     |\n",
      "|    policy_loss        | 8.73      |\n",
      "|    reward             | 1.3316381 |\n",
      "|    std                | 1.11      |\n",
      "|    value_loss         | 1.95      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 79        |\n",
      "|    iterations         | 16200     |\n",
      "|    time_elapsed       | 1024      |\n",
      "|    total_timesteps    | 81000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.7     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16199     |\n",
      "|    policy_loss        | -32.5     |\n",
      "|    reward             | 0.4200498 |\n",
      "|    std                | 1.11      |\n",
      "|    value_loss         | 5.23      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 79        |\n",
      "|    iterations         | 16300     |\n",
      "|    time_elapsed       | 1030      |\n",
      "|    total_timesteps    | 81500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16299     |\n",
      "|    policy_loss        | -11.6     |\n",
      "|    reward             | 0.5068296 |\n",
      "|    std                | 1.11      |\n",
      "|    value_loss         | 1.44      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 79        |\n",
      "|    iterations         | 16400     |\n",
      "|    time_elapsed       | 1036      |\n",
      "|    total_timesteps    | 82000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16399     |\n",
      "|    policy_loss        | -105      |\n",
      "|    reward             | 1.3274297 |\n",
      "|    std                | 1.11      |\n",
      "|    value_loss         | 62.9      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 79       |\n",
      "|    iterations         | 16500    |\n",
      "|    time_elapsed       | 1042     |\n",
      "|    total_timesteps    | 82500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13.7    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 16499    |\n",
      "|    policy_loss        | -112     |\n",
      "|    reward             | 3.134905 |\n",
      "|    std                | 1.11     |\n",
      "|    value_loss         | 123      |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 79         |\n",
      "|    iterations         | 16600      |\n",
      "|    time_elapsed       | 1048       |\n",
      "|    total_timesteps    | 83000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.7      |\n",
      "|    explained_variance | 0.159      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 16599      |\n",
      "|    policy_loss        | -0.462     |\n",
      "|    reward             | 0.88794726 |\n",
      "|    std                | 1.11       |\n",
      "|    value_loss         | 0.109      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 79         |\n",
      "|    iterations         | 16700      |\n",
      "|    time_elapsed       | 1055       |\n",
      "|    total_timesteps    | 83500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.7      |\n",
      "|    explained_variance | 4.46e-05   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 16699      |\n",
      "|    policy_loss        | -45        |\n",
      "|    reward             | -0.3249595 |\n",
      "|    std                | 1.11       |\n",
      "|    value_loss         | 12.8       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 79         |\n",
      "|    iterations         | 16800      |\n",
      "|    time_elapsed       | 1061       |\n",
      "|    total_timesteps    | 84000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 16799      |\n",
      "|    policy_loss        | 18.2       |\n",
      "|    reward             | -1.2875308 |\n",
      "|    std                | 1.11       |\n",
      "|    value_loss         | 3.08       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 79        |\n",
      "|    iterations         | 16900     |\n",
      "|    time_elapsed       | 1066      |\n",
      "|    total_timesteps    | 84500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16899     |\n",
      "|    policy_loss        | -4.6      |\n",
      "|    reward             | 1.6801485 |\n",
      "|    std                | 1.12      |\n",
      "|    value_loss         | 1.25      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 79        |\n",
      "|    iterations         | 17000     |\n",
      "|    time_elapsed       | 1072      |\n",
      "|    total_timesteps    | 85000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.7     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16999     |\n",
      "|    policy_loss        | 8.62      |\n",
      "|    reward             | -7.142354 |\n",
      "|    std                | 1.11      |\n",
      "|    value_loss         | 1.31      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 79        |\n",
      "|    iterations         | 17100     |\n",
      "|    time_elapsed       | 1079      |\n",
      "|    total_timesteps    | 85500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.7     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17099     |\n",
      "|    policy_loss        | -186      |\n",
      "|    reward             | 5.216588  |\n",
      "|    std                | 1.11      |\n",
      "|    value_loss         | 237       |\n",
      "-------------------------------------\n",
      "day: 2956, episode: 30\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 8088916.20\n",
      "total_reward: 7088916.20\n",
      "total_cost: 14406.71\n",
      "total_trades: 15943\n",
      "Sharpe: 1.003\n",
      "=================================\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 79          |\n",
      "|    iterations         | 17200       |\n",
      "|    time_elapsed       | 1085        |\n",
      "|    total_timesteps    | 86000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.7       |\n",
      "|    explained_variance | 0.294       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 17199       |\n",
      "|    policy_loss        | -1.24       |\n",
      "|    reward             | 0.083201624 |\n",
      "|    std                | 1.11        |\n",
      "|    value_loss         | 0.2         |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 79        |\n",
      "|    iterations         | 17300     |\n",
      "|    time_elapsed       | 1091      |\n",
      "|    total_timesteps    | 86500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17299     |\n",
      "|    policy_loss        | -12       |\n",
      "|    reward             | 0.4016248 |\n",
      "|    std                | 1.11      |\n",
      "|    value_loss         | 0.816     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 79         |\n",
      "|    iterations         | 17400      |\n",
      "|    time_elapsed       | 1097       |\n",
      "|    total_timesteps    | 87000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.7      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 17399      |\n",
      "|    policy_loss        | -34.1      |\n",
      "|    reward             | -2.6011682 |\n",
      "|    std                | 1.11       |\n",
      "|    value_loss         | 6.68       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 79       |\n",
      "|    iterations         | 17500    |\n",
      "|    time_elapsed       | 1103     |\n",
      "|    total_timesteps    | 87500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13.7    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 17499    |\n",
      "|    policy_loss        | -70.7    |\n",
      "|    reward             | 1.750157 |\n",
      "|    std                | 1.11     |\n",
      "|    value_loss         | 38       |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 79        |\n",
      "|    iterations         | 17600     |\n",
      "|    time_elapsed       | 1109      |\n",
      "|    total_timesteps    | 88000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17599     |\n",
      "|    policy_loss        | 196       |\n",
      "|    reward             | 0.7726867 |\n",
      "|    std                | 1.11      |\n",
      "|    value_loss         | 239       |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 79       |\n",
      "|    iterations         | 17700    |\n",
      "|    time_elapsed       | 1115     |\n",
      "|    total_timesteps    | 88500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13.7    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 17699    |\n",
      "|    policy_loss        | -87.5    |\n",
      "|    reward             | 8.365296 |\n",
      "|    std                | 1.11     |\n",
      "|    value_loss         | 63.7     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 79         |\n",
      "|    iterations         | 17800      |\n",
      "|    time_elapsed       | 1121       |\n",
      "|    total_timesteps    | 89000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.7      |\n",
      "|    explained_variance | 0.0248     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 17799      |\n",
      "|    policy_loss        | 0.978      |\n",
      "|    reward             | 0.77147144 |\n",
      "|    std                | 1.12       |\n",
      "|    value_loss         | 0.0787     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 79         |\n",
      "|    iterations         | 17900      |\n",
      "|    time_elapsed       | 1127       |\n",
      "|    total_timesteps    | 89500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.7      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 17899      |\n",
      "|    policy_loss        | -12.4      |\n",
      "|    reward             | -2.1238942 |\n",
      "|    std                | 1.12       |\n",
      "|    value_loss         | 1.16       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 79         |\n",
      "|    iterations         | 18000      |\n",
      "|    time_elapsed       | 1133       |\n",
      "|    total_timesteps    | 90000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 17999      |\n",
      "|    policy_loss        | 24.1       |\n",
      "|    reward             | 0.28372777 |\n",
      "|    std                | 1.12       |\n",
      "|    value_loss         | 4.72       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 79        |\n",
      "|    iterations         | 18100     |\n",
      "|    time_elapsed       | 1140      |\n",
      "|    total_timesteps    | 90500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.8     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18099     |\n",
      "|    policy_loss        | 10.6      |\n",
      "|    reward             | 0.9281079 |\n",
      "|    std                | 1.12      |\n",
      "|    value_loss         | 1.1       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 79        |\n",
      "|    iterations         | 18200     |\n",
      "|    time_elapsed       | 1146      |\n",
      "|    total_timesteps    | 91000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.7     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18199     |\n",
      "|    policy_loss        | 54.8      |\n",
      "|    reward             | 1.8480638 |\n",
      "|    std                | 1.12      |\n",
      "|    value_loss         | 31.8      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 79       |\n",
      "|    iterations         | 18300    |\n",
      "|    time_elapsed       | 1153     |\n",
      "|    total_timesteps    | 91500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13.8    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 18299    |\n",
      "|    policy_loss        | -120     |\n",
      "|    reward             | -2.18929 |\n",
      "|    std                | 1.12     |\n",
      "|    value_loss         | 88.6     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 79        |\n",
      "|    iterations         | 18400     |\n",
      "|    time_elapsed       | 1159      |\n",
      "|    total_timesteps    | 92000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.8     |\n",
      "|    explained_variance | 0.0247    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18399     |\n",
      "|    policy_loss        | 22.7      |\n",
      "|    reward             | 0.5664038 |\n",
      "|    std                | 1.12      |\n",
      "|    value_loss         | 2.88      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 79          |\n",
      "|    iterations         | 18500       |\n",
      "|    time_elapsed       | 1165        |\n",
      "|    total_timesteps    | 92500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.8       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 18499       |\n",
      "|    policy_loss        | -22.1       |\n",
      "|    reward             | -0.04423517 |\n",
      "|    std                | 1.12        |\n",
      "|    value_loss         | 5.06        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 79        |\n",
      "|    iterations         | 18600     |\n",
      "|    time_elapsed       | 1172      |\n",
      "|    total_timesteps    | 93000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.8     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18599     |\n",
      "|    policy_loss        | 1.18      |\n",
      "|    reward             | 1.4582814 |\n",
      "|    std                | 1.12      |\n",
      "|    value_loss         | 0.238     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 79        |\n",
      "|    iterations         | 18700     |\n",
      "|    time_elapsed       | 1178      |\n",
      "|    total_timesteps    | 93500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18699     |\n",
      "|    policy_loss        | -13.8     |\n",
      "|    reward             | 2.2057621 |\n",
      "|    std                | 1.12      |\n",
      "|    value_loss         | 1.37      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 79          |\n",
      "|    iterations         | 18800       |\n",
      "|    time_elapsed       | 1184        |\n",
      "|    total_timesteps    | 94000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.8       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 18799       |\n",
      "|    policy_loss        | 27.8        |\n",
      "|    reward             | -0.21439095 |\n",
      "|    std                | 1.12        |\n",
      "|    value_loss         | 12.1        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 79        |\n",
      "|    iterations         | 18900     |\n",
      "|    time_elapsed       | 1190      |\n",
      "|    total_timesteps    | 94500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18899     |\n",
      "|    policy_loss        | -0.963    |\n",
      "|    reward             | 2.4833724 |\n",
      "|    std                | 1.12      |\n",
      "|    value_loss         | 34.9      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 79        |\n",
      "|    iterations         | 19000     |\n",
      "|    time_elapsed       | 1196      |\n",
      "|    total_timesteps    | 95000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.8     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18999     |\n",
      "|    policy_loss        | -7.32     |\n",
      "|    reward             | 1.5099808 |\n",
      "|    std                | 1.12      |\n",
      "|    value_loss         | 0.644     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 79        |\n",
      "|    iterations         | 19100     |\n",
      "|    time_elapsed       | 1203      |\n",
      "|    total_timesteps    | 95500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19099     |\n",
      "|    policy_loss        | -76       |\n",
      "|    reward             | 0.7560019 |\n",
      "|    std                | 1.12      |\n",
      "|    value_loss         | 31.1      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 79          |\n",
      "|    iterations         | 19200       |\n",
      "|    time_elapsed       | 1209        |\n",
      "|    total_timesteps    | 96000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.8       |\n",
      "|    explained_variance | -0.171      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 19199       |\n",
      "|    policy_loss        | -3.73       |\n",
      "|    reward             | -0.26081204 |\n",
      "|    std                | 1.12        |\n",
      "|    value_loss         | 1.62        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 79        |\n",
      "|    iterations         | 19300     |\n",
      "|    time_elapsed       | 1215      |\n",
      "|    total_timesteps    | 96500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.8     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19299     |\n",
      "|    policy_loss        | -40.5     |\n",
      "|    reward             | -0.841941 |\n",
      "|    std                | 1.12      |\n",
      "|    value_loss         | 9.21      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 79        |\n",
      "|    iterations         | 19400     |\n",
      "|    time_elapsed       | 1221      |\n",
      "|    total_timesteps    | 97000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.8     |\n",
      "|    explained_variance | 0.259     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19399     |\n",
      "|    policy_loss        | 86.6      |\n",
      "|    reward             | 1.1298175 |\n",
      "|    std                | 1.12      |\n",
      "|    value_loss         | 50.3      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 79         |\n",
      "|    iterations         | 19500      |\n",
      "|    time_elapsed       | 1227       |\n",
      "|    total_timesteps    | 97500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.8      |\n",
      "|    explained_variance | -0.0407    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 19499      |\n",
      "|    policy_loss        | -19.9      |\n",
      "|    reward             | -0.6656778 |\n",
      "|    std                | 1.12       |\n",
      "|    value_loss         | 3.6        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 79         |\n",
      "|    iterations         | 19600      |\n",
      "|    time_elapsed       | 1233       |\n",
      "|    total_timesteps    | 98000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.8      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 19599      |\n",
      "|    policy_loss        | 17.3       |\n",
      "|    reward             | 0.25974655 |\n",
      "|    std                | 1.13       |\n",
      "|    value_loss         | 7.94       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 79          |\n",
      "|    iterations         | 19700       |\n",
      "|    time_elapsed       | 1239        |\n",
      "|    total_timesteps    | 98500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.8       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 19699       |\n",
      "|    policy_loss        | -28.1       |\n",
      "|    reward             | -0.49245223 |\n",
      "|    std                | 1.12        |\n",
      "|    value_loss         | 6.77        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 79         |\n",
      "|    iterations         | 19800      |\n",
      "|    time_elapsed       | 1244       |\n",
      "|    total_timesteps    | 99000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.8      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 19799      |\n",
      "|    policy_loss        | 3.85       |\n",
      "|    reward             | -7.1894336 |\n",
      "|    std                | 1.12       |\n",
      "|    value_loss         | 0.875      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 79        |\n",
      "|    iterations         | 19900     |\n",
      "|    time_elapsed       | 1251      |\n",
      "|    total_timesteps    | 99500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19899     |\n",
      "|    policy_loss        | -52.8     |\n",
      "|    reward             | -4.009997 |\n",
      "|    std                | 1.11      |\n",
      "|    value_loss         | 13.2      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 79        |\n",
      "|    iterations         | 20000     |\n",
      "|    time_elapsed       | 1262      |\n",
      "|    total_timesteps    | 100000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19999     |\n",
      "|    policy_loss        | -54.8     |\n",
      "|    reward             | -8.491374 |\n",
      "|    std                | 1.12      |\n",
      "|    value_loss         | 57.4      |\n",
      "-------------------------------------\n",
      "hit end!\n",
      "==============Get Baseline Stats===========\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Shape of DataFrame:  (400, 8)\n",
      "Annual return         -0.022145\n",
      "Cumulative returns    -0.034921\n",
      "Annual volatility      0.176822\n",
      "Sharpe ratio          -0.038668\n",
      "Calmar ratio          -0.100930\n",
      "Stability              0.215672\n",
      "Max drawdown          -0.219408\n",
      "Omega ratio            0.993578\n",
      "Sortino ratio         -0.053987\n",
      "Skew                        NaN\n",
      "Kurtosis                    NaN\n",
      "Tail ratio             1.003436\n",
      "Daily value at risk   -0.022305\n",
      "dtype: float64\n",
      "result:                       a2c           dji\n",
      "date                                  \n",
      "2021-10-01  1.000000e+06  1.000000e+06\n",
      "2021-10-04  9.996990e+05  9.905746e+05\n",
      "2021-10-05  1.000187e+06  9.996566e+05\n",
      "2021-10-06  1.000493e+06  1.002637e+06\n",
      "2021-10-07  1.002376e+06  1.012483e+06\n",
      "...                  ...           ...\n",
      "2023-04-27  1.031383e+06  9.854252e+05\n",
      "2023-04-28  1.041034e+06  9.933491e+05\n",
      "2023-05-01  1.041751e+06  9.919956e+05\n",
      "2023-05-02  1.031289e+06  9.812993e+05\n",
      "2023-05-03  1.022830e+06  9.734251e+05\n",
      "\n",
      "[399 rows x 2 columns]\n",
      "A2C로 얻은 투자 수익률>> 2.28 %\n",
      "{'n_steps': 5, 'ent_coef': 0.01, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to results/a2c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\se99a\\anaconda3\\lib\\site-packages\\pyfolio\\timeseries.py:724: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  stats = pd.Series()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 78         |\n",
      "|    iterations         | 100        |\n",
      "|    time_elapsed       | 6          |\n",
      "|    total_timesteps    | 500        |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -12.9      |\n",
      "|    explained_variance | -0.0512    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 99         |\n",
      "|    policy_loss        | -28        |\n",
      "|    reward             | 0.27486485 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 6.42       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 79        |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 12        |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -12.9     |\n",
      "|    explained_variance | -0.0396   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | -0.292    |\n",
      "|    reward             | 0.3426073 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 1.56      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 79         |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 18         |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -12.9      |\n",
      "|    explained_variance | 0.00703    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | -16.7      |\n",
      "|    reward             | -2.9654608 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 2.71       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 80          |\n",
      "|    iterations         | 400         |\n",
      "|    time_elapsed       | 24          |\n",
      "|    total_timesteps    | 2000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -12.9       |\n",
      "|    explained_variance | 0.0493      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 399         |\n",
      "|    policy_loss        | 15.7        |\n",
      "|    reward             | -0.10473994 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 3.84        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 80          |\n",
      "|    iterations         | 500         |\n",
      "|    time_elapsed       | 30          |\n",
      "|    total_timesteps    | 2500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -12.9       |\n",
      "|    explained_variance | 0.0511      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 499         |\n",
      "|    policy_loss        | -13.2       |\n",
      "|    reward             | -0.25617284 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 1.6         |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 81           |\n",
      "|    iterations         | 600          |\n",
      "|    time_elapsed       | 36           |\n",
      "|    total_timesteps    | 3000         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -12.9        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 599          |\n",
      "|    policy_loss        | 1.38         |\n",
      "|    reward             | -0.049272645 |\n",
      "|    std                | 1.01         |\n",
      "|    value_loss         | 0.0218       |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 81         |\n",
      "|    iterations         | 700        |\n",
      "|    time_elapsed       | 43         |\n",
      "|    total_timesteps    | 3500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -12.9      |\n",
      "|    explained_variance | 0.231      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 699        |\n",
      "|    policy_loss        | 14.2       |\n",
      "|    reward             | -0.9288479 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 1.24       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 81          |\n",
      "|    iterations         | 800         |\n",
      "|    time_elapsed       | 48          |\n",
      "|    total_timesteps    | 4000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -12.9       |\n",
      "|    explained_variance | 0.0968      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 799         |\n",
      "|    policy_loss        | 2.25        |\n",
      "|    reward             | -0.33691198 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 0.589       |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 81        |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 55        |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13       |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | 20.6      |\n",
      "|    reward             | 1.8544593 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 3.13      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 81         |\n",
      "|    iterations         | 1000       |\n",
      "|    time_elapsed       | 61         |\n",
      "|    total_timesteps    | 5000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13        |\n",
      "|    explained_variance | 0.0259     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 999        |\n",
      "|    policy_loss        | -73.5      |\n",
      "|    reward             | 0.11575101 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 68.6       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 80         |\n",
      "|    iterations         | 1100       |\n",
      "|    time_elapsed       | 68         |\n",
      "|    total_timesteps    | 5500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13        |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1099       |\n",
      "|    policy_loss        | 8.7        |\n",
      "|    reward             | -0.9966623 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 1.76       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 80          |\n",
      "|    iterations         | 1200        |\n",
      "|    time_elapsed       | 74          |\n",
      "|    total_timesteps    | 6000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13         |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1199        |\n",
      "|    policy_loss        | -10         |\n",
      "|    reward             | -0.61182404 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 0.674       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 80         |\n",
      "|    iterations         | 1300       |\n",
      "|    time_elapsed       | 80         |\n",
      "|    total_timesteps    | 6500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13        |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1299       |\n",
      "|    policy_loss        | -8.38      |\n",
      "|    reward             | 0.94174874 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 0.848      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 80          |\n",
      "|    iterations         | 1400        |\n",
      "|    time_elapsed       | 86          |\n",
      "|    total_timesteps    | 7000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13         |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1399        |\n",
      "|    policy_loss        | 14.2        |\n",
      "|    reward             | -0.34749004 |\n",
      "|    std                | 1.03        |\n",
      "|    value_loss         | 2.68        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 81        |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 92        |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13       |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | -6.24     |\n",
      "|    reward             | 0.9693961 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 0.988     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 81         |\n",
      "|    iterations         | 1600       |\n",
      "|    time_elapsed       | 98         |\n",
      "|    total_timesteps    | 8000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13        |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1599       |\n",
      "|    policy_loss        | -10.1      |\n",
      "|    reward             | -1.5279104 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 1.84       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 81        |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 104       |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13       |\n",
      "|    explained_variance | 4.71e-06  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | 184       |\n",
      "|    reward             | 11.455375 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 184       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 81        |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 110       |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13       |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | -19.5     |\n",
      "|    reward             | 0.2820659 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 3.2       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 81         |\n",
      "|    iterations         | 1900       |\n",
      "|    time_elapsed       | 116        |\n",
      "|    total_timesteps    | 9500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13        |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1899       |\n",
      "|    policy_loss        | -41.1      |\n",
      "|    reward             | 0.27114204 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 13.3       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 81          |\n",
      "|    iterations         | 2000        |\n",
      "|    time_elapsed       | 123         |\n",
      "|    total_timesteps    | 10000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13         |\n",
      "|    explained_variance | 1.19e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1999        |\n",
      "|    policy_loss        | 5.63        |\n",
      "|    reward             | -0.17105463 |\n",
      "|    std                | 1.03        |\n",
      "|    value_loss         | 1.4         |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 81        |\n",
      "|    iterations         | 2100      |\n",
      "|    time_elapsed       | 128       |\n",
      "|    total_timesteps    | 10500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13       |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2099      |\n",
      "|    policy_loss        | -4.58     |\n",
      "|    reward             | 1.8701854 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 1.57      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 81          |\n",
      "|    iterations         | 2200        |\n",
      "|    time_elapsed       | 134         |\n",
      "|    total_timesteps    | 11000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13         |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 2199        |\n",
      "|    policy_loss        | 28.4        |\n",
      "|    reward             | -0.27539843 |\n",
      "|    std                | 1.03        |\n",
      "|    value_loss         | 5.96        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 81        |\n",
      "|    iterations         | 2300      |\n",
      "|    time_elapsed       | 140       |\n",
      "|    total_timesteps    | 11500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13       |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2299      |\n",
      "|    policy_loss        | 115       |\n",
      "|    reward             | 3.0733285 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 74.7      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 81        |\n",
      "|    iterations         | 2400      |\n",
      "|    time_elapsed       | 147       |\n",
      "|    total_timesteps    | 12000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13       |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2399      |\n",
      "|    policy_loss        | 28.8      |\n",
      "|    reward             | 0.3167904 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 4.84      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 81         |\n",
      "|    iterations         | 2500       |\n",
      "|    time_elapsed       | 153        |\n",
      "|    total_timesteps    | 12500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13        |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2499       |\n",
      "|    policy_loss        | -11.9      |\n",
      "|    reward             | 0.53614366 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 1.62       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 81         |\n",
      "|    iterations         | 2600       |\n",
      "|    time_elapsed       | 159        |\n",
      "|    total_timesteps    | 13000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13        |\n",
      "|    explained_variance | 0.0682     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2599       |\n",
      "|    policy_loss        | 5.13       |\n",
      "|    reward             | 0.57795525 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 1.05       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 81          |\n",
      "|    iterations         | 2700        |\n",
      "|    time_elapsed       | 165         |\n",
      "|    total_timesteps    | 13500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13         |\n",
      "|    explained_variance | 0.00898     |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 2699        |\n",
      "|    policy_loss        | -10.8       |\n",
      "|    reward             | -0.94498956 |\n",
      "|    std                | 1.03        |\n",
      "|    value_loss         | 1.28        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 80         |\n",
      "|    iterations         | 2800       |\n",
      "|    time_elapsed       | 173        |\n",
      "|    total_timesteps    | 14000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13        |\n",
      "|    explained_variance | -0.0196    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2799       |\n",
      "|    policy_loss        | -39.5      |\n",
      "|    reward             | 0.16295852 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 8.47       |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 79           |\n",
      "|    iterations         | 2900         |\n",
      "|    time_elapsed       | 181          |\n",
      "|    total_timesteps    | 14500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -13          |\n",
      "|    explained_variance | -0.0683      |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 2899         |\n",
      "|    policy_loss        | 29.3         |\n",
      "|    reward             | -0.057187773 |\n",
      "|    std                | 1.03         |\n",
      "|    value_loss         | 40.8         |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 79          |\n",
      "|    iterations         | 3000        |\n",
      "|    time_elapsed       | 188         |\n",
      "|    total_timesteps    | 15000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13         |\n",
      "|    explained_variance | 0.0961      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 2999        |\n",
      "|    policy_loss        | 4.66        |\n",
      "|    reward             | -0.51950544 |\n",
      "|    std                | 1.03        |\n",
      "|    value_loss         | 0.247       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 79         |\n",
      "|    iterations         | 3100       |\n",
      "|    time_elapsed       | 195        |\n",
      "|    total_timesteps    | 15500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13        |\n",
      "|    explained_variance | 0.0305     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3099       |\n",
      "|    policy_loss        | -32.7      |\n",
      "|    reward             | 0.55066764 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 12         |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 79        |\n",
      "|    iterations         | 3200      |\n",
      "|    time_elapsed       | 201       |\n",
      "|    total_timesteps    | 16000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13       |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3199      |\n",
      "|    policy_loss        | 39.8      |\n",
      "|    reward             | 0.9858543 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 15.7      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 79         |\n",
      "|    iterations         | 3300       |\n",
      "|    time_elapsed       | 207        |\n",
      "|    total_timesteps    | 16500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13        |\n",
      "|    explained_variance | -0.00696   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3299       |\n",
      "|    policy_loss        | -2.98      |\n",
      "|    reward             | -1.1685046 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 0.387      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 79         |\n",
      "|    iterations         | 3400       |\n",
      "|    time_elapsed       | 213        |\n",
      "|    total_timesteps    | 17000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13        |\n",
      "|    explained_variance | 0.0014     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3399       |\n",
      "|    policy_loss        | -91.3      |\n",
      "|    reward             | -1.2248652 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 118        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 79         |\n",
      "|    iterations         | 3500       |\n",
      "|    time_elapsed       | 220        |\n",
      "|    total_timesteps    | 17500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -12.9      |\n",
      "|    explained_variance | 0.00364    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3499       |\n",
      "|    policy_loss        | 99.4       |\n",
      "|    reward             | -1.1284032 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 64.1       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 79         |\n",
      "|    iterations         | 3600       |\n",
      "|    time_elapsed       | 226        |\n",
      "|    total_timesteps    | 18000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -12.9      |\n",
      "|    explained_variance | -0.0242    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3599       |\n",
      "|    policy_loss        | -3.55      |\n",
      "|    reward             | 0.68278736 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 0.126      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 79          |\n",
      "|    iterations         | 3700        |\n",
      "|    time_elapsed       | 232         |\n",
      "|    total_timesteps    | 18500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -12.9       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 3699        |\n",
      "|    policy_loss        | 4.42        |\n",
      "|    reward             | -0.15598045 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 0.953       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 79         |\n",
      "|    iterations         | 3800       |\n",
      "|    time_elapsed       | 239        |\n",
      "|    total_timesteps    | 19000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13        |\n",
      "|    explained_variance | -0.00418   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3799       |\n",
      "|    policy_loss        | 4.92       |\n",
      "|    reward             | 0.11890749 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 3.98       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 79        |\n",
      "|    iterations         | 3900      |\n",
      "|    time_elapsed       | 245       |\n",
      "|    total_timesteps    | 19500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13       |\n",
      "|    explained_variance | 0.9       |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3899      |\n",
      "|    policy_loss        | -0.071    |\n",
      "|    reward             | 0.5581114 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 0.049     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 79         |\n",
      "|    iterations         | 4000       |\n",
      "|    time_elapsed       | 252        |\n",
      "|    total_timesteps    | 20000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13        |\n",
      "|    explained_variance | 0.0136     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3999       |\n",
      "|    policy_loss        | -70.5      |\n",
      "|    reward             | -3.8034446 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 29.5       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 79        |\n",
      "|    iterations         | 4100      |\n",
      "|    time_elapsed       | 258       |\n",
      "|    total_timesteps    | 20500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13       |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4099      |\n",
      "|    policy_loss        | -10.1     |\n",
      "|    reward             | 1.4093034 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 1.12      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 79         |\n",
      "|    iterations         | 4200       |\n",
      "|    time_elapsed       | 264        |\n",
      "|    total_timesteps    | 21000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13        |\n",
      "|    explained_variance | -0.0156    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4199       |\n",
      "|    policy_loss        | -4.24      |\n",
      "|    reward             | -0.9783023 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 0.819      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 79          |\n",
      "|    iterations         | 4300        |\n",
      "|    time_elapsed       | 271         |\n",
      "|    total_timesteps    | 21500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13         |\n",
      "|    explained_variance | 0.0467      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 4299        |\n",
      "|    policy_loss        | 26.5        |\n",
      "|    reward             | -0.08400414 |\n",
      "|    std                | 1.03        |\n",
      "|    value_loss         | 5.55        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 79         |\n",
      "|    iterations         | 4400       |\n",
      "|    time_elapsed       | 277        |\n",
      "|    total_timesteps    | 22000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13        |\n",
      "|    explained_variance | 0.111      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4399       |\n",
      "|    policy_loss        | -12.5      |\n",
      "|    reward             | -3.1857135 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 1.03       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 79        |\n",
      "|    iterations         | 4500      |\n",
      "|    time_elapsed       | 283       |\n",
      "|    total_timesteps    | 22500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13       |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4499      |\n",
      "|    policy_loss        | 21        |\n",
      "|    reward             | 3.0466661 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 3.03      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 79         |\n",
      "|    iterations         | 4600       |\n",
      "|    time_elapsed       | 289        |\n",
      "|    total_timesteps    | 23000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13        |\n",
      "|    explained_variance | 0.00419    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4599       |\n",
      "|    policy_loss        | 34.9       |\n",
      "|    reward             | 0.43967682 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 12.5       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 79          |\n",
      "|    iterations         | 4700        |\n",
      "|    time_elapsed       | 296         |\n",
      "|    total_timesteps    | 23500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13         |\n",
      "|    explained_variance | -0.000404   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 4699        |\n",
      "|    policy_loss        | 69.3        |\n",
      "|    reward             | -0.06081934 |\n",
      "|    std                | 1.03        |\n",
      "|    value_loss         | 44          |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 79         |\n",
      "|    iterations         | 4800       |\n",
      "|    time_elapsed       | 302        |\n",
      "|    total_timesteps    | 24000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13        |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4799       |\n",
      "|    policy_loss        | -5.49      |\n",
      "|    reward             | -0.6075358 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 1.12       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 79          |\n",
      "|    iterations         | 4900        |\n",
      "|    time_elapsed       | 308         |\n",
      "|    total_timesteps    | 24500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13         |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 4899        |\n",
      "|    policy_loss        | 11.9        |\n",
      "|    reward             | -0.10220446 |\n",
      "|    std                | 1.03        |\n",
      "|    value_loss         | 1.19        |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 79       |\n",
      "|    iterations         | 5000     |\n",
      "|    time_elapsed       | 314      |\n",
      "|    total_timesteps    | 25000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13      |\n",
      "|    explained_variance | 1.79e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4999     |\n",
      "|    policy_loss        | -35.8    |\n",
      "|    reward             | 1.029851 |\n",
      "|    std                | 1.03     |\n",
      "|    value_loss         | 8.13     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 79         |\n",
      "|    iterations         | 5100       |\n",
      "|    time_elapsed       | 321        |\n",
      "|    total_timesteps    | 25500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13        |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5099       |\n",
      "|    policy_loss        | 38         |\n",
      "|    reward             | 0.07806224 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 10.9       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 79         |\n",
      "|    iterations         | 5200       |\n",
      "|    time_elapsed       | 327        |\n",
      "|    total_timesteps    | 26000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13        |\n",
      "|    explained_variance | 0.00687    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5199       |\n",
      "|    policy_loss        | 36.1       |\n",
      "|    reward             | 0.32880774 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 9.42       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 79         |\n",
      "|    iterations         | 5300       |\n",
      "|    time_elapsed       | 333        |\n",
      "|    total_timesteps    | 26500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13        |\n",
      "|    explained_variance | 0.176      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5299       |\n",
      "|    policy_loss        | 53.2       |\n",
      "|    reward             | -3.8342888 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 19.8       |\n",
      "--------------------------------------\n",
      "day: 2956, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4867891.48\n",
      "total_reward: 3867891.48\n",
      "total_cost: 37506.80\n",
      "total_trades: 18503\n",
      "Sharpe: 0.842\n",
      "=================================\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 79         |\n",
      "|    iterations         | 5400       |\n",
      "|    time_elapsed       | 340        |\n",
      "|    total_timesteps    | 27000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13        |\n",
      "|    explained_variance | -0.00686   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5399       |\n",
      "|    policy_loss        | -32.3      |\n",
      "|    reward             | -1.1327589 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 8.7        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 79         |\n",
      "|    iterations         | 5500       |\n",
      "|    time_elapsed       | 346        |\n",
      "|    total_timesteps    | 27500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13        |\n",
      "|    explained_variance | 0.111      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5499       |\n",
      "|    policy_loss        | 25.2       |\n",
      "|    reward             | 0.21442883 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 5.82       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 79        |\n",
      "|    iterations         | 5600      |\n",
      "|    time_elapsed       | 352       |\n",
      "|    total_timesteps    | 28000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13       |\n",
      "|    explained_variance | 0.237     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5599      |\n",
      "|    policy_loss        | -9.6      |\n",
      "|    reward             | 0.7038254 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 4.66      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 79       |\n",
      "|    iterations         | 5700     |\n",
      "|    time_elapsed       | 359      |\n",
      "|    total_timesteps    | 28500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13      |\n",
      "|    explained_variance | -0.0423  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5699     |\n",
      "|    policy_loss        | -12.3    |\n",
      "|    reward             | 1.85528  |\n",
      "|    std                | 1.03     |\n",
      "|    value_loss         | 5.93     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 79       |\n",
      "|    iterations         | 5800     |\n",
      "|    time_elapsed       | 365      |\n",
      "|    total_timesteps    | 29000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13      |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5799     |\n",
      "|    policy_loss        | 57.8     |\n",
      "|    reward             | 2.415532 |\n",
      "|    std                | 1.03     |\n",
      "|    value_loss         | 21.9     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 79        |\n",
      "|    iterations         | 5900      |\n",
      "|    time_elapsed       | 372       |\n",
      "|    total_timesteps    | 29500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13       |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5899      |\n",
      "|    policy_loss        | -38.8     |\n",
      "|    reward             | 1.0453361 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 8.88      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 79         |\n",
      "|    iterations         | 6000       |\n",
      "|    time_elapsed       | 378        |\n",
      "|    total_timesteps    | 30000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13        |\n",
      "|    explained_variance | -0.0439    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5999       |\n",
      "|    policy_loss        | -52.1      |\n",
      "|    reward             | 0.48516014 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 15.8       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 79        |\n",
      "|    iterations         | 6100      |\n",
      "|    time_elapsed       | 384       |\n",
      "|    total_timesteps    | 30500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13       |\n",
      "|    explained_variance | 0.0785    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6099      |\n",
      "|    policy_loss        | 18.7      |\n",
      "|    reward             | 0.3814242 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 2.66      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 79        |\n",
      "|    iterations         | 6200      |\n",
      "|    time_elapsed       | 391       |\n",
      "|    total_timesteps    | 31000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13       |\n",
      "|    explained_variance | 0.402     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6199      |\n",
      "|    policy_loss        | -25.6     |\n",
      "|    reward             | -2.222995 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 15.4      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 79         |\n",
      "|    iterations         | 6300       |\n",
      "|    time_elapsed       | 397        |\n",
      "|    total_timesteps    | 31500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13        |\n",
      "|    explained_variance | 0.229      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6299       |\n",
      "|    policy_loss        | 29.2       |\n",
      "|    reward             | 0.30415517 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 6.37       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 79       |\n",
      "|    iterations         | 6400     |\n",
      "|    time_elapsed       | 403      |\n",
      "|    total_timesteps    | 32000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13      |\n",
      "|    explained_variance | 0.0968   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6399     |\n",
      "|    policy_loss        | 41.7     |\n",
      "|    reward             | 5.08771  |\n",
      "|    std                | 1.03     |\n",
      "|    value_loss         | 18.6     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 79        |\n",
      "|    iterations         | 6500      |\n",
      "|    time_elapsed       | 409       |\n",
      "|    total_timesteps    | 32500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13       |\n",
      "|    explained_variance | -0.0981   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6499      |\n",
      "|    policy_loss        | -54       |\n",
      "|    reward             | 2.3647146 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 23.3      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 79         |\n",
      "|    iterations         | 6600       |\n",
      "|    time_elapsed       | 416        |\n",
      "|    total_timesteps    | 33000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13        |\n",
      "|    explained_variance | -1.07      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6599       |\n",
      "|    policy_loss        | -58.9      |\n",
      "|    reward             | -1.8132316 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 28.9       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 79       |\n",
      "|    iterations         | 6700     |\n",
      "|    time_elapsed       | 422      |\n",
      "|    total_timesteps    | 33500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13      |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6699     |\n",
      "|    policy_loss        | 11.2     |\n",
      "|    reward             | 1.040075 |\n",
      "|    std                | 1.03     |\n",
      "|    value_loss         | 0.91     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 79         |\n",
      "|    iterations         | 6800       |\n",
      "|    time_elapsed       | 429        |\n",
      "|    total_timesteps    | 34000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13        |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6799       |\n",
      "|    policy_loss        | 22.8       |\n",
      "|    reward             | -1.5664958 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 3.67       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 79        |\n",
      "|    iterations         | 6900      |\n",
      "|    time_elapsed       | 436       |\n",
      "|    total_timesteps    | 34500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13       |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6899      |\n",
      "|    policy_loss        | -18.8     |\n",
      "|    reward             | 1.5890218 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 4         |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 79        |\n",
      "|    iterations         | 7000      |\n",
      "|    time_elapsed       | 442       |\n",
      "|    total_timesteps    | 35000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6999      |\n",
      "|    policy_loss        | 24.3      |\n",
      "|    reward             | 1.1739386 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 4.59      |\n",
      "-------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 79           |\n",
      "|    iterations         | 7100         |\n",
      "|    time_elapsed       | 449          |\n",
      "|    total_timesteps    | 35500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -13          |\n",
      "|    explained_variance | -1.19e-07    |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 7099         |\n",
      "|    policy_loss        | -4.32        |\n",
      "|    reward             | -0.009152362 |\n",
      "|    std                | 1.03         |\n",
      "|    value_loss         | 0.0867       |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 79          |\n",
      "|    iterations         | 7200        |\n",
      "|    time_elapsed       | 455         |\n",
      "|    total_timesteps    | 36000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13         |\n",
      "|    explained_variance | -0.0641     |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 7199        |\n",
      "|    policy_loss        | 10.5        |\n",
      "|    reward             | -0.53604776 |\n",
      "|    std                | 1.03        |\n",
      "|    value_loss         | 1.06        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 79         |\n",
      "|    iterations         | 7300       |\n",
      "|    time_elapsed       | 461        |\n",
      "|    total_timesteps    | 36500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13        |\n",
      "|    explained_variance | -0.0312    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7299       |\n",
      "|    policy_loss        | 0.68       |\n",
      "|    reward             | -0.5064141 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 0.256      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 78        |\n",
      "|    iterations         | 7400      |\n",
      "|    time_elapsed       | 468       |\n",
      "|    total_timesteps    | 37000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13       |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7399      |\n",
      "|    policy_loss        | -63.4     |\n",
      "|    reward             | 1.9872673 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 27.2      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 78       |\n",
      "|    iterations         | 7500     |\n",
      "|    time_elapsed       | 474      |\n",
      "|    total_timesteps    | 37500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13      |\n",
      "|    explained_variance | -0.396   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7499     |\n",
      "|    policy_loss        | 6.96     |\n",
      "|    reward             | 1.913726 |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 0.252    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 78        |\n",
      "|    iterations         | 7600      |\n",
      "|    time_elapsed       | 481       |\n",
      "|    total_timesteps    | 38000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13       |\n",
      "|    explained_variance | -0.000786 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7599      |\n",
      "|    policy_loss        | 18.9      |\n",
      "|    reward             | 1.7795266 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 4.8       |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 78          |\n",
      "|    iterations         | 7700        |\n",
      "|    time_elapsed       | 487         |\n",
      "|    total_timesteps    | 38500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13         |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 7699        |\n",
      "|    policy_loss        | -0.188      |\n",
      "|    reward             | 0.043427024 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 0.0514      |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 78          |\n",
      "|    iterations         | 7800        |\n",
      "|    time_elapsed       | 493         |\n",
      "|    total_timesteps    | 39000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13         |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 7799        |\n",
      "|    policy_loss        | 8.69        |\n",
      "|    reward             | -0.50766206 |\n",
      "|    std                | 1.03        |\n",
      "|    value_loss         | 0.898       |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 78        |\n",
      "|    iterations         | 7900      |\n",
      "|    time_elapsed       | 501       |\n",
      "|    total_timesteps    | 39500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13       |\n",
      "|    explained_variance | 0.00516   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7899      |\n",
      "|    policy_loss        | -27.1     |\n",
      "|    reward             | 0.8026825 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 5.08      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 78         |\n",
      "|    iterations         | 8000       |\n",
      "|    time_elapsed       | 508        |\n",
      "|    total_timesteps    | 40000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13        |\n",
      "|    explained_variance | -0.128     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7999       |\n",
      "|    policy_loss        | -14.7      |\n",
      "|    reward             | 0.11437894 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 2.2        |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 78       |\n",
      "|    iterations         | 8100     |\n",
      "|    time_elapsed       | 515      |\n",
      "|    total_timesteps    | 40500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13      |\n",
      "|    explained_variance | -0.00864 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8099     |\n",
      "|    policy_loss        | -77      |\n",
      "|    reward             | 4.683803 |\n",
      "|    std                | 1.03     |\n",
      "|    value_loss         | 52.3     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 78        |\n",
      "|    iterations         | 8200      |\n",
      "|    time_elapsed       | 521       |\n",
      "|    total_timesteps    | 41000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13       |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8199      |\n",
      "|    policy_loss        | -301      |\n",
      "|    reward             | -8.685814 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 610       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 78         |\n",
      "|    iterations         | 8300       |\n",
      "|    time_elapsed       | 528        |\n",
      "|    total_timesteps    | 41500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13        |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8299       |\n",
      "|    policy_loss        | -39.6      |\n",
      "|    reward             | -2.2435644 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 10.9       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 78         |\n",
      "|    iterations         | 8400       |\n",
      "|    time_elapsed       | 534        |\n",
      "|    total_timesteps    | 42000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.1      |\n",
      "|    explained_variance | 0.0123     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8399       |\n",
      "|    policy_loss        | -27.5      |\n",
      "|    reward             | 0.27432942 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 6.74       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 78        |\n",
      "|    iterations         | 8500      |\n",
      "|    time_elapsed       | 541       |\n",
      "|    total_timesteps    | 42500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13       |\n",
      "|    explained_variance | -0.00115  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8499      |\n",
      "|    policy_loss        | -1.29     |\n",
      "|    reward             | 1.7162228 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 0.724     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 78        |\n",
      "|    iterations         | 8600      |\n",
      "|    time_elapsed       | 549       |\n",
      "|    total_timesteps    | 43000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.1     |\n",
      "|    explained_variance | -0.0132   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8599      |\n",
      "|    policy_loss        | -4.3      |\n",
      "|    reward             | 1.8541884 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 1.26      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 78        |\n",
      "|    iterations         | 8700      |\n",
      "|    time_elapsed       | 557       |\n",
      "|    total_timesteps    | 43500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.1     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8699      |\n",
      "|    policy_loss        | -60.7     |\n",
      "|    reward             | 1.6543769 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 28.8      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 77         |\n",
      "|    iterations         | 8800       |\n",
      "|    time_elapsed       | 564        |\n",
      "|    total_timesteps    | 44000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.1      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8799       |\n",
      "|    policy_loss        | 125        |\n",
      "|    reward             | -3.2196355 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 112        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 77        |\n",
      "|    iterations         | 8900      |\n",
      "|    time_elapsed       | 571       |\n",
      "|    total_timesteps    | 44500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.1     |\n",
      "|    explained_variance | -0.00129  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8899      |\n",
      "|    policy_loss        | 29.2      |\n",
      "|    reward             | 2.3879273 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 5.49      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 77         |\n",
      "|    iterations         | 9000       |\n",
      "|    time_elapsed       | 577        |\n",
      "|    total_timesteps    | 45000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8999       |\n",
      "|    policy_loss        | 26         |\n",
      "|    reward             | 0.21504015 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 4.35       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 77         |\n",
      "|    iterations         | 9100       |\n",
      "|    time_elapsed       | 584        |\n",
      "|    total_timesteps    | 45500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9099       |\n",
      "|    policy_loss        | 13.2       |\n",
      "|    reward             | 0.07599043 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 1.53       |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 77           |\n",
      "|    iterations         | 9200         |\n",
      "|    time_elapsed       | 590          |\n",
      "|    total_timesteps    | 46000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -13          |\n",
      "|    explained_variance | 0.0245       |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 9199         |\n",
      "|    policy_loss        | 26.7         |\n",
      "|    reward             | -0.017959114 |\n",
      "|    std                | 1.03         |\n",
      "|    value_loss         | 5.78         |\n",
      "----------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 77        |\n",
      "|    iterations         | 9300      |\n",
      "|    time_elapsed       | 597       |\n",
      "|    total_timesteps    | 46500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13       |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9299      |\n",
      "|    policy_loss        | 6.91      |\n",
      "|    reward             | 0.8015005 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 0.548     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 77         |\n",
      "|    iterations         | 9400       |\n",
      "|    time_elapsed       | 603        |\n",
      "|    total_timesteps    | 47000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13        |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9399       |\n",
      "|    policy_loss        | 22.4       |\n",
      "|    reward             | 0.06241595 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 11.7       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 77        |\n",
      "|    iterations         | 9500      |\n",
      "|    time_elapsed       | 609       |\n",
      "|    total_timesteps    | 47500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13       |\n",
      "|    explained_variance | -0.015    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9499      |\n",
      "|    policy_loss        | 9.75      |\n",
      "|    reward             | 0.7607461 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 1.11      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 77         |\n",
      "|    iterations         | 9600       |\n",
      "|    time_elapsed       | 617        |\n",
      "|    total_timesteps    | 48000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13        |\n",
      "|    explained_variance | 0.0752     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9599       |\n",
      "|    policy_loss        | -3.97      |\n",
      "|    reward             | -1.5180316 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 0.884      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 77         |\n",
      "|    iterations         | 9700       |\n",
      "|    time_elapsed       | 623        |\n",
      "|    total_timesteps    | 48500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13        |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9699       |\n",
      "|    policy_loss        | 7.8        |\n",
      "|    reward             | -1.4066269 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 0.881      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 77          |\n",
      "|    iterations         | 9800        |\n",
      "|    time_elapsed       | 629         |\n",
      "|    total_timesteps    | 49000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13         |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 9799        |\n",
      "|    policy_loss        | -73.1       |\n",
      "|    reward             | -0.88047177 |\n",
      "|    std                | 1.03        |\n",
      "|    value_loss         | 32.9        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 77        |\n",
      "|    iterations         | 9900      |\n",
      "|    time_elapsed       | 636       |\n",
      "|    total_timesteps    | 49500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13       |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9899      |\n",
      "|    policy_loss        | -1.2      |\n",
      "|    reward             | 1.1074562 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 0.832     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 77         |\n",
      "|    iterations         | 10000      |\n",
      "|    time_elapsed       | 642        |\n",
      "|    total_timesteps    | 50000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13        |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9999       |\n",
      "|    policy_loss        | 3.84       |\n",
      "|    reward             | -5.8898067 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 2.11       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 77        |\n",
      "|    iterations         | 10100     |\n",
      "|    time_elapsed       | 648       |\n",
      "|    total_timesteps    | 50500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.1     |\n",
      "|    explained_variance | -0.0512   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 10099     |\n",
      "|    policy_loss        | 6.71      |\n",
      "|    reward             | 1.4827405 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 1.26      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 77        |\n",
      "|    iterations         | 10200     |\n",
      "|    time_elapsed       | 655       |\n",
      "|    total_timesteps    | 51000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 10199     |\n",
      "|    policy_loss        | 31.2      |\n",
      "|    reward             | 1.4279021 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 5.11      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 77        |\n",
      "|    iterations         | 10300     |\n",
      "|    time_elapsed       | 661       |\n",
      "|    total_timesteps    | 51500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.1     |\n",
      "|    explained_variance | -0.0252   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 10299     |\n",
      "|    policy_loss        | -2.46     |\n",
      "|    reward             | 1.5737927 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 0.12      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 77         |\n",
      "|    iterations         | 10400      |\n",
      "|    time_elapsed       | 668        |\n",
      "|    total_timesteps    | 52000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.1      |\n",
      "|    explained_variance | -0.1       |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 10399      |\n",
      "|    policy_loss        | 114        |\n",
      "|    reward             | -0.6401213 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 69.6       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 77        |\n",
      "|    iterations         | 10500     |\n",
      "|    time_elapsed       | 674       |\n",
      "|    total_timesteps    | 52500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.2     |\n",
      "|    explained_variance | -0.00677  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 10499     |\n",
      "|    policy_loss        | 54.5      |\n",
      "|    reward             | -4.974034 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 36.3      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 77         |\n",
      "|    iterations         | 10600      |\n",
      "|    time_elapsed       | 680        |\n",
      "|    total_timesteps    | 53000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.1      |\n",
      "|    explained_variance | 0.00171    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 10599      |\n",
      "|    policy_loss        | -123       |\n",
      "|    reward             | 0.11713681 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 167        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 77         |\n",
      "|    iterations         | 10700      |\n",
      "|    time_elapsed       | 687        |\n",
      "|    total_timesteps    | 53500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.1      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 10699      |\n",
      "|    policy_loss        | 8.46       |\n",
      "|    reward             | 0.20135477 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 0.717      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 77          |\n",
      "|    iterations         | 10800       |\n",
      "|    time_elapsed       | 693         |\n",
      "|    total_timesteps    | 54000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.1       |\n",
      "|    explained_variance | 0.0181      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 10799       |\n",
      "|    policy_loss        | -1.57       |\n",
      "|    reward             | -0.42929086 |\n",
      "|    std                | 1.04        |\n",
      "|    value_loss         | 0.665       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 77         |\n",
      "|    iterations         | 10900      |\n",
      "|    time_elapsed       | 699        |\n",
      "|    total_timesteps    | 54500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.1      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 10899      |\n",
      "|    policy_loss        | 12         |\n",
      "|    reward             | -2.9476166 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 2.3        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 77         |\n",
      "|    iterations         | 11000      |\n",
      "|    time_elapsed       | 706        |\n",
      "|    total_timesteps    | 55000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 10999      |\n",
      "|    policy_loss        | -5.03      |\n",
      "|    reward             | 0.19033615 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 0.456      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 77         |\n",
      "|    iterations         | 11100      |\n",
      "|    time_elapsed       | 712        |\n",
      "|    total_timesteps    | 55500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 11099      |\n",
      "|    policy_loss        | 93.9       |\n",
      "|    reward             | 0.24738418 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 72.7       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 77          |\n",
      "|    iterations         | 11200       |\n",
      "|    time_elapsed       | 718         |\n",
      "|    total_timesteps    | 56000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.1       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 11199       |\n",
      "|    policy_loss        | -88.4       |\n",
      "|    reward             | -0.92179435 |\n",
      "|    std                | 1.04        |\n",
      "|    value_loss         | 54.6        |\n",
      "---------------------------------------\n",
      "day: 2956, episode: 20\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4381276.21\n",
      "total_reward: 3381276.21\n",
      "total_cost: 23361.16\n",
      "total_trades: 20097\n",
      "Sharpe: 0.769\n",
      "=================================\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 77         |\n",
      "|    iterations         | 11300      |\n",
      "|    time_elapsed       | 725        |\n",
      "|    total_timesteps    | 56500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.1      |\n",
      "|    explained_variance | 0.0626     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 11299      |\n",
      "|    policy_loss        | 20.8       |\n",
      "|    reward             | -0.5375731 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 2.58       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 77        |\n",
      "|    iterations         | 11400     |\n",
      "|    time_elapsed       | 731       |\n",
      "|    total_timesteps    | 57000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.1     |\n",
      "|    explained_variance | -0.102    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 11399     |\n",
      "|    policy_loss        | 0.0813    |\n",
      "|    reward             | 0.3000247 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 0.376     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 77        |\n",
      "|    iterations         | 11500     |\n",
      "|    time_elapsed       | 738       |\n",
      "|    total_timesteps    | 57500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 11499     |\n",
      "|    policy_loss        | 3         |\n",
      "|    reward             | 2.5695891 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 1.39      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 77        |\n",
      "|    iterations         | 11600     |\n",
      "|    time_elapsed       | 744       |\n",
      "|    total_timesteps    | 58000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.1     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 11599     |\n",
      "|    policy_loss        | -2.61     |\n",
      "|    reward             | -0.555161 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 0.857     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 77         |\n",
      "|    iterations         | 11700      |\n",
      "|    time_elapsed       | 751        |\n",
      "|    total_timesteps    | 58500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 11699      |\n",
      "|    policy_loss        | 51.5       |\n",
      "|    reward             | -1.8260301 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 18.1       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 77       |\n",
      "|    iterations         | 11800    |\n",
      "|    time_elapsed       | 757      |\n",
      "|    total_timesteps    | 59000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13.1    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 11799    |\n",
      "|    policy_loss        | -34.6    |\n",
      "|    reward             | 3.956555 |\n",
      "|    std                | 1.04     |\n",
      "|    value_loss         | 35.1     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 77         |\n",
      "|    iterations         | 11900      |\n",
      "|    time_elapsed       | 763        |\n",
      "|    total_timesteps    | 59500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 11899      |\n",
      "|    policy_loss        | -21.5      |\n",
      "|    reward             | -0.4034629 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 3.73       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 77         |\n",
      "|    iterations         | 12000      |\n",
      "|    time_elapsed       | 769        |\n",
      "|    total_timesteps    | 60000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.2      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 11999      |\n",
      "|    policy_loss        | -27.3      |\n",
      "|    reward             | -2.7324736 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 4.03       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 77         |\n",
      "|    iterations         | 12100      |\n",
      "|    time_elapsed       | 775        |\n",
      "|    total_timesteps    | 60500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 12099      |\n",
      "|    policy_loss        | -8.16      |\n",
      "|    reward             | 0.92997414 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 0.46       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 77        |\n",
      "|    iterations         | 12200     |\n",
      "|    time_elapsed       | 782       |\n",
      "|    total_timesteps    | 61000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 12199     |\n",
      "|    policy_loss        | -65.4     |\n",
      "|    reward             | 0.6165552 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 27.7      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 77        |\n",
      "|    iterations         | 12300     |\n",
      "|    time_elapsed       | 788       |\n",
      "|    total_timesteps    | 61500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 12299     |\n",
      "|    policy_loss        | -62.1     |\n",
      "|    reward             | 3.3725026 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 28.8      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 77        |\n",
      "|    iterations         | 12400     |\n",
      "|    time_elapsed       | 795       |\n",
      "|    total_timesteps    | 62000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 12399     |\n",
      "|    policy_loss        | 78.1      |\n",
      "|    reward             | 4.2632585 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 52.5      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 77       |\n",
      "|    iterations         | 12500    |\n",
      "|    time_elapsed       | 802      |\n",
      "|    total_timesteps    | 62500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13.2    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 12499    |\n",
      "|    policy_loss        | -81      |\n",
      "|    reward             | 4.697497 |\n",
      "|    std                | 1.05     |\n",
      "|    value_loss         | 38.3     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 77         |\n",
      "|    iterations         | 12600      |\n",
      "|    time_elapsed       | 808        |\n",
      "|    total_timesteps    | 63000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 12599      |\n",
      "|    policy_loss        | -8.32      |\n",
      "|    reward             | -0.9725667 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 0.798      |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 77           |\n",
      "|    iterations         | 12700        |\n",
      "|    time_elapsed       | 814          |\n",
      "|    total_timesteps    | 63500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -13.2        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 12699        |\n",
      "|    policy_loss        | -88.9        |\n",
      "|    reward             | -0.007813273 |\n",
      "|    std                | 1.05         |\n",
      "|    value_loss         | 43.8         |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 77         |\n",
      "|    iterations         | 12800      |\n",
      "|    time_elapsed       | 821        |\n",
      "|    total_timesteps    | 64000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.2      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 12799      |\n",
      "|    policy_loss        | 15.3       |\n",
      "|    reward             | -0.4003756 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 1.69       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 77        |\n",
      "|    iterations         | 12900     |\n",
      "|    time_elapsed       | 827       |\n",
      "|    total_timesteps    | 64500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 12899     |\n",
      "|    policy_loss        | -9.83     |\n",
      "|    reward             | 2.4787228 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 0.602     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 77         |\n",
      "|    iterations         | 13000      |\n",
      "|    time_elapsed       | 834        |\n",
      "|    total_timesteps    | 65000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 12999      |\n",
      "|    policy_loss        | 20.9       |\n",
      "|    reward             | -0.7949488 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 4.85       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 77        |\n",
      "|    iterations         | 13100     |\n",
      "|    time_elapsed       | 840       |\n",
      "|    total_timesteps    | 65500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 13099     |\n",
      "|    policy_loss        | -77.2     |\n",
      "|    reward             | 3.3391986 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 36.8      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 77         |\n",
      "|    iterations         | 13200      |\n",
      "|    time_elapsed       | 846        |\n",
      "|    total_timesteps    | 66000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 13199      |\n",
      "|    policy_loss        | -21.7      |\n",
      "|    reward             | -1.6676389 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 3.55       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 77        |\n",
      "|    iterations         | 13300     |\n",
      "|    time_elapsed       | 852       |\n",
      "|    total_timesteps    | 66500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.3     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 13299     |\n",
      "|    policy_loss        | -77.4     |\n",
      "|    reward             | 1.1353098 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 45.1      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 77          |\n",
      "|    iterations         | 13400       |\n",
      "|    time_elapsed       | 859         |\n",
      "|    total_timesteps    | 67000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.2       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 13399       |\n",
      "|    policy_loss        | -14.4       |\n",
      "|    reward             | -0.26545182 |\n",
      "|    std                | 1.05        |\n",
      "|    value_loss         | 1.2         |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 78        |\n",
      "|    iterations         | 13500     |\n",
      "|    time_elapsed       | 865       |\n",
      "|    total_timesteps    | 67500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.2     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 13499     |\n",
      "|    policy_loss        | 16        |\n",
      "|    reward             | -1.326023 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 5.66      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 78         |\n",
      "|    iterations         | 13600      |\n",
      "|    time_elapsed       | 871        |\n",
      "|    total_timesteps    | 68000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 13599      |\n",
      "|    policy_loss        | -93.1      |\n",
      "|    reward             | -2.1698635 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 62.6       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 78       |\n",
      "|    iterations         | 13700    |\n",
      "|    time_elapsed       | 877      |\n",
      "|    total_timesteps    | 68500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13.2    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 13699    |\n",
      "|    policy_loss        | 34.5     |\n",
      "|    reward             | 2.288725 |\n",
      "|    std                | 1.05     |\n",
      "|    value_loss         | 19       |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 78       |\n",
      "|    iterations         | 13800    |\n",
      "|    time_elapsed       | 883      |\n",
      "|    total_timesteps    | 69000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13.2    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 13799    |\n",
      "|    policy_loss        | -12.7    |\n",
      "|    reward             | 1.925558 |\n",
      "|    std                | 1.05     |\n",
      "|    value_loss         | 1.21     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 78         |\n",
      "|    iterations         | 13900      |\n",
      "|    time_elapsed       | 890        |\n",
      "|    total_timesteps    | 69500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 13899      |\n",
      "|    policy_loss        | 7.71       |\n",
      "|    reward             | -2.8145225 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 1.7        |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 78          |\n",
      "|    iterations         | 14000       |\n",
      "|    time_elapsed       | 896         |\n",
      "|    total_timesteps    | 70000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.2       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 13999       |\n",
      "|    policy_loss        | -14.6       |\n",
      "|    reward             | -0.16683263 |\n",
      "|    std                | 1.05        |\n",
      "|    value_loss         | 2.81        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 78        |\n",
      "|    iterations         | 14100     |\n",
      "|    time_elapsed       | 903       |\n",
      "|    total_timesteps    | 70500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 14099     |\n",
      "|    policy_loss        | 2.2       |\n",
      "|    reward             | 0.5896414 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 0.351     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 78         |\n",
      "|    iterations         | 14200      |\n",
      "|    time_elapsed       | 909        |\n",
      "|    total_timesteps    | 71000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 14199      |\n",
      "|    policy_loss        | 3.2        |\n",
      "|    reward             | 0.07954751 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 0.0534     |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 77        |\n",
      "|    iterations         | 14300     |\n",
      "|    time_elapsed       | 917       |\n",
      "|    total_timesteps    | 71500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 14299     |\n",
      "|    policy_loss        | 2.17      |\n",
      "|    reward             | 1.1787611 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 0.39      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 77        |\n",
      "|    iterations         | 14400     |\n",
      "|    time_elapsed       | 923       |\n",
      "|    total_timesteps    | 72000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 14399     |\n",
      "|    policy_loss        | -59.3     |\n",
      "|    reward             | 0.6272489 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 22.3      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 77         |\n",
      "|    iterations         | 14500      |\n",
      "|    time_elapsed       | 929        |\n",
      "|    total_timesteps    | 72500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.2      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 14499      |\n",
      "|    policy_loss        | -53.9      |\n",
      "|    reward             | 0.38480932 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 23.3       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 77         |\n",
      "|    iterations         | 14600      |\n",
      "|    time_elapsed       | 936        |\n",
      "|    total_timesteps    | 73000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.2      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 14599      |\n",
      "|    policy_loss        | 5.73       |\n",
      "|    reward             | -5.5512323 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 1.85       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 77        |\n",
      "|    iterations         | 14700     |\n",
      "|    time_elapsed       | 942       |\n",
      "|    total_timesteps    | 73500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 14699     |\n",
      "|    policy_loss        | 45.1      |\n",
      "|    reward             | -4.386661 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 18        |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 78         |\n",
      "|    iterations         | 14800      |\n",
      "|    time_elapsed       | 948        |\n",
      "|    total_timesteps    | 74000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 14799      |\n",
      "|    policy_loss        | 1.14       |\n",
      "|    reward             | 0.10239146 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 0.016      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 78         |\n",
      "|    iterations         | 14900      |\n",
      "|    time_elapsed       | 954        |\n",
      "|    total_timesteps    | 74500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 14899      |\n",
      "|    policy_loss        | -39.2      |\n",
      "|    reward             | 0.13137333 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 9.46       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 77        |\n",
      "|    iterations         | 15000     |\n",
      "|    time_elapsed       | 962       |\n",
      "|    total_timesteps    | 75000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 14999     |\n",
      "|    policy_loss        | -28.9     |\n",
      "|    reward             | -1.920331 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 7.56      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 77         |\n",
      "|    iterations         | 15100      |\n",
      "|    time_elapsed       | 969        |\n",
      "|    total_timesteps    | 75500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 15099      |\n",
      "|    policy_loss        | -0.982     |\n",
      "|    reward             | -2.6368732 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 0.778      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 77        |\n",
      "|    iterations         | 15200     |\n",
      "|    time_elapsed       | 976       |\n",
      "|    total_timesteps    | 76000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15199     |\n",
      "|    policy_loss        | -62.9     |\n",
      "|    reward             | 4.4973354 |\n",
      "|    std                | 1.07      |\n",
      "|    value_loss         | 67.5      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 77        |\n",
      "|    iterations         | 15300     |\n",
      "|    time_elapsed       | 983       |\n",
      "|    total_timesteps    | 76500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.3     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15299     |\n",
      "|    policy_loss        | -268      |\n",
      "|    reward             | -8.507541 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 759       |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 77          |\n",
      "|    iterations         | 15400       |\n",
      "|    time_elapsed       | 990         |\n",
      "|    total_timesteps    | 77000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.3       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 15399       |\n",
      "|    policy_loss        | 27.1        |\n",
      "|    reward             | -0.19968507 |\n",
      "|    std                | 1.06        |\n",
      "|    value_loss         | 5.83        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 77        |\n",
      "|    iterations         | 15500     |\n",
      "|    time_elapsed       | 996       |\n",
      "|    total_timesteps    | 77500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15499     |\n",
      "|    policy_loss        | 25.3      |\n",
      "|    reward             | 0.6993714 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 5.61      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 77         |\n",
      "|    iterations         | 15600      |\n",
      "|    time_elapsed       | 1002       |\n",
      "|    total_timesteps    | 78000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 15599      |\n",
      "|    policy_loss        | 14.5       |\n",
      "|    reward             | 0.12804739 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 1.3        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 77        |\n",
      "|    iterations         | 15700     |\n",
      "|    time_elapsed       | 1009      |\n",
      "|    total_timesteps    | 78500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15699     |\n",
      "|    policy_loss        | 17.3      |\n",
      "|    reward             | 1.0962015 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 2.17      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 77         |\n",
      "|    iterations         | 15800      |\n",
      "|    time_elapsed       | 1015       |\n",
      "|    total_timesteps    | 79000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.3      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 15799      |\n",
      "|    policy_loss        | -16.9      |\n",
      "|    reward             | 0.62480986 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 2.22       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 77        |\n",
      "|    iterations         | 15900     |\n",
      "|    time_elapsed       | 1023      |\n",
      "|    total_timesteps    | 79500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15899     |\n",
      "|    policy_loss        | 2.08      |\n",
      "|    reward             | 2.6016183 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 11.6      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 77        |\n",
      "|    iterations         | 16000     |\n",
      "|    time_elapsed       | 1031      |\n",
      "|    total_timesteps    | 80000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.3     |\n",
      "|    explained_variance | 0.0436    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15999     |\n",
      "|    policy_loss        | 1.11      |\n",
      "|    reward             | -1.806567 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 2         |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 77         |\n",
      "|    iterations         | 16100      |\n",
      "|    time_elapsed       | 1039       |\n",
      "|    total_timesteps    | 80500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.3      |\n",
      "|    explained_variance | -2.38e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 16099      |\n",
      "|    policy_loss        | 1.78       |\n",
      "|    reward             | 0.23138884 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 0.505      |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 77       |\n",
      "|    iterations         | 16200    |\n",
      "|    time_elapsed       | 1047     |\n",
      "|    total_timesteps    | 81000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13.3    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 16199    |\n",
      "|    policy_loss        | -45.5    |\n",
      "|    reward             | 2.027798 |\n",
      "|    std                | 1.07     |\n",
      "|    value_loss         | 12.7     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 77         |\n",
      "|    iterations         | 16300      |\n",
      "|    time_elapsed       | 1055       |\n",
      "|    total_timesteps    | 81500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 16299      |\n",
      "|    policy_loss        | -14.3      |\n",
      "|    reward             | 0.41734806 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 1.23       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 77        |\n",
      "|    iterations         | 16400     |\n",
      "|    time_elapsed       | 1062      |\n",
      "|    total_timesteps    | 82000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16399     |\n",
      "|    policy_loss        | 17.6      |\n",
      "|    reward             | 1.4488189 |\n",
      "|    std                | 1.07      |\n",
      "|    value_loss         | 10.5      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 16500     |\n",
      "|    time_elapsed       | 1071      |\n",
      "|    total_timesteps    | 82500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16499     |\n",
      "|    policy_loss        | 19.3      |\n",
      "|    reward             | -3.040797 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 7.25      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 16600      |\n",
      "|    time_elapsed       | 1083       |\n",
      "|    total_timesteps    | 83000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.3      |\n",
      "|    explained_variance | 0.00582    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 16599      |\n",
      "|    policy_loss        | 3.99       |\n",
      "|    reward             | 0.62834215 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 0.192      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 16700      |\n",
      "|    time_elapsed       | 1095       |\n",
      "|    total_timesteps    | 83500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 16699      |\n",
      "|    policy_loss        | -35.5      |\n",
      "|    reward             | 0.28810462 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 9.19       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 75        |\n",
      "|    iterations         | 16800     |\n",
      "|    time_elapsed       | 1105      |\n",
      "|    total_timesteps    | 84000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.3     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16799     |\n",
      "|    policy_loss        | 25.7      |\n",
      "|    reward             | -1.387814 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 8.52      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 16900      |\n",
      "|    time_elapsed       | 1116       |\n",
      "|    total_timesteps    | 84500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 16899      |\n",
      "|    policy_loss        | -8.84      |\n",
      "|    reward             | 0.92008376 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 2.5        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 75        |\n",
      "|    iterations         | 17000     |\n",
      "|    time_elapsed       | 1127      |\n",
      "|    total_timesteps    | 85000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.2     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16999     |\n",
      "|    policy_loss        | 2.02      |\n",
      "|    reward             | -2.79538  |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 1.16      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 74        |\n",
      "|    iterations         | 17100     |\n",
      "|    time_elapsed       | 1140      |\n",
      "|    total_timesteps    | 85500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.2     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17099     |\n",
      "|    policy_loss        | -169      |\n",
      "|    reward             | 4.2838135 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 150       |\n",
      "-------------------------------------\n",
      "day: 2956, episode: 30\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4911117.32\n",
      "total_reward: 3911117.32\n",
      "total_cost: 3540.42\n",
      "total_trades: 11218\n",
      "Sharpe: 0.905\n",
      "=================================\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 74          |\n",
      "|    iterations         | 17200       |\n",
      "|    time_elapsed       | 1151        |\n",
      "|    total_timesteps    | 86000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.2       |\n",
      "|    explained_variance | 1.19e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 17199       |\n",
      "|    policy_loss        | 1.42        |\n",
      "|    reward             | -0.10339244 |\n",
      "|    std                | 1.05        |\n",
      "|    value_loss         | 0.0818      |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 74         |\n",
      "|    iterations         | 17300      |\n",
      "|    time_elapsed       | 1166       |\n",
      "|    total_timesteps    | 86500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 17299      |\n",
      "|    policy_loss        | -8.36      |\n",
      "|    reward             | 0.34680584 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 0.846      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 73         |\n",
      "|    iterations         | 17400      |\n",
      "|    time_elapsed       | 1178       |\n",
      "|    total_timesteps    | 87000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 17399      |\n",
      "|    policy_loss        | -6.08      |\n",
      "|    reward             | -2.0871463 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 0.569      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 73        |\n",
      "|    iterations         | 17500     |\n",
      "|    time_elapsed       | 1188      |\n",
      "|    total_timesteps    | 87500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17499     |\n",
      "|    policy_loss        | -24       |\n",
      "|    reward             | 2.6092181 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 5.53      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 73         |\n",
      "|    iterations         | 17600      |\n",
      "|    time_elapsed       | 1197       |\n",
      "|    total_timesteps    | 88000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 17599      |\n",
      "|    policy_loss        | 161        |\n",
      "|    reward             | -1.4215401 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 126        |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 73       |\n",
      "|    iterations         | 17700    |\n",
      "|    time_elapsed       | 1207     |\n",
      "|    total_timesteps    | 88500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13.3    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 17699    |\n",
      "|    policy_loss        | -51.2    |\n",
      "|    reward             | 2.511696 |\n",
      "|    std                | 1.07     |\n",
      "|    value_loss         | 38.3     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 73         |\n",
      "|    iterations         | 17800      |\n",
      "|    time_elapsed       | 1217       |\n",
      "|    total_timesteps    | 89000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 17799      |\n",
      "|    policy_loss        | 2.68       |\n",
      "|    reward             | 0.92966336 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 0.0798     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 72         |\n",
      "|    iterations         | 17900      |\n",
      "|    time_elapsed       | 1226       |\n",
      "|    total_timesteps    | 89500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.3      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 17899      |\n",
      "|    policy_loss        | -0.679     |\n",
      "|    reward             | -2.2182002 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 0.0388     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 72         |\n",
      "|    iterations         | 18000      |\n",
      "|    time_elapsed       | 1235       |\n",
      "|    total_timesteps    | 90000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.3      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 17999      |\n",
      "|    policy_loss        | 10.9       |\n",
      "|    reward             | 0.33829597 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 2.68       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 72        |\n",
      "|    iterations         | 18100     |\n",
      "|    time_elapsed       | 1245      |\n",
      "|    total_timesteps    | 90500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.4     |\n",
      "|    explained_variance | 1.79e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18099     |\n",
      "|    policy_loss        | 22.2      |\n",
      "|    reward             | 1.9812574 |\n",
      "|    std                | 1.07      |\n",
      "|    value_loss         | 4.05      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 72         |\n",
      "|    iterations         | 18200      |\n",
      "|    time_elapsed       | 1251       |\n",
      "|    total_timesteps    | 91000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 18199      |\n",
      "|    policy_loss        | 78.7       |\n",
      "|    reward             | 0.46648008 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 35         |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 72         |\n",
      "|    iterations         | 18300      |\n",
      "|    time_elapsed       | 1257       |\n",
      "|    total_timesteps    | 91500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 18299      |\n",
      "|    policy_loss        | -165       |\n",
      "|    reward             | -1.8817426 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 217        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 72         |\n",
      "|    iterations         | 18400      |\n",
      "|    time_elapsed       | 1264       |\n",
      "|    total_timesteps    | 92000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 18399      |\n",
      "|    policy_loss        | 16.5       |\n",
      "|    reward             | 0.07111228 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 1.54       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 72          |\n",
      "|    iterations         | 18500       |\n",
      "|    time_elapsed       | 1270        |\n",
      "|    total_timesteps    | 92500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.3       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 18499       |\n",
      "|    policy_loss        | -9.54       |\n",
      "|    reward             | -0.18430398 |\n",
      "|    std                | 1.07        |\n",
      "|    value_loss         | 3.41        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 72        |\n",
      "|    iterations         | 18600     |\n",
      "|    time_elapsed       | 1277      |\n",
      "|    total_timesteps    | 93000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18599     |\n",
      "|    policy_loss        | 11.1      |\n",
      "|    reward             | 0.5859366 |\n",
      "|    std                | 1.07      |\n",
      "|    value_loss         | 1.88      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 72        |\n",
      "|    iterations         | 18700     |\n",
      "|    time_elapsed       | 1283      |\n",
      "|    total_timesteps    | 93500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.4     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18699     |\n",
      "|    policy_loss        | -3.97     |\n",
      "|    reward             | 1.5323038 |\n",
      "|    std                | 1.07      |\n",
      "|    value_loss         | 0.374     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 72         |\n",
      "|    iterations         | 18800      |\n",
      "|    time_elapsed       | 1289       |\n",
      "|    total_timesteps    | 94000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 18799      |\n",
      "|    policy_loss        | 1.23       |\n",
      "|    reward             | -2.5082426 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 3.8        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 72         |\n",
      "|    iterations         | 18900      |\n",
      "|    time_elapsed       | 1295       |\n",
      "|    total_timesteps    | 94500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 18899      |\n",
      "|    policy_loss        | 47.7       |\n",
      "|    reward             | -1.1603738 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 44.8       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 72         |\n",
      "|    iterations         | 19000      |\n",
      "|    time_elapsed       | 1301       |\n",
      "|    total_timesteps    | 95000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.4      |\n",
      "|    explained_variance | 0.0046     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 18999      |\n",
      "|    policy_loss        | 0.783      |\n",
      "|    reward             | 0.81883776 |\n",
      "|    std                | 1.08       |\n",
      "|    value_loss         | 0.66       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 73       |\n",
      "|    iterations         | 19100    |\n",
      "|    time_elapsed       | 1307     |\n",
      "|    total_timesteps    | 95500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13.4    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 19099    |\n",
      "|    policy_loss        | -68.5    |\n",
      "|    reward             | 0.867065 |\n",
      "|    std                | 1.08     |\n",
      "|    value_loss         | 35.4     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 73        |\n",
      "|    iterations         | 19200     |\n",
      "|    time_elapsed       | 1313      |\n",
      "|    total_timesteps    | 96000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19199     |\n",
      "|    policy_loss        | 0.754     |\n",
      "|    reward             | 0.3439679 |\n",
      "|    std                | 1.08      |\n",
      "|    value_loss         | 2.56      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 73         |\n",
      "|    iterations         | 19300      |\n",
      "|    time_elapsed       | 1320       |\n",
      "|    total_timesteps    | 96500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 19299      |\n",
      "|    policy_loss        | 6.65       |\n",
      "|    reward             | 0.57243663 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 1.18       |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 73           |\n",
      "|    iterations         | 19400        |\n",
      "|    time_elapsed       | 1326         |\n",
      "|    total_timesteps    | 97000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -13.4        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 19399        |\n",
      "|    policy_loss        | 79           |\n",
      "|    reward             | -0.042321384 |\n",
      "|    std                | 1.07         |\n",
      "|    value_loss         | 54.1         |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 73         |\n",
      "|    iterations         | 19500      |\n",
      "|    time_elapsed       | 1332       |\n",
      "|    total_timesteps    | 97500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 19499      |\n",
      "|    policy_loss        | -90.6      |\n",
      "|    reward             | -1.0374229 |\n",
      "|    std                | 1.08       |\n",
      "|    value_loss         | 46.1       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 73        |\n",
      "|    iterations         | 19600     |\n",
      "|    time_elapsed       | 1339      |\n",
      "|    total_timesteps    | 98000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19599     |\n",
      "|    policy_loss        | 35        |\n",
      "|    reward             | 0.7853261 |\n",
      "|    std                | 1.08      |\n",
      "|    value_loss         | 11.4      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 73        |\n",
      "|    iterations         | 19700     |\n",
      "|    time_elapsed       | 1345      |\n",
      "|    total_timesteps    | 98500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19699     |\n",
      "|    policy_loss        | -27.2     |\n",
      "|    reward             | 0.5804144 |\n",
      "|    std                | 1.08      |\n",
      "|    value_loss         | 6.49      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 73         |\n",
      "|    iterations         | 19800      |\n",
      "|    time_elapsed       | 1352       |\n",
      "|    total_timesteps    | 99000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.4      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 19799      |\n",
      "|    policy_loss        | 21.6       |\n",
      "|    reward             | -10.172388 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 4.21       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 73         |\n",
      "|    iterations         | 19900      |\n",
      "|    time_elapsed       | 1358       |\n",
      "|    total_timesteps    | 99500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.4      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 19899      |\n",
      "|    policy_loss        | -54.8      |\n",
      "|    reward             | -3.5815558 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 18.6       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 73        |\n",
      "|    iterations         | 20000     |\n",
      "|    time_elapsed       | 1364      |\n",
      "|    total_timesteps    | 100000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.4     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19999     |\n",
      "|    policy_loss        | -29.4     |\n",
      "|    reward             | -9.518393 |\n",
      "|    std                | 1.07      |\n",
      "|    value_loss         | 15.9      |\n",
      "-------------------------------------\n",
      "hit end!\n",
      "==============Get Baseline Stats===========\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Shape of DataFrame:  (400, 8)\n",
      "Annual return         -0.022145\n",
      "Cumulative returns    -0.034921\n",
      "Annual volatility      0.176822\n",
      "Sharpe ratio          -0.038668\n",
      "Calmar ratio          -0.100930\n",
      "Stability              0.215672\n",
      "Max drawdown          -0.219408\n",
      "Omega ratio            0.993578\n",
      "Sortino ratio         -0.053987\n",
      "Skew                        NaN\n",
      "Kurtosis                    NaN\n",
      "Tail ratio             1.003436\n",
      "Daily value at risk   -0.022305\n",
      "dtype: float64\n",
      "result:                       a2c           dji\n",
      "date                                  \n",
      "2021-10-01  1.000000e+06  1.000000e+06\n",
      "2021-10-04  9.993732e+05  9.905746e+05\n",
      "2021-10-05  1.000089e+06  9.996566e+05\n",
      "2021-10-06  1.000392e+06  1.002637e+06\n",
      "2021-10-07  1.002052e+06  1.012483e+06\n",
      "...                  ...           ...\n",
      "2023-04-27  9.914827e+05  9.854252e+05\n",
      "2023-04-28  9.987939e+05  9.933491e+05\n",
      "2023-05-01  1.000944e+06  9.919956e+05\n",
      "2023-05-02  9.975519e+05  9.812993e+05\n",
      "2023-05-03  9.948668e+05  9.734251e+05\n",
      "\n",
      "[399 rows x 2 columns]\n",
      "A2C로 얻은 투자 수익률>> -0.51 %\n",
      "{'n_steps': 5, 'ent_coef': 0.01, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to results/a2c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\se99a\\anaconda3\\lib\\site-packages\\pyfolio\\timeseries.py:724: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  stats = pd.Series()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 73         |\n",
      "|    iterations         | 100        |\n",
      "|    time_elapsed       | 6          |\n",
      "|    total_timesteps    | 500        |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -12.9      |\n",
      "|    explained_variance | 0.414      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 99         |\n",
      "|    policy_loss        | -24.3      |\n",
      "|    reward             | 0.17758684 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 3.86       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 74        |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 13        |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -12.9     |\n",
      "|    explained_variance | -0.00475  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | -16.3     |\n",
      "|    reward             | 0.7567687 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 3.65      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 300       |\n",
      "|    time_elapsed       | 19        |\n",
      "|    total_timesteps    | 1500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -12.9     |\n",
      "|    explained_variance | -0.083    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 299       |\n",
      "|    policy_loss        | -21.9     |\n",
      "|    reward             | -2.536288 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 5.68      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 77         |\n",
      "|    iterations         | 400        |\n",
      "|    time_elapsed       | 25         |\n",
      "|    total_timesteps    | 2000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13        |\n",
      "|    explained_variance | -0.255     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 399        |\n",
      "|    policy_loss        | -14.2      |\n",
      "|    reward             | 0.61687464 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 2.48       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 78         |\n",
      "|    iterations         | 500        |\n",
      "|    time_elapsed       | 31         |\n",
      "|    total_timesteps    | 2500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13        |\n",
      "|    explained_variance | -0.136     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 499        |\n",
      "|    policy_loss        | -23.2      |\n",
      "|    reward             | -1.5989462 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 7.34       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 78         |\n",
      "|    iterations         | 600        |\n",
      "|    time_elapsed       | 38         |\n",
      "|    total_timesteps    | 3000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13        |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 599        |\n",
      "|    policy_loss        | -0.0333    |\n",
      "|    reward             | 0.00538014 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 0.0117     |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 78          |\n",
      "|    iterations         | 700         |\n",
      "|    time_elapsed       | 44          |\n",
      "|    total_timesteps    | 3500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13         |\n",
      "|    explained_variance | -0.0152     |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 699         |\n",
      "|    policy_loss        | 5.2         |\n",
      "|    reward             | -0.57789874 |\n",
      "|    std                | 1.03        |\n",
      "|    value_loss         | 0.52        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 79          |\n",
      "|    iterations         | 800         |\n",
      "|    time_elapsed       | 50          |\n",
      "|    total_timesteps    | 4000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13         |\n",
      "|    explained_variance | -0.801      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 799         |\n",
      "|    policy_loss        | -0.447      |\n",
      "|    reward             | 0.063977376 |\n",
      "|    std                | 1.03        |\n",
      "|    value_loss         | 0.892       |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 79        |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 56        |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13       |\n",
      "|    explained_variance | 0.0618    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | 14.8      |\n",
      "|    reward             | 2.0515623 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 2.71      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 79       |\n",
      "|    iterations         | 1000     |\n",
      "|    time_elapsed       | 62       |\n",
      "|    total_timesteps    | 5000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13      |\n",
      "|    explained_variance | 0.00898  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 999      |\n",
      "|    policy_loss        | -103     |\n",
      "|    reward             | 3.161146 |\n",
      "|    std                | 1.03     |\n",
      "|    value_loss         | 120      |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 79         |\n",
      "|    iterations         | 1100       |\n",
      "|    time_elapsed       | 68         |\n",
      "|    total_timesteps    | 5500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13        |\n",
      "|    explained_variance | 0.00511    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1099       |\n",
      "|    policy_loss        | 102        |\n",
      "|    reward             | -1.3669207 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 53.2       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 79          |\n",
      "|    iterations         | 1200        |\n",
      "|    time_elapsed       | 75          |\n",
      "|    total_timesteps    | 6000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13         |\n",
      "|    explained_variance | 1.19e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1199        |\n",
      "|    policy_loss        | -4.14       |\n",
      "|    reward             | -0.25128502 |\n",
      "|    std                | 1.03        |\n",
      "|    value_loss         | 0.18        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 80        |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 81        |\n",
      "|    total_timesteps    | 6500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.1     |\n",
      "|    explained_variance | -0.0626   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1299      |\n",
      "|    policy_loss        | -7.33     |\n",
      "|    reward             | 1.0682741 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 0.715     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 80         |\n",
      "|    iterations         | 1400       |\n",
      "|    time_elapsed       | 87         |\n",
      "|    total_timesteps    | 7000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.1      |\n",
      "|    explained_variance | 0.327      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1399       |\n",
      "|    policy_loss        | 15.7       |\n",
      "|    reward             | 0.39810193 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 3.22       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 80        |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 93        |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.1     |\n",
      "|    explained_variance | -0.185    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | 5.47      |\n",
      "|    reward             | 1.4139687 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 2.26      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 80        |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 99        |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.1     |\n",
      "|    explained_variance | -0.0306   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | 44.1      |\n",
      "|    reward             | 2.3920956 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 14.9      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 79        |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 106       |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.1     |\n",
      "|    explained_variance | 0.11      |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | 84.3      |\n",
      "|    reward             | 1.0388855 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 34.8      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 78          |\n",
      "|    iterations         | 1800        |\n",
      "|    time_elapsed       | 114         |\n",
      "|    total_timesteps    | 9000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.1       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1799        |\n",
      "|    policy_loss        | -3.7        |\n",
      "|    reward             | 0.040790964 |\n",
      "|    std                | 1.04        |\n",
      "|    value_loss         | 0.138       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 77          |\n",
      "|    iterations         | 1900        |\n",
      "|    time_elapsed       | 123         |\n",
      "|    total_timesteps    | 9500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.2       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1899        |\n",
      "|    policy_loss        | -25.3       |\n",
      "|    reward             | -0.23081544 |\n",
      "|    std                | 1.05        |\n",
      "|    value_loss         | 3.97        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 2000       |\n",
      "|    time_elapsed       | 131        |\n",
      "|    total_timesteps    | 10000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.2      |\n",
      "|    explained_variance | 0.377      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1999       |\n",
      "|    policy_loss        | 10.9       |\n",
      "|    reward             | -0.3229601 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 1.15       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 74        |\n",
      "|    iterations         | 2100      |\n",
      "|    time_elapsed       | 140       |\n",
      "|    total_timesteps    | 10500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.2     |\n",
      "|    explained_variance | 0.298     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2099      |\n",
      "|    policy_loss        | -19.8     |\n",
      "|    reward             | 1.9664688 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 2.96      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 73         |\n",
      "|    iterations         | 2200       |\n",
      "|    time_elapsed       | 148        |\n",
      "|    total_timesteps    | 11000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.1      |\n",
      "|    explained_variance | 0.00242    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2199       |\n",
      "|    policy_loss        | 5.86       |\n",
      "|    reward             | -2.9107077 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 1.66       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 73        |\n",
      "|    iterations         | 2300      |\n",
      "|    time_elapsed       | 156       |\n",
      "|    total_timesteps    | 11500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.1     |\n",
      "|    explained_variance | 0.0509    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2299      |\n",
      "|    policy_loss        | 153       |\n",
      "|    reward             | 1.6887208 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 177       |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 71          |\n",
      "|    iterations         | 2400        |\n",
      "|    time_elapsed       | 167         |\n",
      "|    total_timesteps    | 12000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.1       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 2399        |\n",
      "|    policy_loss        | 30.1        |\n",
      "|    reward             | -0.03363665 |\n",
      "|    std                | 1.04        |\n",
      "|    value_loss         | 4.31        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 70        |\n",
      "|    iterations         | 2500      |\n",
      "|    time_elapsed       | 176       |\n",
      "|    total_timesteps    | 12500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.1     |\n",
      "|    explained_variance | -1.05     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2499      |\n",
      "|    policy_loss        | -8.07     |\n",
      "|    reward             | 0.8537926 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 0.79      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 69         |\n",
      "|    iterations         | 2600       |\n",
      "|    time_elapsed       | 186        |\n",
      "|    total_timesteps    | 13000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.1      |\n",
      "|    explained_variance | 0.0271     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2599       |\n",
      "|    policy_loss        | 1.1        |\n",
      "|    reward             | 0.44326138 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 0.913      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 69         |\n",
      "|    iterations         | 2700       |\n",
      "|    time_elapsed       | 195        |\n",
      "|    total_timesteps    | 13500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.1      |\n",
      "|    explained_variance | -0.244     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2699       |\n",
      "|    policy_loss        | -6.61      |\n",
      "|    reward             | -2.6681917 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 0.869      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 68        |\n",
      "|    iterations         | 2800      |\n",
      "|    time_elapsed       | 204       |\n",
      "|    total_timesteps    | 14000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.1     |\n",
      "|    explained_variance | 0.00619   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2799      |\n",
      "|    policy_loss        | -54.2     |\n",
      "|    reward             | 1.5915475 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 18.5      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 67          |\n",
      "|    iterations         | 2900        |\n",
      "|    time_elapsed       | 214         |\n",
      "|    total_timesteps    | 14500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.1       |\n",
      "|    explained_variance | 0.151       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 2899        |\n",
      "|    policy_loss        | 37.1        |\n",
      "|    reward             | 0.074987285 |\n",
      "|    std                | 1.03        |\n",
      "|    value_loss         | 11.4        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 66          |\n",
      "|    iterations         | 3000        |\n",
      "|    time_elapsed       | 224         |\n",
      "|    total_timesteps    | 15000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.1       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 2999        |\n",
      "|    policy_loss        | 9.1         |\n",
      "|    reward             | -0.88883466 |\n",
      "|    std                | 1.03        |\n",
      "|    value_loss         | 0.445       |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 66        |\n",
      "|    iterations         | 3100      |\n",
      "|    time_elapsed       | 233       |\n",
      "|    total_timesteps    | 15500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13       |\n",
      "|    explained_variance | -0.138    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3099      |\n",
      "|    policy_loss        | -27.4     |\n",
      "|    reward             | 0.5712511 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 10.1      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 65       |\n",
      "|    iterations         | 3200     |\n",
      "|    time_elapsed       | 243      |\n",
      "|    total_timesteps    | 16000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13      |\n",
      "|    explained_variance | 0.0102   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3199     |\n",
      "|    policy_loss        | 41.4     |\n",
      "|    reward             | 1.692644 |\n",
      "|    std                | 1.03     |\n",
      "|    value_loss         | 19.9     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 65         |\n",
      "|    iterations         | 3300       |\n",
      "|    time_elapsed       | 253        |\n",
      "|    total_timesteps    | 16500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13        |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3299       |\n",
      "|    policy_loss        | 14.3       |\n",
      "|    reward             | -1.4865857 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 1.49       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 64         |\n",
      "|    iterations         | 3400       |\n",
      "|    time_elapsed       | 262        |\n",
      "|    total_timesteps    | 17000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13        |\n",
      "|    explained_variance | -0.00623   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3399       |\n",
      "|    policy_loss        | -116       |\n",
      "|    reward             | -1.6129278 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 127        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 64         |\n",
      "|    iterations         | 3500       |\n",
      "|    time_elapsed       | 270        |\n",
      "|    total_timesteps    | 17500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13        |\n",
      "|    explained_variance | 0.108      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3499       |\n",
      "|    policy_loss        | 102        |\n",
      "|    reward             | -0.6433222 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 82.8       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 64        |\n",
      "|    iterations         | 3600      |\n",
      "|    time_elapsed       | 280       |\n",
      "|    total_timesteps    | 18000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.1     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3599      |\n",
      "|    policy_loss        | 0.852     |\n",
      "|    reward             | 0.4114659 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 0.0445    |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 63          |\n",
      "|    iterations         | 3700        |\n",
      "|    time_elapsed       | 290         |\n",
      "|    total_timesteps    | 18500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.1       |\n",
      "|    explained_variance | 0.11        |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 3699        |\n",
      "|    policy_loss        | 7.85        |\n",
      "|    reward             | -0.24306129 |\n",
      "|    std                | 1.04        |\n",
      "|    value_loss         | 0.887       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 63         |\n",
      "|    iterations         | 3800       |\n",
      "|    time_elapsed       | 299        |\n",
      "|    total_timesteps    | 19000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13        |\n",
      "|    explained_variance | 0.0979     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3799       |\n",
      "|    policy_loss        | -9.88      |\n",
      "|    reward             | 0.28812954 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 4.03       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 63         |\n",
      "|    iterations         | 3900       |\n",
      "|    time_elapsed       | 307        |\n",
      "|    total_timesteps    | 19500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13        |\n",
      "|    explained_variance | 0.155      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3899       |\n",
      "|    policy_loss        | -4.5       |\n",
      "|    reward             | 0.41184697 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 0.333      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 63         |\n",
      "|    iterations         | 4000       |\n",
      "|    time_elapsed       | 316        |\n",
      "|    total_timesteps    | 20000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13        |\n",
      "|    explained_variance | 0.0283     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3999       |\n",
      "|    policy_loss        | -79.1      |\n",
      "|    reward             | -3.3791835 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 38.3       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 63         |\n",
      "|    iterations         | 4100       |\n",
      "|    time_elapsed       | 323        |\n",
      "|    total_timesteps    | 20500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13        |\n",
      "|    explained_variance | 0.0627     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4099       |\n",
      "|    policy_loss        | 14.8       |\n",
      "|    reward             | -1.2887212 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 3.58       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 63         |\n",
      "|    iterations         | 4200       |\n",
      "|    time_elapsed       | 331        |\n",
      "|    total_timesteps    | 21000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13        |\n",
      "|    explained_variance | -0.283     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4199       |\n",
      "|    policy_loss        | 12.1       |\n",
      "|    reward             | -1.0820515 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 2.19       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 63          |\n",
      "|    iterations         | 4300        |\n",
      "|    time_elapsed       | 339         |\n",
      "|    total_timesteps    | 21500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13         |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 4299        |\n",
      "|    policy_loss        | 26.4        |\n",
      "|    reward             | -0.10020097 |\n",
      "|    std                | 1.03        |\n",
      "|    value_loss         | 4.87        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 63        |\n",
      "|    iterations         | 4400      |\n",
      "|    time_elapsed       | 346       |\n",
      "|    total_timesteps    | 22000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13       |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4399      |\n",
      "|    policy_loss        | -18.5     |\n",
      "|    reward             | -4.071577 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 2.32      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 63        |\n",
      "|    iterations         | 4500      |\n",
      "|    time_elapsed       | 354       |\n",
      "|    total_timesteps    | 22500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13       |\n",
      "|    explained_variance | 0.319     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4499      |\n",
      "|    policy_loss        | 9.65      |\n",
      "|    reward             | 2.6852942 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 1.27      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 63         |\n",
      "|    iterations         | 4600       |\n",
      "|    time_elapsed       | 361        |\n",
      "|    total_timesteps    | 23000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13        |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4599       |\n",
      "|    policy_loss        | 50.4       |\n",
      "|    reward             | -1.1319566 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 26.4       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 63         |\n",
      "|    iterations         | 4700       |\n",
      "|    time_elapsed       | 367        |\n",
      "|    total_timesteps    | 23500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13        |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4699       |\n",
      "|    policy_loss        | 24         |\n",
      "|    reward             | -4.9981456 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 3.08       |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 63           |\n",
      "|    iterations         | 4800         |\n",
      "|    time_elapsed       | 375          |\n",
      "|    total_timesteps    | 24000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -13          |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 4799         |\n",
      "|    policy_loss        | 0.533        |\n",
      "|    reward             | -0.028437562 |\n",
      "|    std                | 1.03         |\n",
      "|    value_loss         | 0.675        |\n",
      "----------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 64        |\n",
      "|    iterations         | 4900      |\n",
      "|    time_elapsed       | 382       |\n",
      "|    total_timesteps    | 24500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13       |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4899      |\n",
      "|    policy_loss        | 0.317     |\n",
      "|    reward             | 1.0463375 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 1.25      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 64        |\n",
      "|    iterations         | 5000      |\n",
      "|    time_elapsed       | 389       |\n",
      "|    total_timesteps    | 25000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13       |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4999      |\n",
      "|    policy_loss        | -96.1     |\n",
      "|    reward             | 1.2084414 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 54.8      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 64       |\n",
      "|    iterations         | 5100     |\n",
      "|    time_elapsed       | 397      |\n",
      "|    total_timesteps    | 25500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13      |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5099     |\n",
      "|    policy_loss        | 35.8     |\n",
      "|    reward             | 0.763381 |\n",
      "|    std                | 1.03     |\n",
      "|    value_loss         | 8.12     |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 64          |\n",
      "|    iterations         | 5200        |\n",
      "|    time_elapsed       | 404         |\n",
      "|    total_timesteps    | 26000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.1       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 5199        |\n",
      "|    policy_loss        | -98.4       |\n",
      "|    reward             | -0.59983456 |\n",
      "|    std                | 1.03        |\n",
      "|    value_loss         | 116         |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 64         |\n",
      "|    iterations         | 5300       |\n",
      "|    time_elapsed       | 409        |\n",
      "|    total_timesteps    | 26500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5299       |\n",
      "|    policy_loss        | 116        |\n",
      "|    reward             | -2.1455147 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 96.7       |\n",
      "--------------------------------------\n",
      "day: 2956, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 5002818.61\n",
      "total_reward: 4002818.61\n",
      "total_cost: 3267.02\n",
      "total_trades: 8839\n",
      "Sharpe: 0.918\n",
      "=================================\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 64         |\n",
      "|    iterations         | 5400       |\n",
      "|    time_elapsed       | 416        |\n",
      "|    total_timesteps    | 27000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5399       |\n",
      "|    policy_loss        | -31        |\n",
      "|    reward             | -1.0349313 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 7.12       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 65         |\n",
      "|    iterations         | 5500       |\n",
      "|    time_elapsed       | 422        |\n",
      "|    total_timesteps    | 27500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.2      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5499       |\n",
      "|    policy_loss        | 22.5       |\n",
      "|    reward             | 0.18567613 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 4.59       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 65        |\n",
      "|    iterations         | 5600      |\n",
      "|    time_elapsed       | 428       |\n",
      "|    total_timesteps    | 28000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.2     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5599      |\n",
      "|    policy_loss        | -6.73     |\n",
      "|    reward             | 1.455123  |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 11.1      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 65        |\n",
      "|    iterations         | 5700      |\n",
      "|    time_elapsed       | 434       |\n",
      "|    total_timesteps    | 28500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5699      |\n",
      "|    policy_loss        | 32.3      |\n",
      "|    reward             | 0.5254998 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 14.4      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 65        |\n",
      "|    iterations         | 5800      |\n",
      "|    time_elapsed       | 440       |\n",
      "|    total_timesteps    | 29000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5799      |\n",
      "|    policy_loss        | 84.3      |\n",
      "|    reward             | 1.2005177 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 44.7      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 66         |\n",
      "|    iterations         | 5900       |\n",
      "|    time_elapsed       | 446        |\n",
      "|    total_timesteps    | 29500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.2      |\n",
      "|    explained_variance | -2.38e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5899       |\n",
      "|    policy_loss        | -18.3      |\n",
      "|    reward             | -2.0944884 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 10.1       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 66        |\n",
      "|    iterations         | 6000      |\n",
      "|    time_elapsed       | 452       |\n",
      "|    total_timesteps    | 30000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5999      |\n",
      "|    policy_loss        | -45.3     |\n",
      "|    reward             | 0.5691621 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 16.1      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 66         |\n",
      "|    iterations         | 6100       |\n",
      "|    time_elapsed       | 458        |\n",
      "|    total_timesteps    | 30500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6099       |\n",
      "|    policy_loss        | 17.2       |\n",
      "|    reward             | 0.48085386 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 2.71       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 66        |\n",
      "|    iterations         | 6200      |\n",
      "|    time_elapsed       | 464       |\n",
      "|    total_timesteps    | 31000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6199      |\n",
      "|    policy_loss        | -103      |\n",
      "|    reward             | -3.952803 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 87.3      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 66          |\n",
      "|    iterations         | 6300        |\n",
      "|    time_elapsed       | 470         |\n",
      "|    total_timesteps    | 31500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.2       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 6299        |\n",
      "|    policy_loss        | 7.8         |\n",
      "|    reward             | -0.22444345 |\n",
      "|    std                | 1.05        |\n",
      "|    value_loss         | 2.66        |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 67       |\n",
      "|    iterations         | 6400     |\n",
      "|    time_elapsed       | 477      |\n",
      "|    total_timesteps    | 32000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13.2    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6399     |\n",
      "|    policy_loss        | 2.16     |\n",
      "|    reward             | 2.625025 |\n",
      "|    std                | 1.05     |\n",
      "|    value_loss         | 6.48     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 67         |\n",
      "|    iterations         | 6500       |\n",
      "|    time_elapsed       | 483        |\n",
      "|    total_timesteps    | 32500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.2      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6499       |\n",
      "|    policy_loss        | 201        |\n",
      "|    reward             | -1.9271445 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 189        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 67        |\n",
      "|    iterations         | 6600      |\n",
      "|    time_elapsed       | 489       |\n",
      "|    total_timesteps    | 33000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6599      |\n",
      "|    policy_loss        | -32.5     |\n",
      "|    reward             | -2.246995 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 8.08      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 67        |\n",
      "|    iterations         | 6700      |\n",
      "|    time_elapsed       | 495       |\n",
      "|    total_timesteps    | 33500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6699      |\n",
      "|    policy_loss        | -13.3     |\n",
      "|    reward             | 1.0749415 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 1.28      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 67         |\n",
      "|    iterations         | 6800       |\n",
      "|    time_elapsed       | 501        |\n",
      "|    total_timesteps    | 34000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6799       |\n",
      "|    policy_loss        | 86.8       |\n",
      "|    reward             | -2.0188954 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 43.7       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 67         |\n",
      "|    iterations         | 6900       |\n",
      "|    time_elapsed       | 507        |\n",
      "|    total_timesteps    | 34500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6899       |\n",
      "|    policy_loss        | -71.3      |\n",
      "|    reward             | -1.4014112 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 32.8       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 68       |\n",
      "|    iterations         | 7000     |\n",
      "|    time_elapsed       | 513      |\n",
      "|    total_timesteps    | 35000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13.3    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6999     |\n",
      "|    policy_loss        | 26.8     |\n",
      "|    reward             | 2.072215 |\n",
      "|    std                | 1.06     |\n",
      "|    value_loss         | 4.72     |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 68          |\n",
      "|    iterations         | 7100        |\n",
      "|    time_elapsed       | 519         |\n",
      "|    total_timesteps    | 35500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.3       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 7099        |\n",
      "|    policy_loss        | -0.968      |\n",
      "|    reward             | 0.010143907 |\n",
      "|    std                | 1.06        |\n",
      "|    value_loss         | 0.00637     |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 68           |\n",
      "|    iterations         | 7200         |\n",
      "|    time_elapsed       | 526          |\n",
      "|    total_timesteps    | 36000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -13.3        |\n",
      "|    explained_variance | -0.00546     |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 7199         |\n",
      "|    policy_loss        | 2.54         |\n",
      "|    reward             | 0.0008303264 |\n",
      "|    std                | 1.06         |\n",
      "|    value_loss         | 0.144        |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 68         |\n",
      "|    iterations         | 7300       |\n",
      "|    time_elapsed       | 532        |\n",
      "|    total_timesteps    | 36500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.3      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7299       |\n",
      "|    policy_loss        | 46.3       |\n",
      "|    reward             | 0.52276766 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 13.3       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 68       |\n",
      "|    iterations         | 7400     |\n",
      "|    time_elapsed       | 538      |\n",
      "|    total_timesteps    | 37000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13.3    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7399     |\n",
      "|    policy_loss        | -58.7    |\n",
      "|    reward             | 3.304342 |\n",
      "|    std                | 1.06     |\n",
      "|    value_loss         | 30.4     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 68        |\n",
      "|    iterations         | 7500      |\n",
      "|    time_elapsed       | 544       |\n",
      "|    total_timesteps    | 37500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.3     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7499      |\n",
      "|    policy_loss        | -3.22     |\n",
      "|    reward             | 2.8538692 |\n",
      "|    std                | 1.07      |\n",
      "|    value_loss         | 0.839     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 68        |\n",
      "|    iterations         | 7600      |\n",
      "|    time_elapsed       | 551       |\n",
      "|    total_timesteps    | 38000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7599      |\n",
      "|    policy_loss        | 35.3      |\n",
      "|    reward             | 1.1120851 |\n",
      "|    std                | 1.07      |\n",
      "|    value_loss         | 12.9      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 69          |\n",
      "|    iterations         | 7700        |\n",
      "|    time_elapsed       | 556         |\n",
      "|    total_timesteps    | 38500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.4       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 7699        |\n",
      "|    policy_loss        | 3.94        |\n",
      "|    reward             | 0.039449707 |\n",
      "|    std                | 1.07        |\n",
      "|    value_loss         | 0.103       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 69          |\n",
      "|    iterations         | 7800        |\n",
      "|    time_elapsed       | 563         |\n",
      "|    total_timesteps    | 39000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.4       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 7799        |\n",
      "|    policy_loss        | 10.1        |\n",
      "|    reward             | -0.45799989 |\n",
      "|    std                | 1.07        |\n",
      "|    value_loss         | 1.11        |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 69       |\n",
      "|    iterations         | 7900     |\n",
      "|    time_elapsed       | 569      |\n",
      "|    total_timesteps    | 39500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13.4    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7899     |\n",
      "|    policy_loss        | -53.1    |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.07     |\n",
      "|    value_loss         | 12.7     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 69         |\n",
      "|    iterations         | 8000       |\n",
      "|    time_elapsed       | 575        |\n",
      "|    total_timesteps    | 40000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.4      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7999       |\n",
      "|    policy_loss        | -34.2      |\n",
      "|    reward             | -3.9960144 |\n",
      "|    std                | 1.08       |\n",
      "|    value_loss         | 7.34       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 69       |\n",
      "|    iterations         | 8100     |\n",
      "|    time_elapsed       | 581      |\n",
      "|    total_timesteps    | 40500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13.4    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8099     |\n",
      "|    policy_loss        | -68.7    |\n",
      "|    reward             | 4.783312 |\n",
      "|    std                | 1.08     |\n",
      "|    value_loss         | 52.9     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 69        |\n",
      "|    iterations         | 8200      |\n",
      "|    time_elapsed       | 587       |\n",
      "|    total_timesteps    | 41000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8199      |\n",
      "|    policy_loss        | -370      |\n",
      "|    reward             | -9.375009 |\n",
      "|    std                | 1.08      |\n",
      "|    value_loss         | 815       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 69         |\n",
      "|    iterations         | 8300       |\n",
      "|    time_elapsed       | 593        |\n",
      "|    total_timesteps    | 41500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.5      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8299       |\n",
      "|    policy_loss        | -28.3      |\n",
      "|    reward             | -1.5198805 |\n",
      "|    std                | 1.08       |\n",
      "|    value_loss         | 4.81       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 69         |\n",
      "|    iterations         | 8400       |\n",
      "|    time_elapsed       | 600        |\n",
      "|    total_timesteps    | 42000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.5      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8399       |\n",
      "|    policy_loss        | -18        |\n",
      "|    reward             | -0.5825496 |\n",
      "|    std                | 1.08       |\n",
      "|    value_loss         | 1.76       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 69        |\n",
      "|    iterations         | 8500      |\n",
      "|    time_elapsed       | 609       |\n",
      "|    total_timesteps    | 42500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8499      |\n",
      "|    policy_loss        | 18.6      |\n",
      "|    reward             | 1.8499321 |\n",
      "|    std                | 1.09      |\n",
      "|    value_loss         | 3.55      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 69        |\n",
      "|    iterations         | 8600      |\n",
      "|    time_elapsed       | 616       |\n",
      "|    total_timesteps    | 43000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.4     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8599      |\n",
      "|    policy_loss        | 11.2      |\n",
      "|    reward             | 3.6519346 |\n",
      "|    std                | 1.08      |\n",
      "|    value_loss         | 3.52      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 69        |\n",
      "|    iterations         | 8700      |\n",
      "|    time_elapsed       | 624       |\n",
      "|    total_timesteps    | 43500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8699      |\n",
      "|    policy_loss        | -134      |\n",
      "|    reward             | 4.3280516 |\n",
      "|    std                | 1.08      |\n",
      "|    value_loss         | 92.4      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 69         |\n",
      "|    iterations         | 8800       |\n",
      "|    time_elapsed       | 630        |\n",
      "|    total_timesteps    | 44000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.5      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8799       |\n",
      "|    policy_loss        | 3.9        |\n",
      "|    reward             | -4.1281357 |\n",
      "|    std                | 1.08       |\n",
      "|    value_loss         | 21.1       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 69       |\n",
      "|    iterations         | 8900     |\n",
      "|    time_elapsed       | 637      |\n",
      "|    total_timesteps    | 44500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13.5    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8899     |\n",
      "|    policy_loss        | 38.4     |\n",
      "|    reward             | 2.287091 |\n",
      "|    std                | 1.09     |\n",
      "|    value_loss         | 9.23     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 69        |\n",
      "|    iterations         | 9000      |\n",
      "|    time_elapsed       | 643       |\n",
      "|    total_timesteps    | 45000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8999      |\n",
      "|    policy_loss        | 22.8      |\n",
      "|    reward             | 0.5991355 |\n",
      "|    std                | 1.09      |\n",
      "|    value_loss         | 4.64      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 70        |\n",
      "|    iterations         | 9100      |\n",
      "|    time_elapsed       | 649       |\n",
      "|    total_timesteps    | 45500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.5     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9099      |\n",
      "|    policy_loss        | -33.4     |\n",
      "|    reward             | 1.8280901 |\n",
      "|    std                | 1.09      |\n",
      "|    value_loss         | 5.64      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 70         |\n",
      "|    iterations         | 9200       |\n",
      "|    time_elapsed       | 656        |\n",
      "|    total_timesteps    | 46000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.5      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9199       |\n",
      "|    policy_loss        | 30.4       |\n",
      "|    reward             | 0.11292166 |\n",
      "|    std                | 1.09       |\n",
      "|    value_loss         | 8.61       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 70        |\n",
      "|    iterations         | 9300      |\n",
      "|    time_elapsed       | 662       |\n",
      "|    total_timesteps    | 46500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9299      |\n",
      "|    policy_loss        | 63.7      |\n",
      "|    reward             | 3.7255838 |\n",
      "|    std                | 1.09      |\n",
      "|    value_loss         | 18.7      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 70          |\n",
      "|    iterations         | 9400        |\n",
      "|    time_elapsed       | 668         |\n",
      "|    total_timesteps    | 47000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.5       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 9399        |\n",
      "|    policy_loss        | 71.3        |\n",
      "|    reward             | 0.040416926 |\n",
      "|    std                | 1.09        |\n",
      "|    value_loss         | 47.7        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 70         |\n",
      "|    iterations         | 9500       |\n",
      "|    time_elapsed       | 675        |\n",
      "|    total_timesteps    | 47500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.5      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9499       |\n",
      "|    policy_loss        | 10.8       |\n",
      "|    reward             | 0.63562876 |\n",
      "|    std                | 1.09       |\n",
      "|    value_loss         | 1.29       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 70          |\n",
      "|    iterations         | 9600        |\n",
      "|    time_elapsed       | 681         |\n",
      "|    total_timesteps    | 48000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.5       |\n",
      "|    explained_variance | 0.0918      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 9599        |\n",
      "|    policy_loss        | 5.55        |\n",
      "|    reward             | -0.55378175 |\n",
      "|    std                | 1.09        |\n",
      "|    value_loss         | 0.674       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 70         |\n",
      "|    iterations         | 9700       |\n",
      "|    time_elapsed       | 687        |\n",
      "|    total_timesteps    | 48500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.5      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9699       |\n",
      "|    policy_loss        | 12.5       |\n",
      "|    reward             | -1.1443403 |\n",
      "|    std                | 1.09       |\n",
      "|    value_loss         | 2.7        |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 70          |\n",
      "|    iterations         | 9800        |\n",
      "|    time_elapsed       | 694         |\n",
      "|    total_timesteps    | 49000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.6       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 9799        |\n",
      "|    policy_loss        | -53.9       |\n",
      "|    reward             | -0.32905242 |\n",
      "|    std                | 1.1         |\n",
      "|    value_loss         | 16.7        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 70        |\n",
      "|    iterations         | 9900      |\n",
      "|    time_elapsed       | 700       |\n",
      "|    total_timesteps    | 49500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9899      |\n",
      "|    policy_loss        | -21.2     |\n",
      "|    reward             | 1.4799435 |\n",
      "|    std                | 1.1       |\n",
      "|    value_loss         | 3.79      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 70         |\n",
      "|    iterations         | 10000      |\n",
      "|    time_elapsed       | 706        |\n",
      "|    total_timesteps    | 50000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.6      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9999       |\n",
      "|    policy_loss        | 13.3       |\n",
      "|    reward             | -5.7776656 |\n",
      "|    std                | 1.1        |\n",
      "|    value_loss         | 1.64       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 70        |\n",
      "|    iterations         | 10100     |\n",
      "|    time_elapsed       | 713       |\n",
      "|    total_timesteps    | 50500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 10099     |\n",
      "|    policy_loss        | -0.406    |\n",
      "|    reward             | 1.3676045 |\n",
      "|    std                | 1.1       |\n",
      "|    value_loss         | 1.11      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 70        |\n",
      "|    iterations         | 10200     |\n",
      "|    time_elapsed       | 720       |\n",
      "|    total_timesteps    | 51000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.6     |\n",
      "|    explained_variance | 8.94e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 10199     |\n",
      "|    policy_loss        | 18.6      |\n",
      "|    reward             | 1.1434947 |\n",
      "|    std                | 1.1       |\n",
      "|    value_loss         | 2.92      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 70        |\n",
      "|    iterations         | 10300     |\n",
      "|    time_elapsed       | 726       |\n",
      "|    total_timesteps    | 51500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 10299     |\n",
      "|    policy_loss        | -26.9     |\n",
      "|    reward             | 1.4840313 |\n",
      "|    std                | 1.1       |\n",
      "|    value_loss         | 4.28      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 70         |\n",
      "|    iterations         | 10400      |\n",
      "|    time_elapsed       | 733        |\n",
      "|    total_timesteps    | 52000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.6      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 10399      |\n",
      "|    policy_loss        | 124        |\n",
      "|    reward             | -1.0617458 |\n",
      "|    std                | 1.11       |\n",
      "|    value_loss         | 98.7       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 71         |\n",
      "|    iterations         | 10500      |\n",
      "|    time_elapsed       | 739        |\n",
      "|    total_timesteps    | 52500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.7      |\n",
      "|    explained_variance | 0.0764     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 10499      |\n",
      "|    policy_loss        | 57.3       |\n",
      "|    reward             | -4.2484703 |\n",
      "|    std                | 1.11       |\n",
      "|    value_loss         | 20.1       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 71          |\n",
      "|    iterations         | 10600       |\n",
      "|    time_elapsed       | 745         |\n",
      "|    total_timesteps    | 53000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.6       |\n",
      "|    explained_variance | 0.0086      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 10599       |\n",
      "|    policy_loss        | -119        |\n",
      "|    reward             | -0.28472367 |\n",
      "|    std                | 1.11        |\n",
      "|    value_loss         | 137         |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 71          |\n",
      "|    iterations         | 10700       |\n",
      "|    time_elapsed       | 751         |\n",
      "|    total_timesteps    | 53500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.6       |\n",
      "|    explained_variance | -0.028      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 10699       |\n",
      "|    policy_loss        | 13          |\n",
      "|    reward             | 0.078465655 |\n",
      "|    std                | 1.11        |\n",
      "|    value_loss         | 1.42        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 71         |\n",
      "|    iterations         | 10800      |\n",
      "|    time_elapsed       | 758        |\n",
      "|    total_timesteps    | 54000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.6      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 10799      |\n",
      "|    policy_loss        | 14.7       |\n",
      "|    reward             | -0.4534799 |\n",
      "|    std                | 1.1        |\n",
      "|    value_loss         | 1.6        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 71         |\n",
      "|    iterations         | 10900      |\n",
      "|    time_elapsed       | 764        |\n",
      "|    total_timesteps    | 54500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.6      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 10899      |\n",
      "|    policy_loss        | 12.3       |\n",
      "|    reward             | -2.1144612 |\n",
      "|    std                | 1.1        |\n",
      "|    value_loss         | 3.33       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 71        |\n",
      "|    iterations         | 11000     |\n",
      "|    time_elapsed       | 770       |\n",
      "|    total_timesteps    | 55000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 10999     |\n",
      "|    policy_loss        | -13.9     |\n",
      "|    reward             | -0.501431 |\n",
      "|    std                | 1.11      |\n",
      "|    value_loss         | 1.71      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 71          |\n",
      "|    iterations         | 11100       |\n",
      "|    time_elapsed       | 777         |\n",
      "|    total_timesteps    | 55500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.6       |\n",
      "|    explained_variance | 0.00133     |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 11099       |\n",
      "|    policy_loss        | 94          |\n",
      "|    reward             | -0.42420495 |\n",
      "|    std                | 1.11        |\n",
      "|    value_loss         | 60.3        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 71        |\n",
      "|    iterations         | 11200     |\n",
      "|    time_elapsed       | 783       |\n",
      "|    total_timesteps    | 56000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.6     |\n",
      "|    explained_variance | -0.00198  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 11199     |\n",
      "|    policy_loss        | 20.7      |\n",
      "|    reward             | 1.9368159 |\n",
      "|    std                | 1.11      |\n",
      "|    value_loss         | 6.29      |\n",
      "-------------------------------------\n",
      "day: 2956, episode: 20\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4920125.63\n",
      "total_reward: 3920125.63\n",
      "total_cost: 9762.75\n",
      "total_trades: 9188\n",
      "Sharpe: 0.890\n",
      "=================================\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 71         |\n",
      "|    iterations         | 11300      |\n",
      "|    time_elapsed       | 790        |\n",
      "|    total_timesteps    | 56500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.7      |\n",
      "|    explained_variance | 0.0468     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 11299      |\n",
      "|    policy_loss        | 14.1       |\n",
      "|    reward             | 0.18480602 |\n",
      "|    std                | 1.11       |\n",
      "|    value_loss         | 1.28       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 71        |\n",
      "|    iterations         | 11400     |\n",
      "|    time_elapsed       | 796       |\n",
      "|    total_timesteps    | 57000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.7     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 11399     |\n",
      "|    policy_loss        | 16.6      |\n",
      "|    reward             | 0.7073575 |\n",
      "|    std                | 1.12      |\n",
      "|    value_loss         | 1.58      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 71       |\n",
      "|    iterations         | 11500    |\n",
      "|    time_elapsed       | 803      |\n",
      "|    total_timesteps    | 57500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13.7    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 11499    |\n",
      "|    policy_loss        | 28.4     |\n",
      "|    reward             | 2.633436 |\n",
      "|    std                | 1.11     |\n",
      "|    value_loss         | 7.43     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 71         |\n",
      "|    iterations         | 11600      |\n",
      "|    time_elapsed       | 810        |\n",
      "|    total_timesteps    | 58000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 11599      |\n",
      "|    policy_loss        | -14.2      |\n",
      "|    reward             | -0.8160117 |\n",
      "|    std                | 1.11       |\n",
      "|    value_loss         | 2.12       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 71         |\n",
      "|    iterations         | 11700      |\n",
      "|    time_elapsed       | 816        |\n",
      "|    total_timesteps    | 58500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 11699      |\n",
      "|    policy_loss        | 82.5       |\n",
      "|    reward             | -2.3431094 |\n",
      "|    std                | 1.11       |\n",
      "|    value_loss         | 42.7       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 71        |\n",
      "|    iterations         | 11800     |\n",
      "|    time_elapsed       | 823       |\n",
      "|    total_timesteps    | 59000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 11799     |\n",
      "|    policy_loss        | -14.2     |\n",
      "|    reward             | 2.9985852 |\n",
      "|    std                | 1.11      |\n",
      "|    value_loss         | 22.1      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 71          |\n",
      "|    iterations         | 11900       |\n",
      "|    time_elapsed       | 829         |\n",
      "|    total_timesteps    | 59500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.8       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 11899       |\n",
      "|    policy_loss        | -18.1       |\n",
      "|    reward             | 0.117631085 |\n",
      "|    std                | 1.12        |\n",
      "|    value_loss         | 2.27        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 71         |\n",
      "|    iterations         | 12000      |\n",
      "|    time_elapsed       | 836        |\n",
      "|    total_timesteps    | 60000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.7      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 11999      |\n",
      "|    policy_loss        | -31.2      |\n",
      "|    reward             | -2.4913592 |\n",
      "|    std                | 1.12       |\n",
      "|    value_loss         | 5.45       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 71        |\n",
      "|    iterations         | 12100     |\n",
      "|    time_elapsed       | 842       |\n",
      "|    total_timesteps    | 60500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.8     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 12099     |\n",
      "|    policy_loss        | -3.67     |\n",
      "|    reward             | 1.1001893 |\n",
      "|    std                | 1.12      |\n",
      "|    value_loss         | 1.05      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 71         |\n",
      "|    iterations         | 12200      |\n",
      "|    time_elapsed       | 849        |\n",
      "|    total_timesteps    | 61000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 12199      |\n",
      "|    policy_loss        | -63.7      |\n",
      "|    reward             | 0.71847856 |\n",
      "|    std                | 1.12       |\n",
      "|    value_loss         | 17.9       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 71        |\n",
      "|    iterations         | 12300     |\n",
      "|    time_elapsed       | 855       |\n",
      "|    total_timesteps    | 61500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 12299     |\n",
      "|    policy_loss        | -59.5     |\n",
      "|    reward             | 3.2374403 |\n",
      "|    std                | 1.12      |\n",
      "|    value_loss         | 25.1      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 71        |\n",
      "|    iterations         | 12400     |\n",
      "|    time_elapsed       | 862       |\n",
      "|    total_timesteps    | 62000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 12399     |\n",
      "|    policy_loss        | 76.7      |\n",
      "|    reward             | 2.5243442 |\n",
      "|    std                | 1.12      |\n",
      "|    value_loss         | 32.2      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 71        |\n",
      "|    iterations         | 12500     |\n",
      "|    time_elapsed       | 868       |\n",
      "|    total_timesteps    | 62500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 12499     |\n",
      "|    policy_loss        | -71.6     |\n",
      "|    reward             | 3.8759322 |\n",
      "|    std                | 1.12      |\n",
      "|    value_loss         | 30.2      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 71         |\n",
      "|    iterations         | 12600      |\n",
      "|    time_elapsed       | 875        |\n",
      "|    total_timesteps    | 63000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.8      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 12599      |\n",
      "|    policy_loss        | -0.296     |\n",
      "|    reward             | -0.8066585 |\n",
      "|    std                | 1.12       |\n",
      "|    value_loss         | 0.257      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 72        |\n",
      "|    iterations         | 12700     |\n",
      "|    time_elapsed       | 881       |\n",
      "|    total_timesteps    | 63500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 12699     |\n",
      "|    policy_loss        | -108      |\n",
      "|    reward             | 1.4651461 |\n",
      "|    std                | 1.12      |\n",
      "|    value_loss         | 61.2      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 72         |\n",
      "|    iterations         | 12800      |\n",
      "|    time_elapsed       | 887        |\n",
      "|    total_timesteps    | 64000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 12799      |\n",
      "|    policy_loss        | 30.3       |\n",
      "|    reward             | -0.8657922 |\n",
      "|    std                | 1.12       |\n",
      "|    value_loss         | 4.55       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 72        |\n",
      "|    iterations         | 12900     |\n",
      "|    time_elapsed       | 894       |\n",
      "|    total_timesteps    | 64500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.8     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 12899     |\n",
      "|    policy_loss        | -52.2     |\n",
      "|    reward             | 1.9696758 |\n",
      "|    std                | 1.12      |\n",
      "|    value_loss         | 16.4      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 72          |\n",
      "|    iterations         | 13000       |\n",
      "|    time_elapsed       | 901         |\n",
      "|    total_timesteps    | 65000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.7       |\n",
      "|    explained_variance | 0.00604     |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 12999       |\n",
      "|    policy_loss        | 23.3        |\n",
      "|    reward             | 0.011779289 |\n",
      "|    std                | 1.12        |\n",
      "|    value_loss         | 4.56        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 72        |\n",
      "|    iterations         | 13100     |\n",
      "|    time_elapsed       | 908       |\n",
      "|    total_timesteps    | 65500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.8     |\n",
      "|    explained_variance | 0.0292    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 13099     |\n",
      "|    policy_loss        | -77.8     |\n",
      "|    reward             | 3.8614101 |\n",
      "|    std                | 1.12      |\n",
      "|    value_loss         | 40.6      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 72       |\n",
      "|    iterations         | 13200    |\n",
      "|    time_elapsed       | 914      |\n",
      "|    total_timesteps    | 66000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13.8    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 13199    |\n",
      "|    policy_loss        | -21.6    |\n",
      "|    reward             | -1.68211 |\n",
      "|    std                | 1.13     |\n",
      "|    value_loss         | 3.44     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 72        |\n",
      "|    iterations         | 13300     |\n",
      "|    time_elapsed       | 921       |\n",
      "|    total_timesteps    | 66500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.8     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 13299     |\n",
      "|    policy_loss        | -79.7     |\n",
      "|    reward             | 1.4373105 |\n",
      "|    std                | 1.13      |\n",
      "|    value_loss         | 46.1      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 72          |\n",
      "|    iterations         | 13400       |\n",
      "|    time_elapsed       | 927         |\n",
      "|    total_timesteps    | 67000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.9       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 13399       |\n",
      "|    policy_loss        | -22.1       |\n",
      "|    reward             | -0.29767674 |\n",
      "|    std                | 1.13        |\n",
      "|    value_loss         | 2.34        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 72         |\n",
      "|    iterations         | 13500      |\n",
      "|    time_elapsed       | 934        |\n",
      "|    total_timesteps    | 67500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.9      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 13499      |\n",
      "|    policy_loss        | 16.9       |\n",
      "|    reward             | -1.3161293 |\n",
      "|    std                | 1.13       |\n",
      "|    value_loss         | 5.31       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 72         |\n",
      "|    iterations         | 13600      |\n",
      "|    time_elapsed       | 940        |\n",
      "|    total_timesteps    | 68000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.9      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 13599      |\n",
      "|    policy_loss        | -96        |\n",
      "|    reward             | -2.3110483 |\n",
      "|    std                | 1.13       |\n",
      "|    value_loss         | 63.9       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 72        |\n",
      "|    iterations         | 13700     |\n",
      "|    time_elapsed       | 946       |\n",
      "|    total_timesteps    | 68500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.9     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 13699     |\n",
      "|    policy_loss        | 30.5      |\n",
      "|    reward             | 2.055693  |\n",
      "|    std                | 1.13      |\n",
      "|    value_loss         | 13.8      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 72       |\n",
      "|    iterations         | 13800    |\n",
      "|    time_elapsed       | 953      |\n",
      "|    total_timesteps    | 69000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13.9    |\n",
      "|    explained_variance | 1.79e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 13799    |\n",
      "|    policy_loss        | -5.18    |\n",
      "|    reward             | 2.402906 |\n",
      "|    std                | 1.14     |\n",
      "|    value_loss         | 0.512    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 72        |\n",
      "|    iterations         | 13900     |\n",
      "|    time_elapsed       | 959       |\n",
      "|    total_timesteps    | 69500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.9     |\n",
      "|    explained_variance | -9.61e-05 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 13899     |\n",
      "|    policy_loss        | -0.311    |\n",
      "|    reward             | -2.017074 |\n",
      "|    std                | 1.14      |\n",
      "|    value_loss         | 1.48      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 72         |\n",
      "|    iterations         | 14000      |\n",
      "|    time_elapsed       | 966        |\n",
      "|    total_timesteps    | 70000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.9      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 13999      |\n",
      "|    policy_loss        | -11        |\n",
      "|    reward             | 0.33181632 |\n",
      "|    std                | 1.14       |\n",
      "|    value_loss         | 1.75       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 72         |\n",
      "|    iterations         | 14100      |\n",
      "|    time_elapsed       | 975        |\n",
      "|    total_timesteps    | 70500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.9      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 14099      |\n",
      "|    policy_loss        | 0.515      |\n",
      "|    reward             | 0.31361154 |\n",
      "|    std                | 1.14       |\n",
      "|    value_loss         | 0.199      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 72          |\n",
      "|    iterations         | 14200       |\n",
      "|    time_elapsed       | 982         |\n",
      "|    total_timesteps    | 71000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.9       |\n",
      "|    explained_variance | 5.96e-08    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 14199       |\n",
      "|    policy_loss        | 3.3         |\n",
      "|    reward             | 0.076698095 |\n",
      "|    std                | 1.14        |\n",
      "|    value_loss         | 0.0576      |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 72         |\n",
      "|    iterations         | 14300      |\n",
      "|    time_elapsed       | 989        |\n",
      "|    total_timesteps    | 71500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.9      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 14299      |\n",
      "|    policy_loss        | 0.525      |\n",
      "|    reward             | 0.61927766 |\n",
      "|    std                | 1.14       |\n",
      "|    value_loss         | 0.416      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 72        |\n",
      "|    iterations         | 14400     |\n",
      "|    time_elapsed       | 995       |\n",
      "|    total_timesteps    | 72000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.9     |\n",
      "|    explained_variance | -0.0382   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 14399     |\n",
      "|    policy_loss        | -53       |\n",
      "|    reward             | 0.9795712 |\n",
      "|    std                | 1.14      |\n",
      "|    value_loss         | 14.6      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 72        |\n",
      "|    iterations         | 14500     |\n",
      "|    time_elapsed       | 1001      |\n",
      "|    total_timesteps    | 72500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.9     |\n",
      "|    explained_variance | -0.138    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 14499     |\n",
      "|    policy_loss        | -22.2     |\n",
      "|    reward             | 3.7575388 |\n",
      "|    std                | 1.14      |\n",
      "|    value_loss         | 4.39      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 72         |\n",
      "|    iterations         | 14600      |\n",
      "|    time_elapsed       | 1007       |\n",
      "|    total_timesteps    | 73000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.9      |\n",
      "|    explained_variance | 0.0631     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 14599      |\n",
      "|    policy_loss        | 3.24       |\n",
      "|    reward             | -2.0988338 |\n",
      "|    std                | 1.14       |\n",
      "|    value_loss         | 0.466      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 72        |\n",
      "|    iterations         | 14700     |\n",
      "|    time_elapsed       | 1013      |\n",
      "|    total_timesteps    | 73500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -14       |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 14699     |\n",
      "|    policy_loss        | 48.5      |\n",
      "|    reward             | -5.373799 |\n",
      "|    std                | 1.15      |\n",
      "|    value_loss         | 13.1      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 72         |\n",
      "|    iterations         | 14800      |\n",
      "|    time_elapsed       | 1019       |\n",
      "|    total_timesteps    | 74000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.9      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 14799      |\n",
      "|    policy_loss        | 5.31       |\n",
      "|    reward             | 0.32598028 |\n",
      "|    std                | 1.14       |\n",
      "|    value_loss         | 0.178      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 72         |\n",
      "|    iterations         | 14900      |\n",
      "|    time_elapsed       | 1026       |\n",
      "|    total_timesteps    | 74500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.9      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 14899      |\n",
      "|    policy_loss        | -45.3      |\n",
      "|    reward             | 0.28832588 |\n",
      "|    std                | 1.14       |\n",
      "|    value_loss         | 14         |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 72         |\n",
      "|    iterations         | 15000      |\n",
      "|    time_elapsed       | 1032       |\n",
      "|    total_timesteps    | 75000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.9      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 14999      |\n",
      "|    policy_loss        | -19.7      |\n",
      "|    reward             | -1.9663886 |\n",
      "|    std                | 1.14       |\n",
      "|    value_loss         | 3.4        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 72         |\n",
      "|    iterations         | 15100      |\n",
      "|    time_elapsed       | 1038       |\n",
      "|    total_timesteps    | 75500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.9      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 15099      |\n",
      "|    policy_loss        | -3.12      |\n",
      "|    reward             | -2.2772462 |\n",
      "|    std                | 1.14       |\n",
      "|    value_loss         | 0.446      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 72        |\n",
      "|    iterations         | 15200     |\n",
      "|    time_elapsed       | 1044      |\n",
      "|    total_timesteps    | 76000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.9     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15199     |\n",
      "|    policy_loss        | -46.4     |\n",
      "|    reward             | 3.6321201 |\n",
      "|    std                | 1.14      |\n",
      "|    value_loss         | 34.2      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 72       |\n",
      "|    iterations         | 15300    |\n",
      "|    time_elapsed       | 1050     |\n",
      "|    total_timesteps    | 76500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13.9    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 15299    |\n",
      "|    policy_loss        | -252     |\n",
      "|    reward             | -9.18803 |\n",
      "|    std                | 1.14     |\n",
      "|    value_loss         | 470      |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 72         |\n",
      "|    iterations         | 15400      |\n",
      "|    time_elapsed       | 1056       |\n",
      "|    total_timesteps    | 77000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.9      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 15399      |\n",
      "|    policy_loss        | 24.8       |\n",
      "|    reward             | -0.2920775 |\n",
      "|    std                | 1.14       |\n",
      "|    value_loss         | 6.61       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 72        |\n",
      "|    iterations         | 15500     |\n",
      "|    time_elapsed       | 1062      |\n",
      "|    total_timesteps    | 77500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.9     |\n",
      "|    explained_variance | -0.0288   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15499     |\n",
      "|    policy_loss        | 45.1      |\n",
      "|    reward             | 1.1623659 |\n",
      "|    std                | 1.14      |\n",
      "|    value_loss         | 9.59      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 72         |\n",
      "|    iterations         | 15600      |\n",
      "|    time_elapsed       | 1068       |\n",
      "|    total_timesteps    | 78000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.9      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 15599      |\n",
      "|    policy_loss        | 34.1       |\n",
      "|    reward             | 0.69596547 |\n",
      "|    std                | 1.14       |\n",
      "|    value_loss         | 7.53       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 73        |\n",
      "|    iterations         | 15700     |\n",
      "|    time_elapsed       | 1075      |\n",
      "|    total_timesteps    | 78500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.9     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15699     |\n",
      "|    policy_loss        | -2.33     |\n",
      "|    reward             | 1.3113273 |\n",
      "|    std                | 1.14      |\n",
      "|    value_loss         | 0.256     |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 73          |\n",
      "|    iterations         | 15800       |\n",
      "|    time_elapsed       | 1082        |\n",
      "|    total_timesteps    | 79000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.9       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 15799       |\n",
      "|    policy_loss        | -16.5       |\n",
      "|    reward             | -0.08880457 |\n",
      "|    std                | 1.14        |\n",
      "|    value_loss         | 3.63        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 73         |\n",
      "|    iterations         | 15900      |\n",
      "|    time_elapsed       | 1088       |\n",
      "|    total_timesteps    | 79500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.9      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 15899      |\n",
      "|    policy_loss        | 92.4       |\n",
      "|    reward             | -1.3762469 |\n",
      "|    std                | 1.14       |\n",
      "|    value_loss         | 97.4       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 73       |\n",
      "|    iterations         | 16000    |\n",
      "|    time_elapsed       | 1094     |\n",
      "|    total_timesteps    | 80000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13.9    |\n",
      "|    explained_variance | 0.192    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 15999    |\n",
      "|    policy_loss        | -1.3     |\n",
      "|    reward             | -1.85755 |\n",
      "|    std                | 1.14     |\n",
      "|    value_loss         | 2.32     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 73        |\n",
      "|    iterations         | 16100     |\n",
      "|    time_elapsed       | 1100      |\n",
      "|    total_timesteps    | 80500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.9     |\n",
      "|    explained_variance | 0.0497    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16099     |\n",
      "|    policy_loss        | 19.2      |\n",
      "|    reward             | 1.4191952 |\n",
      "|    std                | 1.14      |\n",
      "|    value_loss         | 3.58      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 73         |\n",
      "|    iterations         | 16200      |\n",
      "|    time_elapsed       | 1106       |\n",
      "|    total_timesteps    | 81000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.9      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 16199      |\n",
      "|    policy_loss        | -17.8      |\n",
      "|    reward             | 0.44857147 |\n",
      "|    std                | 1.14       |\n",
      "|    value_loss         | 2.22       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 73        |\n",
      "|    iterations         | 16300     |\n",
      "|    time_elapsed       | 1113      |\n",
      "|    total_timesteps    | 81500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.9     |\n",
      "|    explained_variance | 0.259     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16299     |\n",
      "|    policy_loss        | -42.3     |\n",
      "|    reward             | -0.345869 |\n",
      "|    std                | 1.14      |\n",
      "|    value_loss         | 10.2      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 73        |\n",
      "|    iterations         | 16400     |\n",
      "|    time_elapsed       | 1119      |\n",
      "|    total_timesteps    | 82000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.9     |\n",
      "|    explained_variance | -0.0375   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16399     |\n",
      "|    policy_loss        | 38.7      |\n",
      "|    reward             | 1.4272385 |\n",
      "|    std                | 1.14      |\n",
      "|    value_loss         | 13.8      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 73        |\n",
      "|    iterations         | 16500     |\n",
      "|    time_elapsed       | 1125      |\n",
      "|    total_timesteps    | 82500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.8     |\n",
      "|    explained_variance | -0.0798   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16499     |\n",
      "|    policy_loss        | 23.2      |\n",
      "|    reward             | -4.217266 |\n",
      "|    std                | 1.13      |\n",
      "|    value_loss         | 6.45      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 73       |\n",
      "|    iterations         | 16600    |\n",
      "|    time_elapsed       | 1131     |\n",
      "|    total_timesteps    | 83000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13.8    |\n",
      "|    explained_variance | -1.38    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 16599    |\n",
      "|    policy_loss        | 6.43     |\n",
      "|    reward             | 1.093258 |\n",
      "|    std                | 1.13     |\n",
      "|    value_loss         | 0.779    |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 73         |\n",
      "|    iterations         | 16700      |\n",
      "|    time_elapsed       | 1137       |\n",
      "|    total_timesteps    | 83500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 16699      |\n",
      "|    policy_loss        | -36.4      |\n",
      "|    reward             | 0.91633564 |\n",
      "|    std                | 1.13       |\n",
      "|    value_loss         | 6.72       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 73         |\n",
      "|    iterations         | 16800      |\n",
      "|    time_elapsed       | 1144       |\n",
      "|    total_timesteps    | 84000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.8      |\n",
      "|    explained_variance | 0.255      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 16799      |\n",
      "|    policy_loss        | -9.14      |\n",
      "|    reward             | 0.40305564 |\n",
      "|    std                | 1.13       |\n",
      "|    value_loss         | 2.04       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 73         |\n",
      "|    iterations         | 16900      |\n",
      "|    time_elapsed       | 1150       |\n",
      "|    total_timesteps    | 84500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 16899      |\n",
      "|    policy_loss        | 6.11       |\n",
      "|    reward             | 0.70938784 |\n",
      "|    std                | 1.13       |\n",
      "|    value_loss         | 0.919      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 73         |\n",
      "|    iterations         | 17000      |\n",
      "|    time_elapsed       | 1156       |\n",
      "|    total_timesteps    | 85000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 16999      |\n",
      "|    policy_loss        | -44.4      |\n",
      "|    reward             | -1.3697712 |\n",
      "|    std                | 1.13       |\n",
      "|    value_loss         | 15.4       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 73        |\n",
      "|    iterations         | 17100     |\n",
      "|    time_elapsed       | 1162      |\n",
      "|    total_timesteps    | 85500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.9     |\n",
      "|    explained_variance | 0.0248    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17099     |\n",
      "|    policy_loss        | -159      |\n",
      "|    reward             | 1.8785572 |\n",
      "|    std                | 1.13      |\n",
      "|    value_loss         | 157       |\n",
      "-------------------------------------\n",
      "day: 2956, episode: 30\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3621748.16\n",
      "total_reward: 2621748.16\n",
      "total_cost: 23468.02\n",
      "total_trades: 13269\n",
      "Sharpe: 0.675\n",
      "=================================\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 73         |\n",
      "|    iterations         | 17200      |\n",
      "|    time_elapsed       | 1168       |\n",
      "|    total_timesteps    | 86000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.8      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 17199      |\n",
      "|    policy_loss        | 6.35       |\n",
      "|    reward             | 0.05895447 |\n",
      "|    std                | 1.13       |\n",
      "|    value_loss         | 0.289      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 73        |\n",
      "|    iterations         | 17300     |\n",
      "|    time_elapsed       | 1174      |\n",
      "|    total_timesteps    | 86500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.9     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17299     |\n",
      "|    policy_loss        | -2.06     |\n",
      "|    reward             | 0.8231056 |\n",
      "|    std                | 1.13      |\n",
      "|    value_loss         | 0.448     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 73         |\n",
      "|    iterations         | 17400      |\n",
      "|    time_elapsed       | 1180       |\n",
      "|    total_timesteps    | 87000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.9      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 17399      |\n",
      "|    policy_loss        | -18.3      |\n",
      "|    reward             | -0.7993179 |\n",
      "|    std                | 1.13       |\n",
      "|    value_loss         | 2.99       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 73         |\n",
      "|    iterations         | 17500      |\n",
      "|    time_elapsed       | 1186       |\n",
      "|    total_timesteps    | 87500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.9      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 17499      |\n",
      "|    policy_loss        | 4.96       |\n",
      "|    reward             | 0.99900156 |\n",
      "|    std                | 1.14       |\n",
      "|    value_loss         | 0.506      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 73         |\n",
      "|    iterations         | 17600      |\n",
      "|    time_elapsed       | 1192       |\n",
      "|    total_timesteps    | 88000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.9      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 17599      |\n",
      "|    policy_loss        | 81.8       |\n",
      "|    reward             | -1.8804086 |\n",
      "|    std                | 1.14       |\n",
      "|    value_loss         | 39.3       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 73        |\n",
      "|    iterations         | 17700     |\n",
      "|    time_elapsed       | 1198      |\n",
      "|    total_timesteps    | 88500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.9     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17699     |\n",
      "|    policy_loss        | 15.5      |\n",
      "|    reward             | 1.0070244 |\n",
      "|    std                | 1.14      |\n",
      "|    value_loss         | 5.39      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 73       |\n",
      "|    iterations         | 17800    |\n",
      "|    time_elapsed       | 1205     |\n",
      "|    total_timesteps    | 89000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13.9    |\n",
      "|    explained_variance | -0.0258  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 17799    |\n",
      "|    policy_loss        | 8.12     |\n",
      "|    reward             | 1.034131 |\n",
      "|    std                | 1.14     |\n",
      "|    value_loss         | 0.82     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 73        |\n",
      "|    iterations         | 17900     |\n",
      "|    time_elapsed       | 1211      |\n",
      "|    total_timesteps    | 89500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.9     |\n",
      "|    explained_variance | 0.535     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17899     |\n",
      "|    policy_loss        | -0.415    |\n",
      "|    reward             | -2.917833 |\n",
      "|    std                | 1.14      |\n",
      "|    value_loss         | 0.025     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 73         |\n",
      "|    iterations         | 18000      |\n",
      "|    time_elapsed       | 1218       |\n",
      "|    total_timesteps    | 90000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.9      |\n",
      "|    explained_variance | 0.128      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 17999      |\n",
      "|    policy_loss        | 5.09       |\n",
      "|    reward             | 0.24965256 |\n",
      "|    std                | 1.14       |\n",
      "|    value_loss         | 0.862      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 73        |\n",
      "|    iterations         | 18100     |\n",
      "|    time_elapsed       | 1224      |\n",
      "|    total_timesteps    | 90500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.9     |\n",
      "|    explained_variance | 0.0865    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18099     |\n",
      "|    policy_loss        | -3.75     |\n",
      "|    reward             | 1.8206824 |\n",
      "|    std                | 1.14      |\n",
      "|    value_loss         | 0.649     |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 73          |\n",
      "|    iterations         | 18200       |\n",
      "|    time_elapsed       | 1230        |\n",
      "|    total_timesteps    | 91000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.9       |\n",
      "|    explained_variance | 0.483       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 18199       |\n",
      "|    policy_loss        | 35          |\n",
      "|    reward             | -0.08863263 |\n",
      "|    std                | 1.15        |\n",
      "|    value_loss         | 7.43        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 74         |\n",
      "|    iterations         | 18300      |\n",
      "|    time_elapsed       | 1236       |\n",
      "|    total_timesteps    | 91500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.9      |\n",
      "|    explained_variance | 0.00206    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 18299      |\n",
      "|    policy_loss        | -164       |\n",
      "|    reward             | 0.70548666 |\n",
      "|    std                | 1.14       |\n",
      "|    value_loss         | 158        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 72        |\n",
      "|    iterations         | 18400     |\n",
      "|    time_elapsed       | 1262      |\n",
      "|    total_timesteps    | 92000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.9     |\n",
      "|    explained_variance | 0.115     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18399     |\n",
      "|    policy_loss        | 29.2      |\n",
      "|    reward             | 0.8437539 |\n",
      "|    std                | 1.14      |\n",
      "|    value_loss         | 5.43      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 72         |\n",
      "|    iterations         | 18500      |\n",
      "|    time_elapsed       | 1269       |\n",
      "|    total_timesteps    | 92500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.9      |\n",
      "|    explained_variance | -0.06      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 18499      |\n",
      "|    policy_loss        | -24.2      |\n",
      "|    reward             | -0.9008152 |\n",
      "|    std                | 1.14       |\n",
      "|    value_loss         | 6.15       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 72        |\n",
      "|    iterations         | 18600     |\n",
      "|    time_elapsed       | 1275      |\n",
      "|    total_timesteps    | 93000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.9     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18599     |\n",
      "|    policy_loss        | -3.32     |\n",
      "|    reward             | 0.6451875 |\n",
      "|    std                | 1.15      |\n",
      "|    value_loss         | 0.602     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 72       |\n",
      "|    iterations         | 18700    |\n",
      "|    time_elapsed       | 1281     |\n",
      "|    total_timesteps    | 93500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13.9    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 18699    |\n",
      "|    policy_loss        | 1.12     |\n",
      "|    reward             | 1.745003 |\n",
      "|    std                | 1.14     |\n",
      "|    value_loss         | 0.115    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 72        |\n",
      "|    iterations         | 18800     |\n",
      "|    time_elapsed       | 1288      |\n",
      "|    total_timesteps    | 94000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.9     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18799     |\n",
      "|    policy_loss        | 52.2      |\n",
      "|    reward             | 1.0796438 |\n",
      "|    std                | 1.14      |\n",
      "|    value_loss         | 15.4      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 73         |\n",
      "|    iterations         | 18900      |\n",
      "|    time_elapsed       | 1294       |\n",
      "|    total_timesteps    | 94500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -14        |\n",
      "|    explained_variance | 0.0358     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 18899      |\n",
      "|    policy_loss        | 57.5       |\n",
      "|    reward             | -2.9664998 |\n",
      "|    std                | 1.15       |\n",
      "|    value_loss         | 39.5       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 73        |\n",
      "|    iterations         | 19000     |\n",
      "|    time_elapsed       | 1300      |\n",
      "|    total_timesteps    | 95000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -14       |\n",
      "|    explained_variance | -0.0764   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18999     |\n",
      "|    policy_loss        | 0.675     |\n",
      "|    reward             | 1.1764385 |\n",
      "|    std                | 1.15      |\n",
      "|    value_loss         | 1.05      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 73        |\n",
      "|    iterations         | 19100     |\n",
      "|    time_elapsed       | 1306      |\n",
      "|    total_timesteps    | 95500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -14       |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19099     |\n",
      "|    policy_loss        | -80.2     |\n",
      "|    reward             | 1.0384306 |\n",
      "|    std                | 1.15      |\n",
      "|    value_loss         | 32.7      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 73         |\n",
      "|    iterations         | 19200      |\n",
      "|    time_elapsed       | 1312       |\n",
      "|    total_timesteps    | 96000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -14        |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 19199      |\n",
      "|    policy_loss        | -5.33      |\n",
      "|    reward             | -0.5236223 |\n",
      "|    std                | 1.15       |\n",
      "|    value_loss         | 1.14       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 73         |\n",
      "|    iterations         | 19300      |\n",
      "|    time_elapsed       | 1318       |\n",
      "|    total_timesteps    | 96500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -14        |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 19299      |\n",
      "|    policy_loss        | -40.6      |\n",
      "|    reward             | 0.46474507 |\n",
      "|    std                | 1.15       |\n",
      "|    value_loss         | 7.97       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 73        |\n",
      "|    iterations         | 19400     |\n",
      "|    time_elapsed       | 1325      |\n",
      "|    total_timesteps    | 97000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -14       |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19399     |\n",
      "|    policy_loss        | 87.6      |\n",
      "|    reward             | 2.0965147 |\n",
      "|    std                | 1.16      |\n",
      "|    value_loss         | 51.6      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 73        |\n",
      "|    iterations         | 19500     |\n",
      "|    time_elapsed       | 1331      |\n",
      "|    total_timesteps    | 97500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -14       |\n",
      "|    explained_variance | -0.869    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19499     |\n",
      "|    policy_loss        | 76.1      |\n",
      "|    reward             | 2.4291542 |\n",
      "|    std                | 1.15      |\n",
      "|    value_loss         | 39.6      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 73        |\n",
      "|    iterations         | 19600     |\n",
      "|    time_elapsed       | 1337      |\n",
      "|    total_timesteps    | 98000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -14       |\n",
      "|    explained_variance | 1e-05     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19599     |\n",
      "|    policy_loss        | 37.1      |\n",
      "|    reward             | 0.7759139 |\n",
      "|    std                | 1.15      |\n",
      "|    value_loss         | 13.7      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 73         |\n",
      "|    iterations         | 19700      |\n",
      "|    time_elapsed       | 1343       |\n",
      "|    total_timesteps    | 98500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -14        |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 19699      |\n",
      "|    policy_loss        | -29.3      |\n",
      "|    reward             | 0.85162866 |\n",
      "|    std                | 1.15       |\n",
      "|    value_loss         | 6.88       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 73        |\n",
      "|    iterations         | 19800     |\n",
      "|    time_elapsed       | 1350      |\n",
      "|    total_timesteps    | 99000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -14       |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19799     |\n",
      "|    policy_loss        | 9.12      |\n",
      "|    reward             | -7.758368 |\n",
      "|    std                | 1.15      |\n",
      "|    value_loss         | 1.08      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 73         |\n",
      "|    iterations         | 19900      |\n",
      "|    time_elapsed       | 1356       |\n",
      "|    total_timesteps    | 99500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -14        |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 19899      |\n",
      "|    policy_loss        | -53.7      |\n",
      "|    reward             | -3.7881405 |\n",
      "|    std                | 1.15       |\n",
      "|    value_loss         | 14.3       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 73        |\n",
      "|    iterations         | 20000     |\n",
      "|    time_elapsed       | 1362      |\n",
      "|    total_timesteps    | 100000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -14       |\n",
      "|    explained_variance | 0.0208    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19999     |\n",
      "|    policy_loss        | -64.3     |\n",
      "|    reward             | -8.543438 |\n",
      "|    std                | 1.15      |\n",
      "|    value_loss         | 50.8      |\n",
      "-------------------------------------\n",
      "hit end!\n",
      "==============Get Baseline Stats===========\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Shape of DataFrame:  (400, 8)\n",
      "Annual return         -0.022145\n",
      "Cumulative returns    -0.034921\n",
      "Annual volatility      0.176822\n",
      "Sharpe ratio          -0.038668\n",
      "Calmar ratio          -0.100930\n",
      "Stability              0.215672\n",
      "Max drawdown          -0.219408\n",
      "Omega ratio            0.993578\n",
      "Sortino ratio         -0.053987\n",
      "Skew                        NaN\n",
      "Kurtosis                    NaN\n",
      "Tail ratio             1.003436\n",
      "Daily value at risk   -0.022305\n",
      "dtype: float64\n",
      "result:                       a2c           dji\n",
      "date                                  \n",
      "2021-10-01  1.000000e+06  1.000000e+06\n",
      "2021-10-04  9.999732e+05  9.905746e+05\n",
      "2021-10-05  1.000361e+06  9.996566e+05\n",
      "2021-10-06  1.000261e+06  1.002637e+06\n",
      "2021-10-07  1.001103e+06  1.012483e+06\n",
      "...                  ...           ...\n",
      "2023-04-27  1.111014e+06  9.854252e+05\n",
      "2023-04-28  1.123994e+06  9.933491e+05\n",
      "2023-05-01  1.123405e+06  9.919956e+05\n",
      "2023-05-02  1.100585e+06  9.812993e+05\n",
      "2023-05-03  1.090569e+06  9.734251e+05\n",
      "\n",
      "[399 rows x 2 columns]\n",
      "A2C로 얻은 투자 수익률>> 9.06 %\n",
      "{'n_steps': 5, 'ent_coef': 0.01, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to results/a2c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\se99a\\anaconda3\\lib\\site-packages\\pyfolio\\timeseries.py:724: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  stats = pd.Series()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 72         |\n",
      "|    iterations         | 100        |\n",
      "|    time_elapsed       | 6          |\n",
      "|    total_timesteps    | 500        |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -12.9      |\n",
      "|    explained_variance | -0.986     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 99         |\n",
      "|    policy_loss        | -31.1      |\n",
      "|    reward             | 0.21041697 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 11.7       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 200        |\n",
      "|    time_elapsed       | 13         |\n",
      "|    total_timesteps    | 1000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13        |\n",
      "|    explained_variance | 0.446      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 199        |\n",
      "|    policy_loss        | 10.1       |\n",
      "|    reward             | 0.29056805 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 1.15       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 300       |\n",
      "|    time_elapsed       | 19        |\n",
      "|    total_timesteps    | 1500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -12.9     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 299       |\n",
      "|    policy_loss        | -1.97     |\n",
      "|    reward             | -2.849213 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 7.17      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 77          |\n",
      "|    iterations         | 400         |\n",
      "|    time_elapsed       | 25          |\n",
      "|    total_timesteps    | 2000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -12.9       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 399         |\n",
      "|    policy_loss        | 12.5        |\n",
      "|    reward             | -0.38407278 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 3.6         |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 77          |\n",
      "|    iterations         | 500         |\n",
      "|    time_elapsed       | 32          |\n",
      "|    total_timesteps    | 2500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -12.9       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 499         |\n",
      "|    policy_loss        | -19.3       |\n",
      "|    reward             | -0.14079952 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 2.35        |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 77           |\n",
      "|    iterations         | 600          |\n",
      "|    time_elapsed       | 38           |\n",
      "|    total_timesteps    | 3000         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -12.9        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 599          |\n",
      "|    policy_loss        | 0.946        |\n",
      "|    reward             | -0.036858328 |\n",
      "|    std                | 1.01         |\n",
      "|    value_loss         | 0.0213       |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 78         |\n",
      "|    iterations         | 700        |\n",
      "|    time_elapsed       | 44         |\n",
      "|    total_timesteps    | 3500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -12.9      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 699        |\n",
      "|    policy_loss        | 15.9       |\n",
      "|    reward             | -1.1401873 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 2.33       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 78         |\n",
      "|    iterations         | 800        |\n",
      "|    time_elapsed       | 50         |\n",
      "|    total_timesteps    | 4000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -12.9      |\n",
      "|    explained_variance | -0.0407    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 799        |\n",
      "|    policy_loss        | 13.5       |\n",
      "|    reward             | -0.8525697 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 2.73       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 78        |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 56        |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13       |\n",
      "|    explained_variance | -0.381    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | 6.94      |\n",
      "|    reward             | 2.2426713 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 2.37      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 79        |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 62        |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13       |\n",
      "|    explained_variance | 0.000746  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | -74.4     |\n",
      "|    reward             | 1.4899127 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 55        |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 79         |\n",
      "|    iterations         | 1100       |\n",
      "|    time_elapsed       | 69         |\n",
      "|    total_timesteps    | 5500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13        |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1099       |\n",
      "|    policy_loss        | -20.5      |\n",
      "|    reward             | 0.65790725 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 5.68       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 79          |\n",
      "|    iterations         | 1200        |\n",
      "|    time_elapsed       | 75          |\n",
      "|    total_timesteps    | 6000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.1       |\n",
      "|    explained_variance | 1.19e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1199        |\n",
      "|    policy_loss        | -5.78       |\n",
      "|    reward             | -0.35169846 |\n",
      "|    std                | 1.03        |\n",
      "|    value_loss         | 0.213       |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 79       |\n",
      "|    iterations         | 1300     |\n",
      "|    time_elapsed       | 82       |\n",
      "|    total_timesteps    | 6500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13.1    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1299     |\n",
      "|    policy_loss        | 0.0866   |\n",
      "|    reward             | 1.272187 |\n",
      "|    std                | 1.04     |\n",
      "|    value_loss         | 0.12     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 79         |\n",
      "|    iterations         | 1400       |\n",
      "|    time_elapsed       | 88         |\n",
      "|    total_timesteps    | 7000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.1      |\n",
      "|    explained_variance | 0.334      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1399       |\n",
      "|    policy_loss        | 25.9       |\n",
      "|    reward             | 0.21472302 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 6.28       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 79        |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 94        |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.1     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | 7.11      |\n",
      "|    reward             | 1.1537077 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 1.62      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 79       |\n",
      "|    iterations         | 1600     |\n",
      "|    time_elapsed       | 100      |\n",
      "|    total_timesteps    | 8000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13.1    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1599     |\n",
      "|    policy_loss        | 25       |\n",
      "|    reward             | 1.164923 |\n",
      "|    std                | 1.04     |\n",
      "|    value_loss         | 4.74     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 79       |\n",
      "|    iterations         | 1700     |\n",
      "|    time_elapsed       | 107      |\n",
      "|    total_timesteps    | 8500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13.2    |\n",
      "|    explained_variance | -0.118   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1699     |\n",
      "|    policy_loss        | 88.1     |\n",
      "|    reward             | 5.060493 |\n",
      "|    std                | 1.05     |\n",
      "|    value_loss         | 65.6     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 79         |\n",
      "|    iterations         | 1800       |\n",
      "|    time_elapsed       | 113        |\n",
      "|    total_timesteps    | 9000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1799       |\n",
      "|    policy_loss        | -9.85      |\n",
      "|    reward             | 0.28503618 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 0.667      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 79         |\n",
      "|    iterations         | 1900       |\n",
      "|    time_elapsed       | 119        |\n",
      "|    total_timesteps    | 9500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.2      |\n",
      "|    explained_variance | -0.0162    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1899       |\n",
      "|    policy_loss        | -39.8      |\n",
      "|    reward             | 0.01013542 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 10.2       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 79         |\n",
      "|    iterations         | 2000       |\n",
      "|    time_elapsed       | 125        |\n",
      "|    total_timesteps    | 10000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.2      |\n",
      "|    explained_variance | -0.00418   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1999       |\n",
      "|    policy_loss        | 28.3       |\n",
      "|    reward             | 0.18994622 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 6.42       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 80        |\n",
      "|    iterations         | 2100      |\n",
      "|    time_elapsed       | 131       |\n",
      "|    total_timesteps    | 10500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.2     |\n",
      "|    explained_variance | 0.168     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2099      |\n",
      "|    policy_loss        | -21       |\n",
      "|    reward             | 1.9733819 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 3.79      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 78         |\n",
      "|    iterations         | 2200       |\n",
      "|    time_elapsed       | 139        |\n",
      "|    total_timesteps    | 11000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.2      |\n",
      "|    explained_variance | -0.00897   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2199       |\n",
      "|    policy_loss        | 2.2        |\n",
      "|    reward             | -0.3821803 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 1.81       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 77         |\n",
      "|    iterations         | 2300       |\n",
      "|    time_elapsed       | 147        |\n",
      "|    total_timesteps    | 11500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.1      |\n",
      "|    explained_variance | -0.0435    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2299       |\n",
      "|    policy_loss        | 132        |\n",
      "|    reward             | 0.27474517 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 129        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 2400      |\n",
      "|    time_elapsed       | 156       |\n",
      "|    total_timesteps    | 12000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.2     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2399      |\n",
      "|    policy_loss        | 28        |\n",
      "|    reward             | 0.3300215 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 3.75      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 2500      |\n",
      "|    time_elapsed       | 163       |\n",
      "|    total_timesteps    | 12500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.2     |\n",
      "|    explained_variance | -0.217    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2499      |\n",
      "|    policy_loss        | -3.44     |\n",
      "|    reward             | 1.0318654 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 0.441     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 2600      |\n",
      "|    time_elapsed       | 169       |\n",
      "|    total_timesteps    | 13000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2599      |\n",
      "|    policy_loss        | -0.317    |\n",
      "|    reward             | 0.9470169 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 0.829     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 2700       |\n",
      "|    time_elapsed       | 176        |\n",
      "|    total_timesteps    | 13500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.2      |\n",
      "|    explained_variance | 0.00543    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2699       |\n",
      "|    policy_loss        | 12.6       |\n",
      "|    reward             | -1.3670776 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 1.72       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 2800      |\n",
      "|    time_elapsed       | 183       |\n",
      "|    total_timesteps    | 14000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2799      |\n",
      "|    policy_loss        | -17.3     |\n",
      "|    reward             | 0.5150483 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 2.12      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 76          |\n",
      "|    iterations         | 2900        |\n",
      "|    time_elapsed       | 189         |\n",
      "|    total_timesteps    | 14500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.2       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 2899        |\n",
      "|    policy_loss        | 10.2        |\n",
      "|    reward             | -0.57251704 |\n",
      "|    std                | 1.05        |\n",
      "|    value_loss         | 1.02        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 3000       |\n",
      "|    time_elapsed       | 196        |\n",
      "|    total_timesteps    | 15000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.2      |\n",
      "|    explained_variance | -0.489     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2999       |\n",
      "|    policy_loss        | 9.33       |\n",
      "|    reward             | -0.6222843 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 0.633      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 76          |\n",
      "|    iterations         | 3100        |\n",
      "|    time_elapsed       | 202         |\n",
      "|    total_timesteps    | 15500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.2       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 3099        |\n",
      "|    policy_loss        | -7.03       |\n",
      "|    reward             | -0.28219852 |\n",
      "|    std                | 1.05        |\n",
      "|    value_loss         | 3.17        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 3200      |\n",
      "|    time_elapsed       | 209       |\n",
      "|    total_timesteps    | 16000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.2     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3199      |\n",
      "|    policy_loss        | 86.3      |\n",
      "|    reward             | 4.3772383 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 73.5      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 76          |\n",
      "|    iterations         | 3300        |\n",
      "|    time_elapsed       | 215         |\n",
      "|    total_timesteps    | 16500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.2       |\n",
      "|    explained_variance | 0.119       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 3299        |\n",
      "|    policy_loss        | 18.4        |\n",
      "|    reward             | -0.35907882 |\n",
      "|    std                | 1.05        |\n",
      "|    value_loss         | 2.67        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 76          |\n",
      "|    iterations         | 3400        |\n",
      "|    time_elapsed       | 221         |\n",
      "|    total_timesteps    | 17000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.2       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 3399        |\n",
      "|    policy_loss        | -90.4       |\n",
      "|    reward             | -0.32308984 |\n",
      "|    std                | 1.05        |\n",
      "|    value_loss         | 81.1        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 3500       |\n",
      "|    time_elapsed       | 228        |\n",
      "|    total_timesteps    | 17500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3499       |\n",
      "|    policy_loss        | 137        |\n",
      "|    reward             | -2.1122525 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 116        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 3600      |\n",
      "|    time_elapsed       | 234       |\n",
      "|    total_timesteps    | 18000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3599      |\n",
      "|    policy_loss        | 5.2       |\n",
      "|    reward             | 0.8674113 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 0.187     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 3700       |\n",
      "|    time_elapsed       | 241        |\n",
      "|    total_timesteps    | 18500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3699       |\n",
      "|    policy_loss        | -2.19      |\n",
      "|    reward             | -0.1925039 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 1.47       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 3800      |\n",
      "|    time_elapsed       | 247       |\n",
      "|    total_timesteps    | 19000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3799      |\n",
      "|    policy_loss        | 6.61      |\n",
      "|    reward             | 0.9344104 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 5.8       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 3900       |\n",
      "|    time_elapsed       | 254        |\n",
      "|    total_timesteps    | 19500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3899       |\n",
      "|    policy_loss        | 8.5        |\n",
      "|    reward             | 0.38623664 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 1.67       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 4000       |\n",
      "|    time_elapsed       | 261        |\n",
      "|    total_timesteps    | 20000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3999       |\n",
      "|    policy_loss        | -74.7      |\n",
      "|    reward             | -3.1982539 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 39.8       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 4100       |\n",
      "|    time_elapsed       | 268        |\n",
      "|    total_timesteps    | 20500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4099       |\n",
      "|    policy_loss        | 0.164      |\n",
      "|    reward             | -1.8001763 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 1.4        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 4200       |\n",
      "|    time_elapsed       | 275        |\n",
      "|    total_timesteps    | 21000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.2      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4199       |\n",
      "|    policy_loss        | -7.07      |\n",
      "|    reward             | -1.2171993 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 1.03       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 4300       |\n",
      "|    time_elapsed       | 281        |\n",
      "|    total_timesteps    | 21500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4299       |\n",
      "|    policy_loss        | 25.3       |\n",
      "|    reward             | 0.43921265 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 3.19       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 4400      |\n",
      "|    time_elapsed       | 288       |\n",
      "|    total_timesteps    | 22000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.2     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4399      |\n",
      "|    policy_loss        | -24.3     |\n",
      "|    reward             | -3.992684 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 3.47      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 4500      |\n",
      "|    time_elapsed       | 294       |\n",
      "|    total_timesteps    | 22500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4499      |\n",
      "|    policy_loss        | -12.3     |\n",
      "|    reward             | 2.7845702 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 1.22      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 76          |\n",
      "|    iterations         | 4600        |\n",
      "|    time_elapsed       | 301         |\n",
      "|    total_timesteps    | 23000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.2       |\n",
      "|    explained_variance | -1.07e-06   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 4599        |\n",
      "|    policy_loss        | 43.4        |\n",
      "|    reward             | -0.90050876 |\n",
      "|    std                | 1.05        |\n",
      "|    value_loss         | 21.2        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 4700       |\n",
      "|    time_elapsed       | 307        |\n",
      "|    total_timesteps    | 23500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4699       |\n",
      "|    policy_loss        | 14.9       |\n",
      "|    reward             | -3.0099819 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 2.64       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 4800       |\n",
      "|    time_elapsed       | 313        |\n",
      "|    total_timesteps    | 24000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.2      |\n",
      "|    explained_variance | -0.00128   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4799       |\n",
      "|    policy_loss        | -8.01      |\n",
      "|    reward             | -0.4011342 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 4.15       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 76       |\n",
      "|    iterations         | 4900     |\n",
      "|    time_elapsed       | 320      |\n",
      "|    total_timesteps    | 24500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13.2    |\n",
      "|    explained_variance | 0.000764 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4899     |\n",
      "|    policy_loss        | 11.2     |\n",
      "|    reward             | 0.841704 |\n",
      "|    std                | 1.05     |\n",
      "|    value_loss         | 2.05     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 5000       |\n",
      "|    time_elapsed       | 327        |\n",
      "|    total_timesteps    | 25000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4999       |\n",
      "|    policy_loss        | -60.8      |\n",
      "|    reward             | 0.15582566 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 24         |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 5100       |\n",
      "|    time_elapsed       | 333        |\n",
      "|    total_timesteps    | 25500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5099       |\n",
      "|    policy_loss        | 27.1       |\n",
      "|    reward             | 0.30574417 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 4.68       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 76          |\n",
      "|    iterations         | 5200        |\n",
      "|    time_elapsed       | 339         |\n",
      "|    total_timesteps    | 26000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.2       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 5199        |\n",
      "|    policy_loss        | -61.9       |\n",
      "|    reward             | -0.44954076 |\n",
      "|    std                | 1.05        |\n",
      "|    value_loss         | 47.9        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 5300       |\n",
      "|    time_elapsed       | 346        |\n",
      "|    total_timesteps    | 26500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5299       |\n",
      "|    policy_loss        | 85.8       |\n",
      "|    reward             | -1.8884283 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 47.9       |\n",
      "--------------------------------------\n",
      "day: 2956, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3820537.15\n",
      "total_reward: 2820537.15\n",
      "total_cost: 2333.34\n",
      "total_trades: 13863\n",
      "Sharpe: 0.755\n",
      "=================================\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 76          |\n",
      "|    iterations         | 5400        |\n",
      "|    time_elapsed       | 353         |\n",
      "|    total_timesteps    | 27000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.2       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 5399        |\n",
      "|    policy_loss        | -36.7       |\n",
      "|    reward             | -0.71919525 |\n",
      "|    std                | 1.05        |\n",
      "|    value_loss         | 8.01        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 5500       |\n",
      "|    time_elapsed       | 359        |\n",
      "|    total_timesteps    | 27500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5499       |\n",
      "|    policy_loss        | 28         |\n",
      "|    reward             | 0.04317059 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 4.78       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 5600      |\n",
      "|    time_elapsed       | 366       |\n",
      "|    total_timesteps    | 28000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5599      |\n",
      "|    policy_loss        | -20.8     |\n",
      "|    reward             | 1.0930084 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 8.9       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 5700      |\n",
      "|    time_elapsed       | 372       |\n",
      "|    total_timesteps    | 28500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5699      |\n",
      "|    policy_loss        | 9.76      |\n",
      "|    reward             | 1.4079657 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 1.63      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 5800      |\n",
      "|    time_elapsed       | 378       |\n",
      "|    total_timesteps    | 29000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5799      |\n",
      "|    policy_loss        | 65        |\n",
      "|    reward             | 1.3563333 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 29.8      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 76          |\n",
      "|    iterations         | 5900        |\n",
      "|    time_elapsed       | 385         |\n",
      "|    total_timesteps    | 29500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.2       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 5899        |\n",
      "|    policy_loss        | -9.18       |\n",
      "|    reward             | -0.98194546 |\n",
      "|    std                | 1.05        |\n",
      "|    value_loss         | 2.77        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 6000       |\n",
      "|    time_elapsed       | 391        |\n",
      "|    total_timesteps    | 30000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5999       |\n",
      "|    policy_loss        | -47.5      |\n",
      "|    reward             | 0.24744926 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 15.2       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 6100      |\n",
      "|    time_elapsed       | 398       |\n",
      "|    total_timesteps    | 30500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6099      |\n",
      "|    policy_loss        | 13.5      |\n",
      "|    reward             | 0.3050252 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 1.78      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 6200      |\n",
      "|    time_elapsed       | 404       |\n",
      "|    total_timesteps    | 31000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.2     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6199      |\n",
      "|    policy_loss        | -39       |\n",
      "|    reward             | -3.178119 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 34.1      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 6300      |\n",
      "|    time_elapsed       | 411       |\n",
      "|    total_timesteps    | 31500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6299      |\n",
      "|    policy_loss        | 5.18      |\n",
      "|    reward             | 0.5019013 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 0.791     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 76       |\n",
      "|    iterations         | 6400     |\n",
      "|    time_elapsed       | 417      |\n",
      "|    total_timesteps    | 32000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13.2    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6399     |\n",
      "|    policy_loss        | 10       |\n",
      "|    reward             | 2.060485 |\n",
      "|    std                | 1.05     |\n",
      "|    value_loss         | 4.14     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 6500       |\n",
      "|    time_elapsed       | 423        |\n",
      "|    total_timesteps    | 32500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6499       |\n",
      "|    policy_loss        | 61.4       |\n",
      "|    reward             | -0.3360464 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 28.5       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 6600       |\n",
      "|    time_elapsed       | 430        |\n",
      "|    total_timesteps    | 33000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.2      |\n",
      "|    explained_variance | -2.38e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6599       |\n",
      "|    policy_loss        | -37.8      |\n",
      "|    reward             | -1.8053725 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 8.66       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 6700      |\n",
      "|    time_elapsed       | 436       |\n",
      "|    total_timesteps    | 33500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6699      |\n",
      "|    policy_loss        | -0.375    |\n",
      "|    reward             | 1.2178605 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 0.144     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 6800       |\n",
      "|    time_elapsed       | 444        |\n",
      "|    total_timesteps    | 34000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6799       |\n",
      "|    policy_loss        | 52.7       |\n",
      "|    reward             | -1.5647848 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 13.4       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 76          |\n",
      "|    iterations         | 6900        |\n",
      "|    time_elapsed       | 450         |\n",
      "|    total_timesteps    | 34500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.2       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 6899        |\n",
      "|    policy_loss        | -29.8       |\n",
      "|    reward             | -0.41191584 |\n",
      "|    std                | 1.06        |\n",
      "|    value_loss         | 6.98        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 76          |\n",
      "|    iterations         | 7000        |\n",
      "|    time_elapsed       | 456         |\n",
      "|    total_timesteps    | 35000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.2       |\n",
      "|    explained_variance | 1.19e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 6999        |\n",
      "|    policy_loss        | 32.2        |\n",
      "|    reward             | 0.071303174 |\n",
      "|    std                | 1.06        |\n",
      "|    value_loss         | 5.85        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 76          |\n",
      "|    iterations         | 7100        |\n",
      "|    time_elapsed       | 463         |\n",
      "|    total_timesteps    | 35500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.3       |\n",
      "|    explained_variance | 5.96e-08    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 7099        |\n",
      "|    policy_loss        | -1.42       |\n",
      "|    reward             | 0.022602523 |\n",
      "|    std                | 1.06        |\n",
      "|    value_loss         | 0.0115      |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 76           |\n",
      "|    iterations         | 7200         |\n",
      "|    time_elapsed       | 469          |\n",
      "|    total_timesteps    | 36000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -13.3        |\n",
      "|    explained_variance | -1.19e-07    |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 7199         |\n",
      "|    policy_loss        | -8.59        |\n",
      "|    reward             | -0.043387026 |\n",
      "|    std                | 1.06         |\n",
      "|    value_loss         | 0.614        |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 7300       |\n",
      "|    time_elapsed       | 476        |\n",
      "|    total_timesteps    | 36500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.3      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7299       |\n",
      "|    policy_loss        | 7.23       |\n",
      "|    reward             | 0.22416192 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 0.475      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 7400      |\n",
      "|    time_elapsed       | 482       |\n",
      "|    total_timesteps    | 37000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.3     |\n",
      "|    explained_variance | 0.000354  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7399      |\n",
      "|    policy_loss        | -26.2     |\n",
      "|    reward             | 1.4523635 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 7.78      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 7500      |\n",
      "|    time_elapsed       | 488       |\n",
      "|    total_timesteps    | 37500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.3     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7499      |\n",
      "|    policy_loss        | 3.78      |\n",
      "|    reward             | 1.2147431 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 0.244     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 7600      |\n",
      "|    time_elapsed       | 497       |\n",
      "|    total_timesteps    | 38000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.3     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7599      |\n",
      "|    policy_loss        | 24.8      |\n",
      "|    reward             | 0.7272911 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 4.76      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 7700       |\n",
      "|    time_elapsed       | 504        |\n",
      "|    total_timesteps    | 38500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7699       |\n",
      "|    policy_loss        | 2.82       |\n",
      "|    reward             | 0.07195455 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 0.079      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 7800       |\n",
      "|    time_elapsed       | 510        |\n",
      "|    total_timesteps    | 39000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7799       |\n",
      "|    policy_loss        | 5.46       |\n",
      "|    reward             | -1.4417971 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 0.529      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 7900       |\n",
      "|    time_elapsed       | 517        |\n",
      "|    total_timesteps    | 39500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7899       |\n",
      "|    policy_loss        | -37.2      |\n",
      "|    reward             | 0.45041344 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 8.93       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 76          |\n",
      "|    iterations         | 8000        |\n",
      "|    time_elapsed       | 523         |\n",
      "|    total_timesteps    | 40000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.4       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 7999        |\n",
      "|    policy_loss        | -16.5       |\n",
      "|    reward             | -0.94051737 |\n",
      "|    std                | 1.07        |\n",
      "|    value_loss         | 3.37        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 8100      |\n",
      "|    time_elapsed       | 530       |\n",
      "|    total_timesteps    | 40500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.4     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8099      |\n",
      "|    policy_loss        | -43.8     |\n",
      "|    reward             | 3.6815367 |\n",
      "|    std                | 1.08      |\n",
      "|    value_loss         | 28        |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 8200      |\n",
      "|    time_elapsed       | 536       |\n",
      "|    total_timesteps    | 41000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8199      |\n",
      "|    policy_loss        | -241      |\n",
      "|    reward             | -7.686483 |\n",
      "|    std                | 1.08      |\n",
      "|    value_loss         | 472       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 8300       |\n",
      "|    time_elapsed       | 543        |\n",
      "|    total_timesteps    | 41500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.5      |\n",
      "|    explained_variance | 0.877      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8299       |\n",
      "|    policy_loss        | 2.9        |\n",
      "|    reward             | -2.3422384 |\n",
      "|    std                | 1.08       |\n",
      "|    value_loss         | 0.599      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 76          |\n",
      "|    iterations         | 8400        |\n",
      "|    time_elapsed       | 549         |\n",
      "|    total_timesteps    | 42000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.5       |\n",
      "|    explained_variance | 1.19e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 8399        |\n",
      "|    policy_loss        | -27.8       |\n",
      "|    reward             | 0.073032334 |\n",
      "|    std                | 1.09        |\n",
      "|    value_loss         | 6.13        |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 76       |\n",
      "|    iterations         | 8500     |\n",
      "|    time_elapsed       | 556      |\n",
      "|    total_timesteps    | 42500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13.5    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8499     |\n",
      "|    policy_loss        | 6.22     |\n",
      "|    reward             | 1.670236 |\n",
      "|    std                | 1.09     |\n",
      "|    value_loss         | 0.739    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 8600      |\n",
      "|    time_elapsed       | 562       |\n",
      "|    total_timesteps    | 43000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8599      |\n",
      "|    policy_loss        | -11.7     |\n",
      "|    reward             | 2.4182367 |\n",
      "|    std                | 1.09      |\n",
      "|    value_loss         | 2.75      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 8700      |\n",
      "|    time_elapsed       | 569       |\n",
      "|    total_timesteps    | 43500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8699      |\n",
      "|    policy_loss        | -53.9     |\n",
      "|    reward             | 2.0949585 |\n",
      "|    std                | 1.09      |\n",
      "|    value_loss         | 24.2      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 8800       |\n",
      "|    time_elapsed       | 575        |\n",
      "|    total_timesteps    | 44000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.6      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8799       |\n",
      "|    policy_loss        | 75.2       |\n",
      "|    reward             | -3.0242293 |\n",
      "|    std                | 1.09       |\n",
      "|    value_loss         | 41.2       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 8900      |\n",
      "|    time_elapsed       | 582       |\n",
      "|    total_timesteps    | 44500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.6     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8899      |\n",
      "|    policy_loss        | 24        |\n",
      "|    reward             | 2.2588484 |\n",
      "|    std                | 1.09      |\n",
      "|    value_loss         | 3.73      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 9000      |\n",
      "|    time_elapsed       | 589       |\n",
      "|    total_timesteps    | 45000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8999      |\n",
      "|    policy_loss        | 26.9      |\n",
      "|    reward             | 0.2703071 |\n",
      "|    std                | 1.09      |\n",
      "|    value_loss         | 4.82      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 9100       |\n",
      "|    time_elapsed       | 595        |\n",
      "|    total_timesteps    | 45500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.6      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9099       |\n",
      "|    policy_loss        | 0.954      |\n",
      "|    reward             | 0.80054474 |\n",
      "|    std                | 1.09       |\n",
      "|    value_loss         | 0.428      |\n",
      "--------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 76            |\n",
      "|    iterations         | 9200          |\n",
      "|    time_elapsed       | 602           |\n",
      "|    total_timesteps    | 46000         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -13.6         |\n",
      "|    explained_variance | 5.96e-08      |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 9199          |\n",
      "|    policy_loss        | 18.1          |\n",
      "|    reward             | -0.0047496576 |\n",
      "|    std                | 1.09          |\n",
      "|    value_loss         | 3.33          |\n",
      "-----------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 9300      |\n",
      "|    time_elapsed       | 609       |\n",
      "|    total_timesteps    | 46500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9299      |\n",
      "|    policy_loss        | 20        |\n",
      "|    reward             | 1.4995118 |\n",
      "|    std                | 1.1       |\n",
      "|    value_loss         | 2.42      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 76          |\n",
      "|    iterations         | 9400        |\n",
      "|    time_elapsed       | 616         |\n",
      "|    total_timesteps    | 47000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.6       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 9399        |\n",
      "|    policy_loss        | 35          |\n",
      "|    reward             | -0.15988031 |\n",
      "|    std                | 1.1         |\n",
      "|    value_loss         | 16.8        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 9500       |\n",
      "|    time_elapsed       | 623        |\n",
      "|    total_timesteps    | 47500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.6      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9499       |\n",
      "|    policy_loss        | 13.5       |\n",
      "|    reward             | 0.62789416 |\n",
      "|    std                | 1.1        |\n",
      "|    value_loss         | 1.21       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 75        |\n",
      "|    iterations         | 9600      |\n",
      "|    time_elapsed       | 631       |\n",
      "|    total_timesteps    | 48000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.7     |\n",
      "|    explained_variance | -0.00209  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9599      |\n",
      "|    policy_loss        | 5.71      |\n",
      "|    reward             | -1.053255 |\n",
      "|    std                | 1.1       |\n",
      "|    value_loss         | 1.08      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 9700       |\n",
      "|    time_elapsed       | 637        |\n",
      "|    total_timesteps    | 48500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9699       |\n",
      "|    policy_loss        | 11.8       |\n",
      "|    reward             | -1.0911041 |\n",
      "|    std                | 1.11       |\n",
      "|    value_loss         | 1.81       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 9800       |\n",
      "|    time_elapsed       | 644        |\n",
      "|    total_timesteps    | 49000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.7      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9799       |\n",
      "|    policy_loss        | -53.9      |\n",
      "|    reward             | -0.6852425 |\n",
      "|    std                | 1.11       |\n",
      "|    value_loss         | 19.2       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 9900      |\n",
      "|    time_elapsed       | 651       |\n",
      "|    total_timesteps    | 49500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9899      |\n",
      "|    policy_loss        | -7.19     |\n",
      "|    reward             | 1.4614675 |\n",
      "|    std                | 1.11      |\n",
      "|    value_loss         | 1.13      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 10000     |\n",
      "|    time_elapsed       | 657       |\n",
      "|    total_timesteps    | 50000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9999      |\n",
      "|    policy_loss        | 8.59      |\n",
      "|    reward             | -5.143752 |\n",
      "|    std                | 1.11      |\n",
      "|    value_loss         | 1.41      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 10100     |\n",
      "|    time_elapsed       | 664       |\n",
      "|    total_timesteps    | 50500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.7     |\n",
      "|    explained_variance | 0.00131   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 10099     |\n",
      "|    policy_loss        | 1.42      |\n",
      "|    reward             | 1.1677157 |\n",
      "|    std                | 1.11      |\n",
      "|    value_loss         | 0.953     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 10200     |\n",
      "|    time_elapsed       | 670       |\n",
      "|    total_timesteps    | 51000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 10199     |\n",
      "|    policy_loss        | 22.4      |\n",
      "|    reward             | 1.0567722 |\n",
      "|    std                | 1.11      |\n",
      "|    value_loss         | 3.61      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 10300     |\n",
      "|    time_elapsed       | 677       |\n",
      "|    total_timesteps    | 51500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 10299     |\n",
      "|    policy_loss        | -18.3     |\n",
      "|    reward             | 1.5617483 |\n",
      "|    std                | 1.1       |\n",
      "|    value_loss         | 1.75      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 10400      |\n",
      "|    time_elapsed       | 683        |\n",
      "|    total_timesteps    | 52000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.7      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 10399      |\n",
      "|    policy_loss        | 80         |\n",
      "|    reward             | -0.7305483 |\n",
      "|    std                | 1.11       |\n",
      "|    value_loss         | 34.5       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 10500      |\n",
      "|    time_elapsed       | 690        |\n",
      "|    total_timesteps    | 52500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 10499      |\n",
      "|    policy_loss        | 56.1       |\n",
      "|    reward             | -3.2211938 |\n",
      "|    std                | 1.12       |\n",
      "|    value_loss         | 18.4       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 76          |\n",
      "|    iterations         | 10600       |\n",
      "|    time_elapsed       | 696         |\n",
      "|    total_timesteps    | 53000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.7       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 10599       |\n",
      "|    policy_loss        | -77.2       |\n",
      "|    reward             | -0.12701938 |\n",
      "|    std                | 1.11        |\n",
      "|    value_loss         | 70.5        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 10700      |\n",
      "|    time_elapsed       | 703        |\n",
      "|    total_timesteps    | 53500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.7      |\n",
      "|    explained_variance | 1.79e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 10699      |\n",
      "|    policy_loss        | 5.55       |\n",
      "|    reward             | 0.24968919 |\n",
      "|    std                | 1.11       |\n",
      "|    value_loss         | 0.469      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 76          |\n",
      "|    iterations         | 10800       |\n",
      "|    time_elapsed       | 709         |\n",
      "|    total_timesteps    | 54000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.7       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 10799       |\n",
      "|    policy_loss        | 13.8        |\n",
      "|    reward             | -0.54115534 |\n",
      "|    std                | 1.11        |\n",
      "|    value_loss         | 1.45        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 10900      |\n",
      "|    time_elapsed       | 716        |\n",
      "|    total_timesteps    | 54500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 10899      |\n",
      "|    policy_loss        | 17.7       |\n",
      "|    reward             | -1.3072451 |\n",
      "|    std                | 1.11       |\n",
      "|    value_loss         | 3.47       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 11000     |\n",
      "|    time_elapsed       | 722       |\n",
      "|    total_timesteps    | 55000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 10999     |\n",
      "|    policy_loss        | -14.6     |\n",
      "|    reward             | 0.5806056 |\n",
      "|    std                | 1.11      |\n",
      "|    value_loss         | 1.3       |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 76          |\n",
      "|    iterations         | 11100       |\n",
      "|    time_elapsed       | 728         |\n",
      "|    total_timesteps    | 55500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.7       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 11099       |\n",
      "|    policy_loss        | 64          |\n",
      "|    reward             | -0.26855582 |\n",
      "|    std                | 1.11        |\n",
      "|    value_loss         | 28.7        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 11200     |\n",
      "|    time_elapsed       | 735       |\n",
      "|    total_timesteps    | 56000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.7     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 11199     |\n",
      "|    policy_loss        | -6        |\n",
      "|    reward             | 0.5708982 |\n",
      "|    std                | 1.11      |\n",
      "|    value_loss         | 1.16      |\n",
      "-------------------------------------\n",
      "day: 2956, episode: 20\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3096110.18\n",
      "total_reward: 2096110.18\n",
      "total_cost: 1330.28\n",
      "total_trades: 15578\n",
      "Sharpe: 0.655\n",
      "=================================\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 11300      |\n",
      "|    time_elapsed       | 742        |\n",
      "|    total_timesteps    | 56500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.7      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 11299      |\n",
      "|    policy_loss        | 21.1       |\n",
      "|    reward             | -0.5560755 |\n",
      "|    std                | 1.11       |\n",
      "|    value_loss         | 2.78       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 11400     |\n",
      "|    time_elapsed       | 749       |\n",
      "|    total_timesteps    | 57000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.6     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 11399     |\n",
      "|    policy_loss        | 11.9      |\n",
      "|    reward             | 0.5178107 |\n",
      "|    std                | 1.1       |\n",
      "|    value_loss         | 0.991     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 11500     |\n",
      "|    time_elapsed       | 755       |\n",
      "|    total_timesteps    | 57500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 11499     |\n",
      "|    policy_loss        | 13.6      |\n",
      "|    reward             | 2.5873103 |\n",
      "|    std                | 1.1       |\n",
      "|    value_loss         | 2.69      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 76          |\n",
      "|    iterations         | 11600       |\n",
      "|    time_elapsed       | 761         |\n",
      "|    total_timesteps    | 58000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.7       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 11599       |\n",
      "|    policy_loss        | -0.00677    |\n",
      "|    reward             | -0.34688893 |\n",
      "|    std                | 1.11        |\n",
      "|    value_loss         | 0.805       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 11700      |\n",
      "|    time_elapsed       | 768        |\n",
      "|    total_timesteps    | 58500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 11699      |\n",
      "|    policy_loss        | 48.4       |\n",
      "|    reward             | -0.5136273 |\n",
      "|    std                | 1.11       |\n",
      "|    value_loss         | 14         |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 11800     |\n",
      "|    time_elapsed       | 774       |\n",
      "|    total_timesteps    | 59000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 11799     |\n",
      "|    policy_loss        | 42.9      |\n",
      "|    reward             | 0.7253645 |\n",
      "|    std                | 1.11      |\n",
      "|    value_loss         | 16.5      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 11900      |\n",
      "|    time_elapsed       | 781        |\n",
      "|    total_timesteps    | 59500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 11899      |\n",
      "|    policy_loss        | -18.4      |\n",
      "|    reward             | -0.1266126 |\n",
      "|    std                | 1.11       |\n",
      "|    value_loss         | 2.64       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 12000      |\n",
      "|    time_elapsed       | 788        |\n",
      "|    total_timesteps    | 60000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.7      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 11999      |\n",
      "|    policy_loss        | -17.1      |\n",
      "|    reward             | -2.2351153 |\n",
      "|    std                | 1.11       |\n",
      "|    value_loss         | 2.07       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 12100      |\n",
      "|    time_elapsed       | 794        |\n",
      "|    total_timesteps    | 60500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.7      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 12099      |\n",
      "|    policy_loss        | -8.91      |\n",
      "|    reward             | 0.22923543 |\n",
      "|    std                | 1.11       |\n",
      "|    value_loss         | 0.48       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 12200     |\n",
      "|    time_elapsed       | 802       |\n",
      "|    total_timesteps    | 61000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.7     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 12199     |\n",
      "|    policy_loss        | -41.3     |\n",
      "|    reward             | 0.3988339 |\n",
      "|    std                | 1.11      |\n",
      "|    value_loss         | 7.35      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 75        |\n",
      "|    iterations         | 12300     |\n",
      "|    time_elapsed       | 809       |\n",
      "|    total_timesteps    | 61500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 12299     |\n",
      "|    policy_loss        | -18.3     |\n",
      "|    reward             | 2.0224595 |\n",
      "|    std                | 1.11      |\n",
      "|    value_loss         | 4.54      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 75        |\n",
      "|    iterations         | 12400     |\n",
      "|    time_elapsed       | 816       |\n",
      "|    total_timesteps    | 62000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 12399     |\n",
      "|    policy_loss        | 83.3      |\n",
      "|    reward             | 3.0632935 |\n",
      "|    std                | 1.11      |\n",
      "|    value_loss         | 40.8      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 75       |\n",
      "|    iterations         | 12500    |\n",
      "|    time_elapsed       | 822      |\n",
      "|    total_timesteps    | 62500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13.7    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 12499    |\n",
      "|    policy_loss        | -67      |\n",
      "|    reward             | 4.57148  |\n",
      "|    std                | 1.11     |\n",
      "|    value_loss         | 26.9     |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 75          |\n",
      "|    iterations         | 12600       |\n",
      "|    time_elapsed       | 829         |\n",
      "|    total_timesteps    | 63000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.7       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 12599       |\n",
      "|    policy_loss        | -7.47       |\n",
      "|    reward             | -0.84842956 |\n",
      "|    std                | 1.11        |\n",
      "|    value_loss         | 0.598       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 76          |\n",
      "|    iterations         | 12700       |\n",
      "|    time_elapsed       | 835         |\n",
      "|    total_timesteps    | 63500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.7       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 12699       |\n",
      "|    policy_loss        | -61         |\n",
      "|    reward             | -0.30088276 |\n",
      "|    std                | 1.11        |\n",
      "|    value_loss         | 26.1        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 76          |\n",
      "|    iterations         | 12800       |\n",
      "|    time_elapsed       | 841         |\n",
      "|    total_timesteps    | 64000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.7       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 12799       |\n",
      "|    policy_loss        | 21.4        |\n",
      "|    reward             | -0.28017792 |\n",
      "|    std                | 1.11        |\n",
      "|    value_loss         | 2.75        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 12900     |\n",
      "|    time_elapsed       | 848       |\n",
      "|    total_timesteps    | 64500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 12899     |\n",
      "|    policy_loss        | -14.4     |\n",
      "|    reward             | 1.7946421 |\n",
      "|    std                | 1.11      |\n",
      "|    value_loss         | 1.33      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 13000      |\n",
      "|    time_elapsed       | 854        |\n",
      "|    total_timesteps    | 65000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 12999      |\n",
      "|    policy_loss        | 10         |\n",
      "|    reward             | -1.3700444 |\n",
      "|    std                | 1.12       |\n",
      "|    value_loss         | 2.33       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 13100     |\n",
      "|    time_elapsed       | 861       |\n",
      "|    total_timesteps    | 65500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.8     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 13099     |\n",
      "|    policy_loss        | -80       |\n",
      "|    reward             | 3.5061855 |\n",
      "|    std                | 1.12      |\n",
      "|    value_loss         | 34.3      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 13200      |\n",
      "|    time_elapsed       | 867        |\n",
      "|    total_timesteps    | 66000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.8      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 13199      |\n",
      "|    policy_loss        | -24.3      |\n",
      "|    reward             | -1.3941191 |\n",
      "|    std                | 1.12       |\n",
      "|    value_loss         | 3.67       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 76       |\n",
      "|    iterations         | 13300    |\n",
      "|    time_elapsed       | 874      |\n",
      "|    total_timesteps    | 66500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13.8    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 13299    |\n",
      "|    policy_loss        | -53.8    |\n",
      "|    reward             | 0.888579 |\n",
      "|    std                | 1.13     |\n",
      "|    value_loss         | 21.8     |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 76          |\n",
      "|    iterations         | 13400       |\n",
      "|    time_elapsed       | 880         |\n",
      "|    total_timesteps    | 67000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.9       |\n",
      "|    explained_variance | 1.19e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 13399       |\n",
      "|    policy_loss        | -0.693      |\n",
      "|    reward             | -0.22242492 |\n",
      "|    std                | 1.13        |\n",
      "|    value_loss         | 0.0724      |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 13500      |\n",
      "|    time_elapsed       | 886        |\n",
      "|    total_timesteps    | 67500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.9      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 13499      |\n",
      "|    policy_loss        | 14.8       |\n",
      "|    reward             | -0.4966314 |\n",
      "|    std                | 1.13       |\n",
      "|    value_loss         | 2.82       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 13600      |\n",
      "|    time_elapsed       | 892        |\n",
      "|    total_timesteps    | 68000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.9      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 13599      |\n",
      "|    policy_loss        | -71.5      |\n",
      "|    reward             | -1.9045023 |\n",
      "|    std                | 1.14       |\n",
      "|    value_loss         | 22.1       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 13700     |\n",
      "|    time_elapsed       | 899       |\n",
      "|    total_timesteps    | 68500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.9     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 13699     |\n",
      "|    policy_loss        | 37.9      |\n",
      "|    reward             | 1.8872564 |\n",
      "|    std                | 1.14      |\n",
      "|    value_loss         | 17.4      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 13800     |\n",
      "|    time_elapsed       | 905       |\n",
      "|    total_timesteps    | 69000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.9     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 13799     |\n",
      "|    policy_loss        | -14.5     |\n",
      "|    reward             | 1.7286607 |\n",
      "|    std                | 1.14      |\n",
      "|    value_loss         | 1.4       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 13900      |\n",
      "|    time_elapsed       | 911        |\n",
      "|    total_timesteps    | 69500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.9      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 13899      |\n",
      "|    policy_loss        | 19.7       |\n",
      "|    reward             | -2.5792902 |\n",
      "|    std                | 1.14       |\n",
      "|    value_loss         | 2.53       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 76          |\n",
      "|    iterations         | 14000       |\n",
      "|    time_elapsed       | 919         |\n",
      "|    total_timesteps    | 70000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -14         |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 13999       |\n",
      "|    policy_loss        | -5.79       |\n",
      "|    reward             | -0.59969866 |\n",
      "|    std                | 1.15        |\n",
      "|    value_loss         | 1.52        |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 76       |\n",
      "|    iterations         | 14100    |\n",
      "|    time_elapsed       | 926      |\n",
      "|    total_timesteps    | 70500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -14      |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 14099    |\n",
      "|    policy_loss        | 1.58     |\n",
      "|    reward             | 0.636334 |\n",
      "|    std                | 1.15     |\n",
      "|    value_loss         | 0.129    |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 14200      |\n",
      "|    time_elapsed       | 932        |\n",
      "|    total_timesteps    | 71000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -14        |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 14199      |\n",
      "|    policy_loss        | 6.38       |\n",
      "|    reward             | 0.10201796 |\n",
      "|    std                | 1.15       |\n",
      "|    value_loss         | 0.223      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 14300     |\n",
      "|    time_elapsed       | 939       |\n",
      "|    total_timesteps    | 71500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -14       |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 14299     |\n",
      "|    policy_loss        | 16.9      |\n",
      "|    reward             | 0.7483689 |\n",
      "|    std                | 1.15      |\n",
      "|    value_loss         | 1.99      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 14400      |\n",
      "|    time_elapsed       | 945        |\n",
      "|    total_timesteps    | 72000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -14        |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 14399      |\n",
      "|    policy_loss        | -57.7      |\n",
      "|    reward             | 0.44097292 |\n",
      "|    std                | 1.15       |\n",
      "|    value_loss         | 18.6       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 76       |\n",
      "|    iterations         | 14500    |\n",
      "|    time_elapsed       | 952      |\n",
      "|    total_timesteps    | 72500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -14      |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 14499    |\n",
      "|    policy_loss        | -7.37    |\n",
      "|    reward             | 0.166733 |\n",
      "|    std                | 1.15     |\n",
      "|    value_loss         | 2.15     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 14600      |\n",
      "|    time_elapsed       | 958        |\n",
      "|    total_timesteps    | 73000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -14        |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 14599      |\n",
      "|    policy_loss        | 17.2       |\n",
      "|    reward             | -3.5714152 |\n",
      "|    std                | 1.15       |\n",
      "|    value_loss         | 3.18       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 14700      |\n",
      "|    time_elapsed       | 964        |\n",
      "|    total_timesteps    | 73500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -14        |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 14699      |\n",
      "|    policy_loss        | 20.7       |\n",
      "|    reward             | -3.2272916 |\n",
      "|    std                | 1.15       |\n",
      "|    value_loss         | 3.26       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 76          |\n",
      "|    iterations         | 14800       |\n",
      "|    time_elapsed       | 971         |\n",
      "|    total_timesteps    | 74000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -14         |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 14799       |\n",
      "|    policy_loss        | -0.754      |\n",
      "|    reward             | 0.059532017 |\n",
      "|    std                | 1.15        |\n",
      "|    value_loss         | 0.0277      |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 76          |\n",
      "|    iterations         | 14900       |\n",
      "|    time_elapsed       | 978         |\n",
      "|    total_timesteps    | 74500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -14         |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 14899       |\n",
      "|    policy_loss        | -43.9       |\n",
      "|    reward             | -0.09065839 |\n",
      "|    std                | 1.15        |\n",
      "|    value_loss         | 9.91        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 15000      |\n",
      "|    time_elapsed       | 984        |\n",
      "|    total_timesteps    | 75000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -14        |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 14999      |\n",
      "|    policy_loss        | -10.2      |\n",
      "|    reward             | -1.2264578 |\n",
      "|    std                | 1.15       |\n",
      "|    value_loss         | 1.59       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 15100      |\n",
      "|    time_elapsed       | 991        |\n",
      "|    total_timesteps    | 75500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -14.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 15099      |\n",
      "|    policy_loss        | 1.67       |\n",
      "|    reward             | -1.9230642 |\n",
      "|    std                | 1.16       |\n",
      "|    value_loss         | 0.478      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 15200     |\n",
      "|    time_elapsed       | 998       |\n",
      "|    total_timesteps    | 76000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -14.1     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15199     |\n",
      "|    policy_loss        | -27.2     |\n",
      "|    reward             | 3.357722  |\n",
      "|    std                | 1.17      |\n",
      "|    value_loss         | 14.6      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 15300      |\n",
      "|    time_elapsed       | 1005       |\n",
      "|    total_timesteps    | 76500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -14.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 15299      |\n",
      "|    policy_loss        | -188       |\n",
      "|    reward             | -5.1026993 |\n",
      "|    std                | 1.16       |\n",
      "|    value_loss         | 251        |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 76          |\n",
      "|    iterations         | 15400       |\n",
      "|    time_elapsed       | 1012        |\n",
      "|    total_timesteps    | 77000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -14.1       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 15399       |\n",
      "|    policy_loss        | 25.8        |\n",
      "|    reward             | -0.20180637 |\n",
      "|    std                | 1.17        |\n",
      "|    value_loss         | 5.62        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 15500     |\n",
      "|    time_elapsed       | 1018      |\n",
      "|    total_timesteps    | 77500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -14.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15499     |\n",
      "|    policy_loss        | 30.1      |\n",
      "|    reward             | 1.1968281 |\n",
      "|    std                | 1.17      |\n",
      "|    value_loss         | 6.52      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 15600     |\n",
      "|    time_elapsed       | 1024      |\n",
      "|    total_timesteps    | 78000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -14.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15599     |\n",
      "|    policy_loss        | 18.9      |\n",
      "|    reward             | 0.6942148 |\n",
      "|    std                | 1.17      |\n",
      "|    value_loss         | 1.69      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 15700      |\n",
      "|    time_elapsed       | 1031       |\n",
      "|    total_timesteps    | 78500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -14.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 15699      |\n",
      "|    policy_loss        | 3.33       |\n",
      "|    reward             | 0.64300376 |\n",
      "|    std                | 1.16       |\n",
      "|    value_loss         | 0.313      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 15800      |\n",
      "|    time_elapsed       | 1040       |\n",
      "|    total_timesteps    | 79000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -14.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 15799      |\n",
      "|    policy_loss        | -43.7      |\n",
      "|    reward             | 0.34339246 |\n",
      "|    std                | 1.16       |\n",
      "|    value_loss         | 10.9       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 75        |\n",
      "|    iterations         | 15900     |\n",
      "|    time_elapsed       | 1048      |\n",
      "|    total_timesteps    | 79500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -14.1     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15899     |\n",
      "|    policy_loss        | 9.44      |\n",
      "|    reward             | 0.8152368 |\n",
      "|    std                | 1.16      |\n",
      "|    value_loss         | 10.2      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 75        |\n",
      "|    iterations         | 16000     |\n",
      "|    time_elapsed       | 1056      |\n",
      "|    total_timesteps    | 80000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -14.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15999     |\n",
      "|    policy_loss        | 4.75      |\n",
      "|    reward             | -1.593721 |\n",
      "|    std                | 1.16      |\n",
      "|    value_loss         | 1.94      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 16100      |\n",
      "|    time_elapsed       | 1062       |\n",
      "|    total_timesteps    | 80500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -14.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 16099      |\n",
      "|    policy_loss        | 17         |\n",
      "|    reward             | 0.81083256 |\n",
      "|    std                | 1.16       |\n",
      "|    value_loss         | 2.65       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 75        |\n",
      "|    iterations         | 16200     |\n",
      "|    time_elapsed       | 1068      |\n",
      "|    total_timesteps    | 81000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -14.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16199     |\n",
      "|    policy_loss        | -29.2     |\n",
      "|    reward             | 0.4330741 |\n",
      "|    std                | 1.16      |\n",
      "|    value_loss         | 5.5       |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 75          |\n",
      "|    iterations         | 16300       |\n",
      "|    time_elapsed       | 1075        |\n",
      "|    total_timesteps    | 81500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -14         |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 16299       |\n",
      "|    policy_loss        | -18.9       |\n",
      "|    reward             | -0.07355666 |\n",
      "|    std                | 1.16        |\n",
      "|    value_loss         | 2.25        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 75        |\n",
      "|    iterations         | 16400     |\n",
      "|    time_elapsed       | 1081      |\n",
      "|    total_timesteps    | 82000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -14.1     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16399     |\n",
      "|    policy_loss        | 29.7      |\n",
      "|    reward             | 1.1086828 |\n",
      "|    std                | 1.16      |\n",
      "|    value_loss         | 6         |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 16500      |\n",
      "|    time_elapsed       | 1087       |\n",
      "|    total_timesteps    | 82500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -14.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 16499      |\n",
      "|    policy_loss        | 24         |\n",
      "|    reward             | -2.9256852 |\n",
      "|    std                | 1.16       |\n",
      "|    value_loss         | 4.82       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 75        |\n",
      "|    iterations         | 16600     |\n",
      "|    time_elapsed       | 1094      |\n",
      "|    total_timesteps    | 83000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -14.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16599     |\n",
      "|    policy_loss        | 5.49      |\n",
      "|    reward             | 0.7411814 |\n",
      "|    std                | 1.16      |\n",
      "|    value_loss         | 0.321     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 75        |\n",
      "|    iterations         | 16700     |\n",
      "|    time_elapsed       | 1103      |\n",
      "|    total_timesteps    | 83500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -14       |\n",
      "|    explained_variance | 2.38e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16699     |\n",
      "|    policy_loss        | -34       |\n",
      "|    reward             | 0.9074942 |\n",
      "|    std                | 1.16      |\n",
      "|    value_loss         | 6         |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 75          |\n",
      "|    iterations         | 16800       |\n",
      "|    time_elapsed       | 1111        |\n",
      "|    total_timesteps    | 84000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -14         |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 16799       |\n",
      "|    policy_loss        | 8.41        |\n",
      "|    reward             | -0.21296987 |\n",
      "|    std                | 1.16        |\n",
      "|    value_loss         | 1.83        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 75        |\n",
      "|    iterations         | 16900     |\n",
      "|    time_elapsed       | 1118      |\n",
      "|    total_timesteps    | 84500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -14       |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16899     |\n",
      "|    policy_loss        | 10.4      |\n",
      "|    reward             | 1.1230139 |\n",
      "|    std                | 1.15      |\n",
      "|    value_loss         | 1.71      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 17000      |\n",
      "|    time_elapsed       | 1124       |\n",
      "|    total_timesteps    | 85000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -14        |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 16999      |\n",
      "|    policy_loss        | -22.5      |\n",
      "|    reward             | -1.3311161 |\n",
      "|    std                | 1.15       |\n",
      "|    value_loss         | 3.63       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 75       |\n",
      "|    iterations         | 17100    |\n",
      "|    time_elapsed       | 1131     |\n",
      "|    total_timesteps    | 85500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -14      |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 17099    |\n",
      "|    policy_loss        | -143     |\n",
      "|    reward             | 2.17161  |\n",
      "|    std                | 1.15     |\n",
      "|    value_loss         | 98.1     |\n",
      "------------------------------------\n",
      "day: 2956, episode: 30\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3125552.05\n",
      "total_reward: 2125552.05\n",
      "total_cost: 1060.36\n",
      "total_trades: 15816\n",
      "Sharpe: 0.652\n",
      "=================================\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 75          |\n",
      "|    iterations         | 17200       |\n",
      "|    time_elapsed       | 1138        |\n",
      "|    total_timesteps    | 86000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -14         |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 17199       |\n",
      "|    policy_loss        | 4.86        |\n",
      "|    reward             | -0.16457966 |\n",
      "|    std                | 1.15        |\n",
      "|    value_loss         | 0.21        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 75        |\n",
      "|    iterations         | 17300     |\n",
      "|    time_elapsed       | 1145      |\n",
      "|    total_timesteps    | 86500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -14.1     |\n",
      "|    explained_variance | 0.0114    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17299     |\n",
      "|    policy_loss        | -5.65     |\n",
      "|    reward             | 0.7578686 |\n",
      "|    std                | 1.16      |\n",
      "|    value_loss         | 0.425     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 17400      |\n",
      "|    time_elapsed       | 1151       |\n",
      "|    total_timesteps    | 87000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -14.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 17399      |\n",
      "|    policy_loss        | -29.1      |\n",
      "|    reward             | -0.4930048 |\n",
      "|    std                | 1.16       |\n",
      "|    value_loss         | 6.22       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 75        |\n",
      "|    iterations         | 17500     |\n",
      "|    time_elapsed       | 1158      |\n",
      "|    total_timesteps    | 87500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -14.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17499     |\n",
      "|    policy_loss        | 7.64      |\n",
      "|    reward             | 1.4008565 |\n",
      "|    std                | 1.16      |\n",
      "|    value_loss         | 0.641     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 17600      |\n",
      "|    time_elapsed       | 1164       |\n",
      "|    total_timesteps    | 88000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -14.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 17599      |\n",
      "|    policy_loss        | 71.6       |\n",
      "|    reward             | -1.7202207 |\n",
      "|    std                | 1.16       |\n",
      "|    value_loss         | 23.8       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 75        |\n",
      "|    iterations         | 17700     |\n",
      "|    time_elapsed       | 1170      |\n",
      "|    total_timesteps    | 88500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -14.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17699     |\n",
      "|    policy_loss        | 7.55      |\n",
      "|    reward             | 1.6379637 |\n",
      "|    std                | 1.16      |\n",
      "|    value_loss         | 5.11      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 75        |\n",
      "|    iterations         | 17800     |\n",
      "|    time_elapsed       | 1178      |\n",
      "|    total_timesteps    | 89000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -14.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17799     |\n",
      "|    policy_loss        | 12.4      |\n",
      "|    reward             | 1.0000808 |\n",
      "|    std                | 1.16      |\n",
      "|    value_loss         | 1.15      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 17900      |\n",
      "|    time_elapsed       | 1184       |\n",
      "|    total_timesteps    | 89500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -14.1      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 17899      |\n",
      "|    policy_loss        | -3.78      |\n",
      "|    reward             | -2.7482648 |\n",
      "|    std                | 1.17       |\n",
      "|    value_loss         | 0.113      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 75          |\n",
      "|    iterations         | 18000       |\n",
      "|    time_elapsed       | 1191        |\n",
      "|    total_timesteps    | 90000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -14.1       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 17999       |\n",
      "|    policy_loss        | 6.49        |\n",
      "|    reward             | -0.19451535 |\n",
      "|    std                | 1.17        |\n",
      "|    value_loss         | 1.03        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 75        |\n",
      "|    iterations         | 18100     |\n",
      "|    time_elapsed       | 1197      |\n",
      "|    total_timesteps    | 90500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -14.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18099     |\n",
      "|    policy_loss        | 3.78      |\n",
      "|    reward             | 1.0474153 |\n",
      "|    std                | 1.17      |\n",
      "|    value_loss         | 0.31      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 18200      |\n",
      "|    time_elapsed       | 1203       |\n",
      "|    total_timesteps    | 91000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -14.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 18199      |\n",
      "|    policy_loss        | 48.7       |\n",
      "|    reward             | 0.14879635 |\n",
      "|    std                | 1.17       |\n",
      "|    value_loss         | 15.3       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 75        |\n",
      "|    iterations         | 18300     |\n",
      "|    time_elapsed       | 1209      |\n",
      "|    total_timesteps    | 91500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -14.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18299     |\n",
      "|    policy_loss        | -117      |\n",
      "|    reward             | 0.7991226 |\n",
      "|    std                | 1.16      |\n",
      "|    value_loss         | 91        |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 75        |\n",
      "|    iterations         | 18400     |\n",
      "|    time_elapsed       | 1216      |\n",
      "|    total_timesteps    | 92000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -14.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18399     |\n",
      "|    policy_loss        | 27.8      |\n",
      "|    reward             | 0.8434765 |\n",
      "|    std                | 1.17      |\n",
      "|    value_loss         | 4.45      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 18500      |\n",
      "|    time_elapsed       | 1222       |\n",
      "|    total_timesteps    | 92500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -14.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 18499      |\n",
      "|    policy_loss        | -9.9       |\n",
      "|    reward             | -0.5112504 |\n",
      "|    std                | 1.17       |\n",
      "|    value_loss         | 3.86       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 75        |\n",
      "|    iterations         | 18600     |\n",
      "|    time_elapsed       | 1229      |\n",
      "|    total_timesteps    | 93000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -14.2     |\n",
      "|    explained_variance | 1.79e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18599     |\n",
      "|    policy_loss        | 30        |\n",
      "|    reward             | 0.6003243 |\n",
      "|    std                | 1.18      |\n",
      "|    value_loss         | 4.73      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 75        |\n",
      "|    iterations         | 18700     |\n",
      "|    time_elapsed       | 1235      |\n",
      "|    total_timesteps    | 93500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -14.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18699     |\n",
      "|    policy_loss        | 1.71      |\n",
      "|    reward             | 1.1696442 |\n",
      "|    std                | 1.18      |\n",
      "|    value_loss         | 0.0811    |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 18800      |\n",
      "|    time_elapsed       | 1241       |\n",
      "|    total_timesteps    | 94000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -14.2      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 18799      |\n",
      "|    policy_loss        | 17.3       |\n",
      "|    reward             | -0.4504907 |\n",
      "|    std                | 1.18       |\n",
      "|    value_loss         | 2.63       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 18900      |\n",
      "|    time_elapsed       | 1248       |\n",
      "|    total_timesteps    | 94500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -14.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 18899      |\n",
      "|    policy_loss        | 47.1       |\n",
      "|    reward             | -1.5428516 |\n",
      "|    std                | 1.18       |\n",
      "|    value_loss         | 26.2       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 75        |\n",
      "|    iterations         | 19000     |\n",
      "|    time_elapsed       | 1254      |\n",
      "|    total_timesteps    | 95000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -14.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18999     |\n",
      "|    policy_loss        | -1.66     |\n",
      "|    reward             | 1.4012325 |\n",
      "|    std                | 1.18      |\n",
      "|    value_loss         | 0.852     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 19100      |\n",
      "|    time_elapsed       | 1260       |\n",
      "|    total_timesteps    | 95500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -14.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 19099      |\n",
      "|    policy_loss        | -66.6      |\n",
      "|    reward             | 0.54533195 |\n",
      "|    std                | 1.19       |\n",
      "|    value_loss         | 25.9       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 19200      |\n",
      "|    time_elapsed       | 1266       |\n",
      "|    total_timesteps    | 96000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -14.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 19199      |\n",
      "|    policy_loss        | -2.6       |\n",
      "|    reward             | -0.0763814 |\n",
      "|    std                | 1.18       |\n",
      "|    value_loss         | 1.26       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 19300      |\n",
      "|    time_elapsed       | 1272       |\n",
      "|    total_timesteps    | 96500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -14.2      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 19299      |\n",
      "|    policy_loss        | 12         |\n",
      "|    reward             | 0.62521863 |\n",
      "|    std                | 1.18       |\n",
      "|    value_loss         | 1.07       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 75        |\n",
      "|    iterations         | 19400     |\n",
      "|    time_elapsed       | 1279      |\n",
      "|    total_timesteps    | 97000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -14.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19399     |\n",
      "|    policy_loss        | 74.2      |\n",
      "|    reward             | 0.9595242 |\n",
      "|    std                | 1.18      |\n",
      "|    value_loss         | 40.1      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 75          |\n",
      "|    iterations         | 19500       |\n",
      "|    time_elapsed       | 1285        |\n",
      "|    total_timesteps    | 97500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -14.2       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 19499       |\n",
      "|    policy_loss        | 13.6        |\n",
      "|    reward             | -0.08855076 |\n",
      "|    std                | 1.18        |\n",
      "|    value_loss         | 1.78        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 75        |\n",
      "|    iterations         | 19600     |\n",
      "|    time_elapsed       | 1291      |\n",
      "|    total_timesteps    | 98000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -14.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19599     |\n",
      "|    policy_loss        | 24.4      |\n",
      "|    reward             | 0.6491391 |\n",
      "|    std                | 1.18      |\n",
      "|    value_loss         | 10.9      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 19700      |\n",
      "|    time_elapsed       | 1297       |\n",
      "|    total_timesteps    | 98500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -14.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 19699      |\n",
      "|    policy_loss        | -31.8      |\n",
      "|    reward             | 0.64789236 |\n",
      "|    std                | 1.19       |\n",
      "|    value_loss         | 6.71       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 75        |\n",
      "|    iterations         | 19800     |\n",
      "|    time_elapsed       | 1304      |\n",
      "|    total_timesteps    | 99000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -14.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19799     |\n",
      "|    policy_loss        | 6.48      |\n",
      "|    reward             | -7.391693 |\n",
      "|    std                | 1.19      |\n",
      "|    value_loss         | 0.695     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 19900      |\n",
      "|    time_elapsed       | 1311       |\n",
      "|    total_timesteps    | 99500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -14.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 19899      |\n",
      "|    policy_loss        | -39.7      |\n",
      "|    reward             | -2.9199648 |\n",
      "|    std                | 1.19       |\n",
      "|    value_loss         | 8.82       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 75        |\n",
      "|    iterations         | 20000     |\n",
      "|    time_elapsed       | 1317      |\n",
      "|    total_timesteps    | 100000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -14.4     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19999     |\n",
      "|    policy_loss        | -42.1     |\n",
      "|    reward             | -6.810991 |\n",
      "|    std                | 1.2       |\n",
      "|    value_loss         | 21.4      |\n",
      "-------------------------------------\n",
      "hit end!\n",
      "==============Get Baseline Stats===========\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Shape of DataFrame:  (400, 8)\n",
      "Annual return         -0.022145\n",
      "Cumulative returns    -0.034921\n",
      "Annual volatility      0.176822\n",
      "Sharpe ratio          -0.038668\n",
      "Calmar ratio          -0.100930\n",
      "Stability              0.215672\n",
      "Max drawdown          -0.219408\n",
      "Omega ratio            0.993578\n",
      "Sortino ratio         -0.053987\n",
      "Skew                        NaN\n",
      "Kurtosis                    NaN\n",
      "Tail ratio             1.003436\n",
      "Daily value at risk   -0.022305\n",
      "dtype: float64\n",
      "result:                       a2c           dji\n",
      "date                                  \n",
      "2021-10-01  1.000000e+06  1.000000e+06\n",
      "2021-10-04  9.997444e+05  9.905746e+05\n",
      "2021-10-05  1.000326e+06  9.996566e+05\n",
      "2021-10-06  1.000333e+06  1.002637e+06\n",
      "2021-10-07  1.001924e+06  1.012483e+06\n",
      "...                  ...           ...\n",
      "2023-04-27  1.099391e+06  9.854252e+05\n",
      "2023-04-28  1.110356e+06  9.933491e+05\n",
      "2023-05-01  1.111104e+06  9.919956e+05\n",
      "2023-05-02  1.096685e+06  9.812993e+05\n",
      "2023-05-03  1.088411e+06  9.734251e+05\n",
      "\n",
      "[399 rows x 2 columns]\n",
      "A2C로 얻은 투자 수익률>> 8.84 %\n",
      "{'n_steps': 5, 'ent_coef': 0.01, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to results/a2c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\se99a\\anaconda3\\lib\\site-packages\\pyfolio\\timeseries.py:724: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  stats = pd.Series()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 78         |\n",
      "|    iterations         | 100        |\n",
      "|    time_elapsed       | 6          |\n",
      "|    total_timesteps    | 500        |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -12.8      |\n",
      "|    explained_variance | -0.182     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 99         |\n",
      "|    policy_loss        | -18.9      |\n",
      "|    reward             | 0.37346768 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 3.63       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 200        |\n",
      "|    time_elapsed       | 13         |\n",
      "|    total_timesteps    | 1000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -12.9      |\n",
      "|    explained_variance | 0.163      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 199        |\n",
      "|    policy_loss        | 4.02       |\n",
      "|    reward             | 0.19715863 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 1.37       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 68         |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 21         |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -12.9      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | -20.1      |\n",
      "|    reward             | -2.1541097 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 4.14       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 61         |\n",
      "|    iterations         | 400        |\n",
      "|    time_elapsed       | 32         |\n",
      "|    total_timesteps    | 2000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -12.9      |\n",
      "|    explained_variance | 0.113      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 399        |\n",
      "|    policy_loss        | -11.9      |\n",
      "|    reward             | -1.1341494 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 2.55       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 56        |\n",
      "|    iterations         | 500       |\n",
      "|    time_elapsed       | 44        |\n",
      "|    total_timesteps    | 2500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -12.9     |\n",
      "|    explained_variance | -0.0298   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 499       |\n",
      "|    policy_loss        | -42.5     |\n",
      "|    reward             | -0.855661 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 12.9      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 56         |\n",
      "|    iterations         | 600        |\n",
      "|    time_elapsed       | 53         |\n",
      "|    total_timesteps    | 3000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -12.9      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 599        |\n",
      "|    policy_loss        | 2.92       |\n",
      "|    reward             | -0.0603571 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 0.0889     |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 56          |\n",
      "|    iterations         | 700         |\n",
      "|    time_elapsed       | 61          |\n",
      "|    total_timesteps    | 3500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -12.9       |\n",
      "|    explained_variance | 0.351       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 699         |\n",
      "|    policy_loss        | 9.77        |\n",
      "|    reward             | -0.80265695 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 0.628       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 56          |\n",
      "|    iterations         | 800         |\n",
      "|    time_elapsed       | 70          |\n",
      "|    total_timesteps    | 4000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -12.9       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 799         |\n",
      "|    policy_loss        | 8           |\n",
      "|    reward             | -0.46115896 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 1.64        |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 56       |\n",
      "|    iterations         | 900      |\n",
      "|    time_elapsed       | 80       |\n",
      "|    total_timesteps    | 4500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -12.9    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 899      |\n",
      "|    policy_loss        | -22.8    |\n",
      "|    reward             | 2.528743 |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 6.34     |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 55          |\n",
      "|    iterations         | 1000        |\n",
      "|    time_elapsed       | 90          |\n",
      "|    total_timesteps    | 5000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13         |\n",
      "|    explained_variance | -0.0204     |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 999         |\n",
      "|    policy_loss        | -90.5       |\n",
      "|    reward             | -0.71857625 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 68.8        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 54        |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 100       |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -12.9     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | -10.1     |\n",
      "|    reward             | 1.0930682 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 2.3       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 54         |\n",
      "|    iterations         | 1200       |\n",
      "|    time_elapsed       | 110        |\n",
      "|    total_timesteps    | 6000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -12.9      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1199       |\n",
      "|    policy_loss        | -12.4      |\n",
      "|    reward             | -1.5370817 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 1.78       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 54        |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 119       |\n",
      "|    total_timesteps    | 6500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -12.9     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1299      |\n",
      "|    policy_loss        | -4.07     |\n",
      "|    reward             | 0.5245674 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 0.599     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 54        |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 128       |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -12.9     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | 10.1      |\n",
      "|    reward             | 0.0924262 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 1.71      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 54         |\n",
      "|    iterations         | 1500       |\n",
      "|    time_elapsed       | 138        |\n",
      "|    total_timesteps    | 7500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -12.9      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1499       |\n",
      "|    policy_loss        | 10.2       |\n",
      "|    reward             | 0.01917343 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 1.25       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 54          |\n",
      "|    iterations         | 1600        |\n",
      "|    time_elapsed       | 147         |\n",
      "|    total_timesteps    | 8000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -12.9       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1599        |\n",
      "|    policy_loss        | -14.1       |\n",
      "|    reward             | 0.021413397 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 4.01        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 54        |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 156       |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13       |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | 168       |\n",
      "|    reward             | 10.272268 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 242       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 54        |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 165       |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13       |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | -36.3     |\n",
      "|    reward             | 0.5626415 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 12.6      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 54       |\n",
      "|    iterations         | 1900     |\n",
      "|    time_elapsed       | 173      |\n",
      "|    total_timesteps    | 9500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -12.9    |\n",
      "|    explained_variance | 0.073    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1899     |\n",
      "|    policy_loss        | -32.9    |\n",
      "|    reward             | 0.534817 |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 6.6      |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 54        |\n",
      "|    iterations         | 2000      |\n",
      "|    time_elapsed       | 182       |\n",
      "|    total_timesteps    | 10000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13       |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1999      |\n",
      "|    policy_loss        | 10.4      |\n",
      "|    reward             | -0.160255 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 2.01      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 54        |\n",
      "|    iterations         | 2100      |\n",
      "|    time_elapsed       | 191       |\n",
      "|    total_timesteps    | 10500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -12.9     |\n",
      "|    explained_variance | -0.202    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2099      |\n",
      "|    policy_loss        | -19.3     |\n",
      "|    reward             | 3.0323389 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 6.42      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 55         |\n",
      "|    iterations         | 2200       |\n",
      "|    time_elapsed       | 198        |\n",
      "|    total_timesteps    | 11000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13        |\n",
      "|    explained_variance | -0.254     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2199       |\n",
      "|    policy_loss        | 18.1       |\n",
      "|    reward             | -0.8174926 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 2.49       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 55        |\n",
      "|    iterations         | 2300      |\n",
      "|    time_elapsed       | 205       |\n",
      "|    total_timesteps    | 11500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -12.9     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2299      |\n",
      "|    policy_loss        | 144       |\n",
      "|    reward             | 4.0964594 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 167       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 56         |\n",
      "|    iterations         | 2400       |\n",
      "|    time_elapsed       | 212        |\n",
      "|    total_timesteps    | 12000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -12.9      |\n",
      "|    explained_variance | -0.00484   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2399       |\n",
      "|    policy_loss        | 21.7       |\n",
      "|    reward             | 0.65422475 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 3.26       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 57         |\n",
      "|    iterations         | 2500       |\n",
      "|    time_elapsed       | 218        |\n",
      "|    total_timesteps    | 12500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -12.9      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2499       |\n",
      "|    policy_loss        | -9.04      |\n",
      "|    reward             | 0.81372374 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 1.19       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 57        |\n",
      "|    iterations         | 2600      |\n",
      "|    time_elapsed       | 226       |\n",
      "|    total_timesteps    | 13000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -12.9     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2599      |\n",
      "|    policy_loss        | 13.3      |\n",
      "|    reward             | 0.8891016 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 2.27      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 57         |\n",
      "|    iterations         | 2700       |\n",
      "|    time_elapsed       | 233        |\n",
      "|    total_timesteps    | 13500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -12.9      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2699       |\n",
      "|    policy_loss        | -10.3      |\n",
      "|    reward             | -1.0178611 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 1.02       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 58          |\n",
      "|    iterations         | 2800        |\n",
      "|    time_elapsed       | 240         |\n",
      "|    total_timesteps    | 14000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -12.9       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 2799        |\n",
      "|    policy_loss        | -43.6       |\n",
      "|    reward             | -0.27203566 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 12.3        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 58          |\n",
      "|    iterations         | 2900        |\n",
      "|    time_elapsed       | 247         |\n",
      "|    total_timesteps    | 14500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -12.9       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 2899        |\n",
      "|    policy_loss        | 40.2        |\n",
      "|    reward             | -0.14517088 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 53.9        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 59         |\n",
      "|    iterations         | 3000       |\n",
      "|    time_elapsed       | 253        |\n",
      "|    total_timesteps    | 15000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -12.9      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2999       |\n",
      "|    policy_loss        | 3.89       |\n",
      "|    reward             | -0.5691294 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 0.191      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 59         |\n",
      "|    iterations         | 3100       |\n",
      "|    time_elapsed       | 260        |\n",
      "|    total_timesteps    | 15500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -12.9      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3099       |\n",
      "|    policy_loss        | -25.1      |\n",
      "|    reward             | 0.39150107 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 6.73       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 59        |\n",
      "|    iterations         | 3200      |\n",
      "|    time_elapsed       | 267       |\n",
      "|    total_timesteps    | 16000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -12.9     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3199      |\n",
      "|    policy_loss        | 55.7      |\n",
      "|    reward             | 1.5703157 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 27.5      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 60         |\n",
      "|    iterations         | 3300       |\n",
      "|    time_elapsed       | 274        |\n",
      "|    total_timesteps    | 16500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -12.9      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3299       |\n",
      "|    policy_loss        | 5.56       |\n",
      "|    reward             | -0.7214406 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 0.91       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 60          |\n",
      "|    iterations         | 3400        |\n",
      "|    time_elapsed       | 281         |\n",
      "|    total_timesteps    | 17000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -12.8       |\n",
      "|    explained_variance | 6.75e-05    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 3399        |\n",
      "|    policy_loss        | -60.4       |\n",
      "|    reward             | -0.72263575 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 123         |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 60         |\n",
      "|    iterations         | 3500       |\n",
      "|    time_elapsed       | 288        |\n",
      "|    total_timesteps    | 17500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -12.9      |\n",
      "|    explained_variance | 3.72e-05   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3499       |\n",
      "|    policy_loss        | 99.7       |\n",
      "|    reward             | -2.3605468 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 79.4       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 60         |\n",
      "|    iterations         | 3600       |\n",
      "|    time_elapsed       | 295        |\n",
      "|    total_timesteps    | 18000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -12.9      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3599       |\n",
      "|    policy_loss        | 2          |\n",
      "|    reward             | 0.74578094 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 0.114      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 61         |\n",
      "|    iterations         | 3700       |\n",
      "|    time_elapsed       | 301        |\n",
      "|    total_timesteps    | 18500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -12.9      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3699       |\n",
      "|    policy_loss        | -2.29      |\n",
      "|    reward             | -0.2921553 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 1.65       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 61         |\n",
      "|    iterations         | 3800       |\n",
      "|    time_elapsed       | 308        |\n",
      "|    total_timesteps    | 19000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -12.9      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3799       |\n",
      "|    policy_loss        | 6.36       |\n",
      "|    reward             | 0.23831333 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 5.9        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 61        |\n",
      "|    iterations         | 3900      |\n",
      "|    time_elapsed       | 315       |\n",
      "|    total_timesteps    | 19500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -12.9     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3899      |\n",
      "|    policy_loss        | 0.633     |\n",
      "|    reward             | 0.7817085 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 0.765     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 62         |\n",
      "|    iterations         | 4000       |\n",
      "|    time_elapsed       | 321        |\n",
      "|    total_timesteps    | 20000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -12.9      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3999       |\n",
      "|    policy_loss        | -59.8      |\n",
      "|    reward             | -7.1277056 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 29         |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 62        |\n",
      "|    iterations         | 4100      |\n",
      "|    time_elapsed       | 330       |\n",
      "|    total_timesteps    | 20500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13       |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4099      |\n",
      "|    policy_loss        | -57.2     |\n",
      "|    reward             | 3.0262315 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 36.9      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 62         |\n",
      "|    iterations         | 4200       |\n",
      "|    time_elapsed       | 338        |\n",
      "|    total_timesteps    | 21000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13        |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4199       |\n",
      "|    policy_loss        | -11        |\n",
      "|    reward             | -1.2910137 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 1.45       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 62         |\n",
      "|    iterations         | 4300       |\n",
      "|    time_elapsed       | 344        |\n",
      "|    total_timesteps    | 21500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -12.9      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4299       |\n",
      "|    policy_loss        | 15.5       |\n",
      "|    reward             | -0.0392661 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 1.91       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 62         |\n",
      "|    iterations         | 4400       |\n",
      "|    time_elapsed       | 351        |\n",
      "|    total_timesteps    | 22000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -12.9      |\n",
      "|    explained_variance | 1.79e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4399       |\n",
      "|    policy_loss        | -21.6      |\n",
      "|    reward             | -2.8966386 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 3.18       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 62        |\n",
      "|    iterations         | 4500      |\n",
      "|    time_elapsed       | 359       |\n",
      "|    total_timesteps    | 22500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -12.9     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4499      |\n",
      "|    policy_loss        | -6.08     |\n",
      "|    reward             | 3.4307816 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 0.384     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 62         |\n",
      "|    iterations         | 4600       |\n",
      "|    time_elapsed       | 366        |\n",
      "|    total_timesteps    | 23000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -12.9      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4599       |\n",
      "|    policy_loss        | 36.8       |\n",
      "|    reward             | 0.26451916 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 14.8       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 62         |\n",
      "|    iterations         | 4700       |\n",
      "|    time_elapsed       | 373        |\n",
      "|    total_timesteps    | 23500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -12.9      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4699       |\n",
      "|    policy_loss        | 62.4       |\n",
      "|    reward             | 0.14211461 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 33.6       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 63         |\n",
      "|    iterations         | 4800       |\n",
      "|    time_elapsed       | 380        |\n",
      "|    total_timesteps    | 24000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -12.9      |\n",
      "|    explained_variance | 0.000177   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4799       |\n",
      "|    policy_loss        | -8.49      |\n",
      "|    reward             | -0.9643945 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 3.54       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 63          |\n",
      "|    iterations         | 4900        |\n",
      "|    time_elapsed       | 387         |\n",
      "|    total_timesteps    | 24500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -12.9       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 4899        |\n",
      "|    policy_loss        | 25.5        |\n",
      "|    reward             | -0.13395627 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 5.67        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 63         |\n",
      "|    iterations         | 5000       |\n",
      "|    time_elapsed       | 393        |\n",
      "|    total_timesteps    | 25000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -12.8      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4999       |\n",
      "|    policy_loss        | -29.8      |\n",
      "|    reward             | 0.91231734 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 6.32       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 63        |\n",
      "|    iterations         | 5100      |\n",
      "|    time_elapsed       | 400       |\n",
      "|    total_timesteps    | 25500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -12.8     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5099      |\n",
      "|    policy_loss        | 45.3      |\n",
      "|    reward             | 0.4821404 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 15.2      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 63        |\n",
      "|    iterations         | 5200      |\n",
      "|    time_elapsed       | 407       |\n",
      "|    total_timesteps    | 26000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -12.9     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5199      |\n",
      "|    policy_loss        | 33.3      |\n",
      "|    reward             | 0.2706141 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 9.95      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 64       |\n",
      "|    iterations         | 5300     |\n",
      "|    time_elapsed       | 413      |\n",
      "|    total_timesteps    | 26500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -12.9    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5299     |\n",
      "|    policy_loss        | 115      |\n",
      "|    reward             | -7.81506 |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 78.7     |\n",
      "------------------------------------\n",
      "day: 2956, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 7030227.44\n",
      "total_reward: 6030227.44\n",
      "total_cost: 6353.09\n",
      "total_trades: 15710\n",
      "Sharpe: 0.931\n",
      "=================================\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 64         |\n",
      "|    iterations         | 5400       |\n",
      "|    time_elapsed       | 421        |\n",
      "|    total_timesteps    | 27000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -12.9      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5399       |\n",
      "|    policy_loss        | -36.9      |\n",
      "|    reward             | -0.6434056 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 6.91       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 64         |\n",
      "|    iterations         | 5500       |\n",
      "|    time_elapsed       | 427        |\n",
      "|    total_timesteps    | 27500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -12.9      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5499       |\n",
      "|    policy_loss        | 15.1       |\n",
      "|    reward             | 0.37199798 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 1.77       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 64          |\n",
      "|    iterations         | 5600        |\n",
      "|    time_elapsed       | 434         |\n",
      "|    total_timesteps    | 28000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13         |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 5599        |\n",
      "|    policy_loss        | -29.2       |\n",
      "|    reward             | -0.50597465 |\n",
      "|    std                | 1.03        |\n",
      "|    value_loss         | 14.3        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 64         |\n",
      "|    iterations         | 5700       |\n",
      "|    time_elapsed       | 441        |\n",
      "|    total_timesteps    | 28500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13        |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5699       |\n",
      "|    policy_loss        | -7.97      |\n",
      "|    reward             | -1.4058943 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 1.23       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 64         |\n",
      "|    iterations         | 5800       |\n",
      "|    time_elapsed       | 448        |\n",
      "|    total_timesteps    | 29000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13        |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5799       |\n",
      "|    policy_loss        | 67.6       |\n",
      "|    reward             | 0.10616293 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 57.8       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 64         |\n",
      "|    iterations         | 5900       |\n",
      "|    time_elapsed       | 454        |\n",
      "|    total_timesteps    | 29500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13        |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5899       |\n",
      "|    policy_loss        | -11.6      |\n",
      "|    reward             | -0.7937558 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 31.4       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 65         |\n",
      "|    iterations         | 6000       |\n",
      "|    time_elapsed       | 461        |\n",
      "|    total_timesteps    | 30000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13        |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5999       |\n",
      "|    policy_loss        | -41.4      |\n",
      "|    reward             | 0.73445165 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 12.5       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 65        |\n",
      "|    iterations         | 6100      |\n",
      "|    time_elapsed       | 468       |\n",
      "|    total_timesteps    | 30500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13       |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6099      |\n",
      "|    policy_loss        | 7.84      |\n",
      "|    reward             | 0.3048041 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 1.16      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 65         |\n",
      "|    iterations         | 6200       |\n",
      "|    time_elapsed       | 475        |\n",
      "|    total_timesteps    | 31000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13        |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6199       |\n",
      "|    policy_loss        | -72.7      |\n",
      "|    reward             | -3.1958854 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 60.5       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 65          |\n",
      "|    iterations         | 6300        |\n",
      "|    time_elapsed       | 481         |\n",
      "|    total_timesteps    | 31500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.1       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 6299        |\n",
      "|    policy_loss        | 6.42        |\n",
      "|    reward             | -0.13584507 |\n",
      "|    std                | 1.04        |\n",
      "|    value_loss         | 2.36        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 65        |\n",
      "|    iterations         | 6400      |\n",
      "|    time_elapsed       | 488       |\n",
      "|    total_timesteps    | 32000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6399      |\n",
      "|    policy_loss        | 28.5      |\n",
      "|    reward             | 4.8535743 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 20.7      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 65         |\n",
      "|    iterations         | 6500       |\n",
      "|    time_elapsed       | 496        |\n",
      "|    total_timesteps    | 32500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13        |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6499       |\n",
      "|    policy_loss        | 77         |\n",
      "|    reward             | -1.0848448 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 51.1       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 65         |\n",
      "|    iterations         | 6600       |\n",
      "|    time_elapsed       | 503        |\n",
      "|    total_timesteps    | 33000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6599       |\n",
      "|    policy_loss        | -31.4      |\n",
      "|    reward             | -2.1435895 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 7.06       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 65       |\n",
      "|    iterations         | 6700     |\n",
      "|    time_elapsed       | 509      |\n",
      "|    total_timesteps    | 33500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13.1    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6699     |\n",
      "|    policy_loss        | -7.77    |\n",
      "|    reward             | 1.128554 |\n",
      "|    std                | 1.04     |\n",
      "|    value_loss         | 0.455    |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 65         |\n",
      "|    iterations         | 6800       |\n",
      "|    time_elapsed       | 516        |\n",
      "|    total_timesteps    | 34000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.1      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6799       |\n",
      "|    policy_loss        | 70.6       |\n",
      "|    reward             | -1.9058748 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 30.7       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 65         |\n",
      "|    iterations         | 6900       |\n",
      "|    time_elapsed       | 523        |\n",
      "|    total_timesteps    | 34500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.1      |\n",
      "|    explained_variance | -0.283     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6899       |\n",
      "|    policy_loss        | -60.7      |\n",
      "|    reward             | -1.1762501 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 24.2       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 66        |\n",
      "|    iterations         | 7000      |\n",
      "|    time_elapsed       | 529       |\n",
      "|    total_timesteps    | 35000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6999      |\n",
      "|    policy_loss        | 17.3      |\n",
      "|    reward             | 2.1029415 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 2.41      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 66          |\n",
      "|    iterations         | 7100        |\n",
      "|    time_elapsed       | 536         |\n",
      "|    total_timesteps    | 35500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.1       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 7099        |\n",
      "|    policy_loss        | -1.92       |\n",
      "|    reward             | 0.051752932 |\n",
      "|    std                | 1.04        |\n",
      "|    value_loss         | 0.0201      |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 66         |\n",
      "|    iterations         | 7200       |\n",
      "|    time_elapsed       | 543        |\n",
      "|    total_timesteps    | 36000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.1      |\n",
      "|    explained_variance | 0.892      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7199       |\n",
      "|    policy_loss        | 1.4        |\n",
      "|    reward             | 0.10893267 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 0.0148     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 66         |\n",
      "|    iterations         | 7300       |\n",
      "|    time_elapsed       | 551        |\n",
      "|    total_timesteps    | 36500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.2      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7299       |\n",
      "|    policy_loss        | 9.45       |\n",
      "|    reward             | 0.29035383 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 1.1        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 66        |\n",
      "|    iterations         | 7400      |\n",
      "|    time_elapsed       | 558       |\n",
      "|    total_timesteps    | 37000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7399      |\n",
      "|    policy_loss        | -60.7     |\n",
      "|    reward             | 2.6407592 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 30.1      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 66        |\n",
      "|    iterations         | 7500      |\n",
      "|    time_elapsed       | 565       |\n",
      "|    total_timesteps    | 37500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7499      |\n",
      "|    policy_loss        | -11.4     |\n",
      "|    reward             | 3.0859706 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 2.08      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 66        |\n",
      "|    iterations         | 7600      |\n",
      "|    time_elapsed       | 573       |\n",
      "|    total_timesteps    | 38000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.1     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7599      |\n",
      "|    policy_loss        | 44.7      |\n",
      "|    reward             | 5.560908  |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 14.7      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 66          |\n",
      "|    iterations         | 7700        |\n",
      "|    time_elapsed       | 580         |\n",
      "|    total_timesteps    | 38500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.2       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 7699        |\n",
      "|    policy_loss        | 2.26        |\n",
      "|    reward             | 0.007342174 |\n",
      "|    std                | 1.05        |\n",
      "|    value_loss         | 0.06        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 66         |\n",
      "|    iterations         | 7800       |\n",
      "|    time_elapsed       | 588        |\n",
      "|    total_timesteps    | 39000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7799       |\n",
      "|    policy_loss        | 5.67       |\n",
      "|    reward             | -0.6327393 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 0.631      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 66         |\n",
      "|    iterations         | 7900       |\n",
      "|    time_elapsed       | 595        |\n",
      "|    total_timesteps    | 39500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.1      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7899       |\n",
      "|    policy_loss        | -36.7      |\n",
      "|    reward             | 0.46874478 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 9.09       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 66        |\n",
      "|    iterations         | 8000      |\n",
      "|    time_elapsed       | 602       |\n",
      "|    total_timesteps    | 40000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.1     |\n",
      "|    explained_variance | 0.0643    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7999      |\n",
      "|    policy_loss        | -33.7     |\n",
      "|    reward             | -1.875058 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 5         |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 66       |\n",
      "|    iterations         | 8100     |\n",
      "|    time_elapsed       | 608      |\n",
      "|    total_timesteps    | 40500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13.1    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8099     |\n",
      "|    policy_loss        | -49.1    |\n",
      "|    reward             | 4.716933 |\n",
      "|    std                | 1.04     |\n",
      "|    value_loss         | 35.4     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 66        |\n",
      "|    iterations         | 8200      |\n",
      "|    time_elapsed       | 617       |\n",
      "|    total_timesteps    | 41000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.2     |\n",
      "|    explained_variance | 0.00088   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8199      |\n",
      "|    policy_loss        | -431      |\n",
      "|    reward             | -9.026962 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 858       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 66         |\n",
      "|    iterations         | 8300       |\n",
      "|    time_elapsed       | 628        |\n",
      "|    total_timesteps    | 41500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.2      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8299       |\n",
      "|    policy_loss        | -30.9      |\n",
      "|    reward             | -1.9656345 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 8.83       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 65          |\n",
      "|    iterations         | 8400        |\n",
      "|    time_elapsed       | 638         |\n",
      "|    total_timesteps    | 42000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.2       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 8399        |\n",
      "|    policy_loss        | -26.1       |\n",
      "|    reward             | -0.22196396 |\n",
      "|    std                | 1.05        |\n",
      "|    value_loss         | 5.94        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 65        |\n",
      "|    iterations         | 8500      |\n",
      "|    time_elapsed       | 649       |\n",
      "|    total_timesteps    | 42500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8499      |\n",
      "|    policy_loss        | 5.43      |\n",
      "|    reward             | 1.3821497 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 0.588     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 65        |\n",
      "|    iterations         | 8600      |\n",
      "|    time_elapsed       | 657       |\n",
      "|    total_timesteps    | 43000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8599      |\n",
      "|    policy_loss        | 7.27      |\n",
      "|    reward             | 2.4765394 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 1.5       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 65        |\n",
      "|    iterations         | 8700      |\n",
      "|    time_elapsed       | 665       |\n",
      "|    total_timesteps    | 43500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8699      |\n",
      "|    policy_loss        | -74.1     |\n",
      "|    reward             | 3.5424778 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 45.5      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 65        |\n",
      "|    iterations         | 8800      |\n",
      "|    time_elapsed       | 673       |\n",
      "|    total_timesteps    | 44000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8799      |\n",
      "|    policy_loss        | 9.3       |\n",
      "|    reward             | -4.504639 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 10        |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 65        |\n",
      "|    iterations         | 8900      |\n",
      "|    time_elapsed       | 680       |\n",
      "|    total_timesteps    | 44500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8899      |\n",
      "|    policy_loss        | 13.6      |\n",
      "|    reward             | 2.2060769 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 1.62      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 65         |\n",
      "|    iterations         | 9000       |\n",
      "|    time_elapsed       | 687        |\n",
      "|    total_timesteps    | 45000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8999       |\n",
      "|    policy_loss        | 18.5       |\n",
      "|    reward             | 0.43892783 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 3.1        |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 65       |\n",
      "|    iterations         | 9100     |\n",
      "|    time_elapsed       | 694      |\n",
      "|    total_timesteps    | 45500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13.2    |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9099     |\n",
      "|    policy_loss        | -23.4    |\n",
      "|    reward             | 1.311423 |\n",
      "|    std                | 1.05     |\n",
      "|    value_loss         | 3.31     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 65         |\n",
      "|    iterations         | 9200       |\n",
      "|    time_elapsed       | 701        |\n",
      "|    total_timesteps    | 46000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9199       |\n",
      "|    policy_loss        | 20.4       |\n",
      "|    reward             | 0.25152954 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 3.73       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 65        |\n",
      "|    iterations         | 9300      |\n",
      "|    time_elapsed       | 708       |\n",
      "|    total_timesteps    | 46500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.2     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9299      |\n",
      "|    policy_loss        | 40.7      |\n",
      "|    reward             | 2.5936747 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 13.3      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 65         |\n",
      "|    iterations         | 9400       |\n",
      "|    time_elapsed       | 716        |\n",
      "|    total_timesteps    | 47000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.2      |\n",
      "|    explained_variance | -0.0373    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9399       |\n",
      "|    policy_loss        | 51.2       |\n",
      "|    reward             | 0.35695457 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 27         |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 65         |\n",
      "|    iterations         | 9500       |\n",
      "|    time_elapsed       | 722        |\n",
      "|    total_timesteps    | 47500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.2      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9499       |\n",
      "|    policy_loss        | 9.44       |\n",
      "|    reward             | 0.44088984 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 0.788      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 65          |\n",
      "|    iterations         | 9600        |\n",
      "|    time_elapsed       | 730         |\n",
      "|    total_timesteps    | 48000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.2       |\n",
      "|    explained_variance | -0.00835    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 9599        |\n",
      "|    policy_loss        | 4.64        |\n",
      "|    reward             | -0.60300434 |\n",
      "|    std                | 1.06        |\n",
      "|    value_loss         | 0.41        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 65         |\n",
      "|    iterations         | 9700       |\n",
      "|    time_elapsed       | 739        |\n",
      "|    total_timesteps    | 48500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9699       |\n",
      "|    policy_loss        | 12.4       |\n",
      "|    reward             | -1.3379363 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 1.6        |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 65          |\n",
      "|    iterations         | 9800        |\n",
      "|    time_elapsed       | 747         |\n",
      "|    total_timesteps    | 49000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.2       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 9799        |\n",
      "|    policy_loss        | -79.3       |\n",
      "|    reward             | -0.39100817 |\n",
      "|    std                | 1.06        |\n",
      "|    value_loss         | 44.3        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 65        |\n",
      "|    iterations         | 9900      |\n",
      "|    time_elapsed       | 755       |\n",
      "|    total_timesteps    | 49500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9899      |\n",
      "|    policy_loss        | -0.891    |\n",
      "|    reward             | 1.8927401 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 0.826     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 65        |\n",
      "|    iterations         | 10000     |\n",
      "|    time_elapsed       | 762       |\n",
      "|    total_timesteps    | 50000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9999      |\n",
      "|    policy_loss        | 46.8      |\n",
      "|    reward             | -8.208395 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 15.6      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 65        |\n",
      "|    iterations         | 10100     |\n",
      "|    time_elapsed       | 770       |\n",
      "|    total_timesteps    | 50500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 10099     |\n",
      "|    policy_loss        | -2.48     |\n",
      "|    reward             | 1.0561032 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 0.69      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 65        |\n",
      "|    iterations         | 10200     |\n",
      "|    time_elapsed       | 777       |\n",
      "|    total_timesteps    | 51000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 10199     |\n",
      "|    policy_loss        | 17.9      |\n",
      "|    reward             | 1.0941342 |\n",
      "|    std                | 1.07      |\n",
      "|    value_loss         | 2.79      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 65        |\n",
      "|    iterations         | 10300     |\n",
      "|    time_elapsed       | 784       |\n",
      "|    total_timesteps    | 51500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 10299     |\n",
      "|    policy_loss        | -22.9     |\n",
      "|    reward             | 1.0320883 |\n",
      "|    std                | 1.07      |\n",
      "|    value_loss         | 3.55      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 65         |\n",
      "|    iterations         | 10400      |\n",
      "|    time_elapsed       | 791        |\n",
      "|    total_timesteps    | 52000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.3      |\n",
      "|    explained_variance | 0.0441     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 10399      |\n",
      "|    policy_loss        | 122        |\n",
      "|    reward             | -0.7795608 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 81.6       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 65         |\n",
      "|    iterations         | 10500      |\n",
      "|    time_elapsed       | 798        |\n",
      "|    total_timesteps    | 52500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.4      |\n",
      "|    explained_variance | -0.0204    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 10499      |\n",
      "|    policy_loss        | 45.9       |\n",
      "|    reward             | -4.2213664 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 15.3       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 65        |\n",
      "|    iterations         | 10600     |\n",
      "|    time_elapsed       | 806       |\n",
      "|    total_timesteps    | 53000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.4     |\n",
      "|    explained_variance | 0.0296    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 10599     |\n",
      "|    policy_loss        | -167      |\n",
      "|    reward             | 0.5891526 |\n",
      "|    std                | 1.08      |\n",
      "|    value_loss         | 206       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 65         |\n",
      "|    iterations         | 10700      |\n",
      "|    time_elapsed       | 815        |\n",
      "|    total_timesteps    | 53500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 10699      |\n",
      "|    policy_loss        | 2.22       |\n",
      "|    reward             | 0.17274398 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 0.0372     |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 65          |\n",
      "|    iterations         | 10800       |\n",
      "|    time_elapsed       | 822         |\n",
      "|    total_timesteps    | 54000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.4       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 10799       |\n",
      "|    policy_loss        | 12.2        |\n",
      "|    reward             | -0.33018205 |\n",
      "|    std                | 1.08        |\n",
      "|    value_loss         | 1.11        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 65         |\n",
      "|    iterations         | 10900      |\n",
      "|    time_elapsed       | 828        |\n",
      "|    total_timesteps    | 54500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 10899      |\n",
      "|    policy_loss        | 18         |\n",
      "|    reward             | -1.5705653 |\n",
      "|    std                | 1.08       |\n",
      "|    value_loss         | 4.02       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 65          |\n",
      "|    iterations         | 11000       |\n",
      "|    time_elapsed       | 835         |\n",
      "|    total_timesteps    | 55000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.4       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 10999       |\n",
      "|    policy_loss        | -11.6       |\n",
      "|    reward             | -0.34400082 |\n",
      "|    std                | 1.08        |\n",
      "|    value_loss         | 1.23        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 65         |\n",
      "|    iterations         | 11100      |\n",
      "|    time_elapsed       | 841        |\n",
      "|    total_timesteps    | 55500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.4      |\n",
      "|    explained_variance | 0.00324    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 11099      |\n",
      "|    policy_loss        | 83.9       |\n",
      "|    reward             | -0.3106339 |\n",
      "|    std                | 1.08       |\n",
      "|    value_loss         | 51.6       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 66          |\n",
      "|    iterations         | 11200       |\n",
      "|    time_elapsed       | 847         |\n",
      "|    total_timesteps    | 56000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.4       |\n",
      "|    explained_variance | -0.0696     |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 11199       |\n",
      "|    policy_loss        | -7.3        |\n",
      "|    reward             | -0.05076629 |\n",
      "|    std                | 1.07        |\n",
      "|    value_loss         | 6.28        |\n",
      "---------------------------------------\n",
      "day: 2956, episode: 20\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 5004019.87\n",
      "total_reward: 4004019.87\n",
      "total_cost: 29035.96\n",
      "total_trades: 18157\n",
      "Sharpe: 0.945\n",
      "=================================\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 66         |\n",
      "|    iterations         | 11300      |\n",
      "|    time_elapsed       | 854        |\n",
      "|    total_timesteps    | 56500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.4      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 11299      |\n",
      "|    policy_loss        | 15.6       |\n",
      "|    reward             | 0.38230205 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 1.29       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 66         |\n",
      "|    iterations         | 11400      |\n",
      "|    time_elapsed       | 860        |\n",
      "|    total_timesteps    | 57000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.4      |\n",
      "|    explained_variance | -0.236     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 11399      |\n",
      "|    policy_loss        | 15.2       |\n",
      "|    reward             | 0.96460897 |\n",
      "|    std                | 1.08       |\n",
      "|    value_loss         | 1.37       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 66        |\n",
      "|    iterations         | 11500     |\n",
      "|    time_elapsed       | 867       |\n",
      "|    total_timesteps    | 57500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.4     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 11499     |\n",
      "|    policy_loss        | 23.1      |\n",
      "|    reward             | 2.4709063 |\n",
      "|    std                | 1.08      |\n",
      "|    value_loss         | 7.66      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 66          |\n",
      "|    iterations         | 11600       |\n",
      "|    time_elapsed       | 873         |\n",
      "|    total_timesteps    | 58000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.4       |\n",
      "|    explained_variance | 5.96e-08    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 11599       |\n",
      "|    policy_loss        | -6.09       |\n",
      "|    reward             | -0.79226595 |\n",
      "|    std                | 1.08        |\n",
      "|    value_loss         | 1.81        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 66         |\n",
      "|    iterations         | 11700      |\n",
      "|    time_elapsed       | 879        |\n",
      "|    total_timesteps    | 58500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 11699      |\n",
      "|    policy_loss        | 67.3       |\n",
      "|    reward             | -1.2165793 |\n",
      "|    std                | 1.08       |\n",
      "|    value_loss         | 29.5       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 66        |\n",
      "|    iterations         | 11800     |\n",
      "|    time_elapsed       | 886       |\n",
      "|    total_timesteps    | 59000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.4     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 11799     |\n",
      "|    policy_loss        | 15.8      |\n",
      "|    reward             | 3.4170754 |\n",
      "|    std                | 1.08      |\n",
      "|    value_loss         | 16.7      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 66          |\n",
      "|    iterations         | 11900       |\n",
      "|    time_elapsed       | 892         |\n",
      "|    total_timesteps    | 59500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.4       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 11899       |\n",
      "|    policy_loss        | -16.6       |\n",
      "|    reward             | -0.17885083 |\n",
      "|    std                | 1.08        |\n",
      "|    value_loss         | 1.87        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 66         |\n",
      "|    iterations         | 12000      |\n",
      "|    time_elapsed       | 898        |\n",
      "|    total_timesteps    | 60000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.5      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 11999      |\n",
      "|    policy_loss        | -33.6      |\n",
      "|    reward             | -2.4564085 |\n",
      "|    std                | 1.08       |\n",
      "|    value_loss         | 5.9        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 66        |\n",
      "|    iterations         | 12100     |\n",
      "|    time_elapsed       | 904       |\n",
      "|    total_timesteps    | 60500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.5     |\n",
      "|    explained_variance | -2.38e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 12099     |\n",
      "|    policy_loss        | -2.92     |\n",
      "|    reward             | 0.8713749 |\n",
      "|    std                | 1.09      |\n",
      "|    value_loss         | 0.402     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 66        |\n",
      "|    iterations         | 12200     |\n",
      "|    time_elapsed       | 910       |\n",
      "|    total_timesteps    | 61000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 12199     |\n",
      "|    policy_loss        | -65.8     |\n",
      "|    reward             | 0.8607046 |\n",
      "|    std                | 1.09      |\n",
      "|    value_loss         | 25.2      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 67       |\n",
      "|    iterations         | 12300    |\n",
      "|    time_elapsed       | 917      |\n",
      "|    total_timesteps    | 61500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13.5    |\n",
      "|    explained_variance | -0.111   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 12299    |\n",
      "|    policy_loss        | -67.1    |\n",
      "|    reward             | 2.85007  |\n",
      "|    std                | 1.09     |\n",
      "|    value_loss         | 41.7     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 67       |\n",
      "|    iterations         | 12400    |\n",
      "|    time_elapsed       | 924      |\n",
      "|    total_timesteps    | 62000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13.5    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 12399    |\n",
      "|    policy_loss        | 50.4     |\n",
      "|    reward             | 5.48881  |\n",
      "|    std                | 1.09     |\n",
      "|    value_loss         | 13       |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 67        |\n",
      "|    iterations         | 12500     |\n",
      "|    time_elapsed       | 930       |\n",
      "|    total_timesteps    | 62500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 12499     |\n",
      "|    policy_loss        | -61.4     |\n",
      "|    reward             | 4.3453994 |\n",
      "|    std                | 1.09      |\n",
      "|    value_loss         | 27.1      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 67          |\n",
      "|    iterations         | 12600       |\n",
      "|    time_elapsed       | 936         |\n",
      "|    total_timesteps    | 63000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.5       |\n",
      "|    explained_variance | 1.19e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 12599       |\n",
      "|    policy_loss        | -9          |\n",
      "|    reward             | -0.78805643 |\n",
      "|    std                | 1.09        |\n",
      "|    value_loss         | 0.486       |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 67        |\n",
      "|    iterations         | 12700     |\n",
      "|    time_elapsed       | 942       |\n",
      "|    total_timesteps    | 63500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.5     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 12699     |\n",
      "|    policy_loss        | -67.3     |\n",
      "|    reward             | 1.2237334 |\n",
      "|    std                | 1.09      |\n",
      "|    value_loss         | 27.9      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 67          |\n",
      "|    iterations         | 12800       |\n",
      "|    time_elapsed       | 949         |\n",
      "|    total_timesteps    | 64000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.5       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 12799       |\n",
      "|    policy_loss        | 34.2        |\n",
      "|    reward             | -0.73964494 |\n",
      "|    std                | 1.09        |\n",
      "|    value_loss         | 5.25        |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 67       |\n",
      "|    iterations         | 12900    |\n",
      "|    time_elapsed       | 956      |\n",
      "|    total_timesteps    | 64500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13.6    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 12899    |\n",
      "|    policy_loss        | -29      |\n",
      "|    reward             | 2.222862 |\n",
      "|    std                | 1.1      |\n",
      "|    value_loss         | 6.39     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 67         |\n",
      "|    iterations         | 13000      |\n",
      "|    time_elapsed       | 963        |\n",
      "|    total_timesteps    | 65000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.6      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 12999      |\n",
      "|    policy_loss        | 28         |\n",
      "|    reward             | 0.27297854 |\n",
      "|    std                | 1.1        |\n",
      "|    value_loss         | 8.36       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 67        |\n",
      "|    iterations         | 13100     |\n",
      "|    time_elapsed       | 969       |\n",
      "|    total_timesteps    | 65500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 13099     |\n",
      "|    policy_loss        | -75       |\n",
      "|    reward             | 2.6347895 |\n",
      "|    std                | 1.1       |\n",
      "|    value_loss         | 26.2      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 67         |\n",
      "|    iterations         | 13200      |\n",
      "|    time_elapsed       | 975        |\n",
      "|    total_timesteps    | 66000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.6      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 13199      |\n",
      "|    policy_loss        | -15.8      |\n",
      "|    reward             | -1.9168996 |\n",
      "|    std                | 1.1        |\n",
      "|    value_loss         | 2.2        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 67        |\n",
      "|    iterations         | 13300     |\n",
      "|    time_elapsed       | 981       |\n",
      "|    total_timesteps    | 66500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 13299     |\n",
      "|    policy_loss        | -65.2     |\n",
      "|    reward             | 1.8610797 |\n",
      "|    std                | 1.1       |\n",
      "|    value_loss         | 40.4      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 67          |\n",
      "|    iterations         | 13400       |\n",
      "|    time_elapsed       | 987         |\n",
      "|    total_timesteps    | 67000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.6       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 13399       |\n",
      "|    policy_loss        | -49.8       |\n",
      "|    reward             | -0.46972525 |\n",
      "|    std                | 1.1         |\n",
      "|    value_loss         | 11.5        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 67         |\n",
      "|    iterations         | 13500      |\n",
      "|    time_elapsed       | 993        |\n",
      "|    total_timesteps    | 67500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.6      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 13499      |\n",
      "|    policy_loss        | 1.95       |\n",
      "|    reward             | -1.5921485 |\n",
      "|    std                | 1.1        |\n",
      "|    value_loss         | 2.49       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 68         |\n",
      "|    iterations         | 13600      |\n",
      "|    time_elapsed       | 999        |\n",
      "|    total_timesteps    | 68000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.6      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 13599      |\n",
      "|    policy_loss        | -100       |\n",
      "|    reward             | -1.5143025 |\n",
      "|    std                | 1.1        |\n",
      "|    value_loss         | 45.7       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 68        |\n",
      "|    iterations         | 13700     |\n",
      "|    time_elapsed       | 1006      |\n",
      "|    total_timesteps    | 68500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.6     |\n",
      "|    explained_variance | 0.0754    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 13699     |\n",
      "|    policy_loss        | 13.7      |\n",
      "|    reward             | 1.5979518 |\n",
      "|    std                | 1.1       |\n",
      "|    value_loss         | 6.42      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 68       |\n",
      "|    iterations         | 13800    |\n",
      "|    time_elapsed       | 1012     |\n",
      "|    total_timesteps    | 69000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13.7    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 13799    |\n",
      "|    policy_loss        | -5.92    |\n",
      "|    reward             | 2.371595 |\n",
      "|    std                | 1.11     |\n",
      "|    value_loss         | 0.81     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 68         |\n",
      "|    iterations         | 13900      |\n",
      "|    time_elapsed       | 1018       |\n",
      "|    total_timesteps    | 69500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.6      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 13899      |\n",
      "|    policy_loss        | 3.23       |\n",
      "|    reward             | -2.2003179 |\n",
      "|    std                | 1.1        |\n",
      "|    value_loss         | 2.26       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 68         |\n",
      "|    iterations         | 14000      |\n",
      "|    time_elapsed       | 1024       |\n",
      "|    total_timesteps    | 70000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 13999      |\n",
      "|    policy_loss        | -6.88      |\n",
      "|    reward             | 0.38442037 |\n",
      "|    std                | 1.11       |\n",
      "|    value_loss         | 0.873      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 68          |\n",
      "|    iterations         | 14100       |\n",
      "|    time_elapsed       | 1030        |\n",
      "|    total_timesteps    | 70500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.7       |\n",
      "|    explained_variance | 0.226       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 14099       |\n",
      "|    policy_loss        | 16.1        |\n",
      "|    reward             | 0.011965858 |\n",
      "|    std                | 1.11        |\n",
      "|    value_loss         | 2.36        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 68          |\n",
      "|    iterations         | 14200       |\n",
      "|    time_elapsed       | 1036        |\n",
      "|    total_timesteps    | 71000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.7       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 14199       |\n",
      "|    policy_loss        | 4.62        |\n",
      "|    reward             | 0.019015789 |\n",
      "|    std                | 1.11        |\n",
      "|    value_loss         | 0.116       |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 68        |\n",
      "|    iterations         | 14300     |\n",
      "|    time_elapsed       | 1042      |\n",
      "|    total_timesteps    | 71500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.7     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 14299     |\n",
      "|    policy_loss        | -2.7      |\n",
      "|    reward             | 0.6784714 |\n",
      "|    std                | 1.11      |\n",
      "|    value_loss         | 0.177     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 68       |\n",
      "|    iterations         | 14400    |\n",
      "|    time_elapsed       | 1049     |\n",
      "|    total_timesteps    | 72000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13.7    |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 14399    |\n",
      "|    policy_loss        | -50.6    |\n",
      "|    reward             | 0.996761 |\n",
      "|    std                | 1.12     |\n",
      "|    value_loss         | 12.4     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 68         |\n",
      "|    iterations         | 14500      |\n",
      "|    time_elapsed       | 1055       |\n",
      "|    total_timesteps    | 72500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.8      |\n",
      "|    explained_variance | 0.00171    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 14499      |\n",
      "|    policy_loss        | -29.4      |\n",
      "|    reward             | -1.1169418 |\n",
      "|    std                | 1.12       |\n",
      "|    value_loss         | 6.74       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 68         |\n",
      "|    iterations         | 14600      |\n",
      "|    time_elapsed       | 1061       |\n",
      "|    total_timesteps    | 73000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 14599      |\n",
      "|    policy_loss        | 34         |\n",
      "|    reward             | -4.2212644 |\n",
      "|    std                | 1.12       |\n",
      "|    value_loss         | 10         |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 68         |\n",
      "|    iterations         | 14700      |\n",
      "|    time_elapsed       | 1067       |\n",
      "|    total_timesteps    | 73500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 14699      |\n",
      "|    policy_loss        | 77         |\n",
      "|    reward             | -6.0870724 |\n",
      "|    std                | 1.12       |\n",
      "|    value_loss         | 29         |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 68          |\n",
      "|    iterations         | 14800       |\n",
      "|    time_elapsed       | 1073        |\n",
      "|    total_timesteps    | 74000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.8       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 14799       |\n",
      "|    policy_loss        | 5.4         |\n",
      "|    reward             | -0.01733623 |\n",
      "|    std                | 1.12        |\n",
      "|    value_loss         | 0.229       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 68         |\n",
      "|    iterations         | 14900      |\n",
      "|    time_elapsed       | 1079       |\n",
      "|    total_timesteps    | 74500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.8      |\n",
      "|    explained_variance | 0.149      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 14899      |\n",
      "|    policy_loss        | -21.9      |\n",
      "|    reward             | -0.5048211 |\n",
      "|    std                | 1.12       |\n",
      "|    value_loss         | 3.72       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 69         |\n",
      "|    iterations         | 15000      |\n",
      "|    time_elapsed       | 1085       |\n",
      "|    total_timesteps    | 75000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 14999      |\n",
      "|    policy_loss        | -29.5      |\n",
      "|    reward             | -1.6672288 |\n",
      "|    std                | 1.13       |\n",
      "|    value_loss         | 7.55       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 69         |\n",
      "|    iterations         | 15100      |\n",
      "|    time_elapsed       | 1092       |\n",
      "|    total_timesteps    | 75500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 15099      |\n",
      "|    policy_loss        | 24         |\n",
      "|    reward             | -3.0655627 |\n",
      "|    std                | 1.13       |\n",
      "|    value_loss         | 4.63       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 69        |\n",
      "|    iterations         | 15200     |\n",
      "|    time_elapsed       | 1098      |\n",
      "|    total_timesteps    | 76000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.9     |\n",
      "|    explained_variance | 0.00235   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15199     |\n",
      "|    policy_loss        | -23.6     |\n",
      "|    reward             | 3.2430885 |\n",
      "|    std                | 1.13      |\n",
      "|    value_loss         | 60.2      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 69         |\n",
      "|    iterations         | 15300      |\n",
      "|    time_elapsed       | 1104       |\n",
      "|    total_timesteps    | 76500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.8      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 15299      |\n",
      "|    policy_loss        | -373       |\n",
      "|    reward             | -17.839767 |\n",
      "|    std                | 1.13       |\n",
      "|    value_loss         | 1.08e+03   |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 69          |\n",
      "|    iterations         | 15400       |\n",
      "|    time_elapsed       | 1110        |\n",
      "|    total_timesteps    | 77000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.8       |\n",
      "|    explained_variance | 0.198       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 15399       |\n",
      "|    policy_loss        | 28.1        |\n",
      "|    reward             | -0.43813667 |\n",
      "|    std                | 1.13        |\n",
      "|    value_loss         | 6.38        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 69        |\n",
      "|    iterations         | 15500     |\n",
      "|    time_elapsed       | 1116      |\n",
      "|    total_timesteps    | 77500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.8     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15499     |\n",
      "|    policy_loss        | 29.9      |\n",
      "|    reward             | 1.0698664 |\n",
      "|    std                | 1.13      |\n",
      "|    value_loss         | 6.85      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 69       |\n",
      "|    iterations         | 15600    |\n",
      "|    time_elapsed       | 1123     |\n",
      "|    total_timesteps    | 78000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13.8    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 15599    |\n",
      "|    policy_loss        | 16.6     |\n",
      "|    reward             | 0.574859 |\n",
      "|    std                | 1.13     |\n",
      "|    value_loss         | 1.56     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 69        |\n",
      "|    iterations         | 15700     |\n",
      "|    time_elapsed       | 1129      |\n",
      "|    total_timesteps    | 78500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.8     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15699     |\n",
      "|    policy_loss        | 7.64      |\n",
      "|    reward             | 0.6568412 |\n",
      "|    std                | 1.13      |\n",
      "|    value_loss         | 1.04      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 69        |\n",
      "|    iterations         | 15800     |\n",
      "|    time_elapsed       | 1136      |\n",
      "|    total_timesteps    | 79000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.8     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15799     |\n",
      "|    policy_loss        | -3.08     |\n",
      "|    reward             | 1.9316231 |\n",
      "|    std                | 1.12      |\n",
      "|    value_loss         | 1.06      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 69        |\n",
      "|    iterations         | 15900     |\n",
      "|    time_elapsed       | 1142      |\n",
      "|    total_timesteps    | 79500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.8     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15899     |\n",
      "|    policy_loss        | 49.6      |\n",
      "|    reward             | 4.1108093 |\n",
      "|    std                | 1.12      |\n",
      "|    value_loss         | 54.1      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 69         |\n",
      "|    iterations         | 16000      |\n",
      "|    time_elapsed       | 1148       |\n",
      "|    total_timesteps    | 80000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.8      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 15999      |\n",
      "|    policy_loss        | -7.68      |\n",
      "|    reward             | -1.6452436 |\n",
      "|    std                | 1.13       |\n",
      "|    value_loss         | 3.17       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 69        |\n",
      "|    iterations         | 16100     |\n",
      "|    time_elapsed       | 1154      |\n",
      "|    total_timesteps    | 80500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.8     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16099     |\n",
      "|    policy_loss        | 7.35      |\n",
      "|    reward             | 0.8612557 |\n",
      "|    std                | 1.13      |\n",
      "|    value_loss         | 1.31      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 69        |\n",
      "|    iterations         | 16200     |\n",
      "|    time_elapsed       | 1161      |\n",
      "|    total_timesteps    | 81000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.8     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16199     |\n",
      "|    policy_loss        | -40.9     |\n",
      "|    reward             | 1.2775742 |\n",
      "|    std                | 1.13      |\n",
      "|    value_loss         | 9.24      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 69        |\n",
      "|    iterations         | 16300     |\n",
      "|    time_elapsed       | 1167      |\n",
      "|    total_timesteps    | 81500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.9     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16299     |\n",
      "|    policy_loss        | -16.5     |\n",
      "|    reward             | 0.3855038 |\n",
      "|    std                | 1.13      |\n",
      "|    value_loss         | 1.72      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 69        |\n",
      "|    iterations         | 16400     |\n",
      "|    time_elapsed       | 1173      |\n",
      "|    total_timesteps    | 82000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.9     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16399     |\n",
      "|    policy_loss        | -25.9     |\n",
      "|    reward             | 1.2949125 |\n",
      "|    std                | 1.13      |\n",
      "|    value_loss         | 7.98      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 69          |\n",
      "|    iterations         | 16500       |\n",
      "|    time_elapsed       | 1180        |\n",
      "|    total_timesteps    | 82500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.9       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 16499       |\n",
      "|    policy_loss        | -30.1       |\n",
      "|    reward             | -0.85843134 |\n",
      "|    std                | 1.14        |\n",
      "|    value_loss         | 14.3        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 69        |\n",
      "|    iterations         | 16600     |\n",
      "|    time_elapsed       | 1186      |\n",
      "|    total_timesteps    | 83000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.9     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16599     |\n",
      "|    policy_loss        | 4.91      |\n",
      "|    reward             | 0.3271621 |\n",
      "|    std                | 1.13      |\n",
      "|    value_loss         | 0.227     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 69         |\n",
      "|    iterations         | 16700      |\n",
      "|    time_elapsed       | 1193       |\n",
      "|    total_timesteps    | 83500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.9      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 16699      |\n",
      "|    policy_loss        | -43.4      |\n",
      "|    reward             | 0.19904795 |\n",
      "|    std                | 1.13       |\n",
      "|    value_loss         | 8.86       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 68         |\n",
      "|    iterations         | 16800      |\n",
      "|    time_elapsed       | 1222       |\n",
      "|    total_timesteps    | 84000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.9      |\n",
      "|    explained_variance | 0.0672     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 16799      |\n",
      "|    policy_loss        | 18.2       |\n",
      "|    reward             | -1.5040816 |\n",
      "|    std                | 1.14       |\n",
      "|    value_loss         | 3.98       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 68        |\n",
      "|    iterations         | 16900     |\n",
      "|    time_elapsed       | 1230      |\n",
      "|    total_timesteps    | 84500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.9     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16899     |\n",
      "|    policy_loss        | -4.54     |\n",
      "|    reward             | 0.8992968 |\n",
      "|    std                | 1.14      |\n",
      "|    value_loss         | 1.27      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 68         |\n",
      "|    iterations         | 17000      |\n",
      "|    time_elapsed       | 1236       |\n",
      "|    total_timesteps    | 85000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.9      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 16999      |\n",
      "|    policy_loss        | -6.3       |\n",
      "|    reward             | -3.7607307 |\n",
      "|    std                | 1.14       |\n",
      "|    value_loss         | 1.42       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 68        |\n",
      "|    iterations         | 17100     |\n",
      "|    time_elapsed       | 1243      |\n",
      "|    total_timesteps    | 85500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.9     |\n",
      "|    explained_variance | 0.0353    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17099     |\n",
      "|    policy_loss        | -163      |\n",
      "|    reward             | 3.9891093 |\n",
      "|    std                | 1.13      |\n",
      "|    value_loss         | 154       |\n",
      "-------------------------------------\n",
      "day: 2956, episode: 30\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 5700448.15\n",
      "total_reward: 4700448.15\n",
      "total_cost: 4997.54\n",
      "total_trades: 21975\n",
      "Sharpe: 0.947\n",
      "=================================\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 68          |\n",
      "|    iterations         | 17200       |\n",
      "|    time_elapsed       | 1249        |\n",
      "|    total_timesteps    | 86000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.9       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 17199       |\n",
      "|    policy_loss        | -2.65       |\n",
      "|    reward             | 0.001707804 |\n",
      "|    std                | 1.13        |\n",
      "|    value_loss         | 0.0906      |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 68         |\n",
      "|    iterations         | 17300      |\n",
      "|    time_elapsed       | 1256       |\n",
      "|    total_timesteps    | 86500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.9      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 17299      |\n",
      "|    policy_loss        | -8.11      |\n",
      "|    reward             | 0.39469087 |\n",
      "|    std                | 1.14       |\n",
      "|    value_loss         | 0.657      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 68         |\n",
      "|    iterations         | 17400      |\n",
      "|    time_elapsed       | 1262       |\n",
      "|    total_timesteps    | 87000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.9      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 17399      |\n",
      "|    policy_loss        | -15        |\n",
      "|    reward             | -1.9682088 |\n",
      "|    std                | 1.14       |\n",
      "|    value_loss         | 1.05       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 68        |\n",
      "|    iterations         | 17500     |\n",
      "|    time_elapsed       | 1268      |\n",
      "|    total_timesteps    | 87500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.9     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17499     |\n",
      "|    policy_loss        | -34.4     |\n",
      "|    reward             | 2.2195444 |\n",
      "|    std                | 1.14      |\n",
      "|    value_loss         | 8.56      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 69          |\n",
      "|    iterations         | 17600       |\n",
      "|    time_elapsed       | 1275        |\n",
      "|    total_timesteps    | 88000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.8       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 17599       |\n",
      "|    policy_loss        | 156         |\n",
      "|    reward             | -0.83662486 |\n",
      "|    std                | 1.13        |\n",
      "|    value_loss         | 130         |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 69        |\n",
      "|    iterations         | 17700     |\n",
      "|    time_elapsed       | 1281      |\n",
      "|    total_timesteps    | 88500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.8     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17699     |\n",
      "|    policy_loss        | -64.8     |\n",
      "|    reward             | 3.6588757 |\n",
      "|    std                | 1.13      |\n",
      "|    value_loss         | 34        |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 69        |\n",
      "|    iterations         | 17800     |\n",
      "|    time_elapsed       | 1287      |\n",
      "|    total_timesteps    | 89000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.8     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17799     |\n",
      "|    policy_loss        | 6.42      |\n",
      "|    reward             | 0.8682316 |\n",
      "|    std                | 1.13      |\n",
      "|    value_loss         | 0.394     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 69         |\n",
      "|    iterations         | 17900      |\n",
      "|    time_elapsed       | 1294       |\n",
      "|    total_timesteps    | 89500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 17899      |\n",
      "|    policy_loss        | -7.1       |\n",
      "|    reward             | -2.5602486 |\n",
      "|    std                | 1.13       |\n",
      "|    value_loss         | 0.277      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 69          |\n",
      "|    iterations         | 18000       |\n",
      "|    time_elapsed       | 1300        |\n",
      "|    total_timesteps    | 90000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.8       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 17999       |\n",
      "|    policy_loss        | 7.76        |\n",
      "|    reward             | 0.091046266 |\n",
      "|    std                | 1.13        |\n",
      "|    value_loss         | 1.23        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 69        |\n",
      "|    iterations         | 18100     |\n",
      "|    time_elapsed       | 1306      |\n",
      "|    total_timesteps    | 90500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.8     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18099     |\n",
      "|    policy_loss        | 11.6      |\n",
      "|    reward             | 1.386781  |\n",
      "|    std                | 1.13      |\n",
      "|    value_loss         | 1.15      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 69         |\n",
      "|    iterations         | 18200      |\n",
      "|    time_elapsed       | 1313       |\n",
      "|    total_timesteps    | 91000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.9      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 18199      |\n",
      "|    policy_loss        | 61.5       |\n",
      "|    reward             | 0.44182482 |\n",
      "|    std                | 1.13       |\n",
      "|    value_loss         | 25.6       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 69         |\n",
      "|    iterations         | 18300      |\n",
      "|    time_elapsed       | 1320       |\n",
      "|    total_timesteps    | 91500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.9      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 18299      |\n",
      "|    policy_loss        | -138       |\n",
      "|    reward             | -1.8975279 |\n",
      "|    std                | 1.13       |\n",
      "|    value_loss         | 88.9       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 69        |\n",
      "|    iterations         | 18400     |\n",
      "|    time_elapsed       | 1327      |\n",
      "|    total_timesteps    | 92000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.8     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18399     |\n",
      "|    policy_loss        | 20.4      |\n",
      "|    reward             | 0.4841894 |\n",
      "|    std                | 1.13      |\n",
      "|    value_loss         | 2.8       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 69         |\n",
      "|    iterations         | 18500      |\n",
      "|    time_elapsed       | 1333       |\n",
      "|    total_timesteps    | 92500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 18499      |\n",
      "|    policy_loss        | -11.9      |\n",
      "|    reward             | -0.0543231 |\n",
      "|    std                | 1.13       |\n",
      "|    value_loss         | 3.38       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 69        |\n",
      "|    iterations         | 18600     |\n",
      "|    time_elapsed       | 1340      |\n",
      "|    total_timesteps    | 93000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.8     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18599     |\n",
      "|    policy_loss        | 8.88      |\n",
      "|    reward             | 0.8907221 |\n",
      "|    std                | 1.13      |\n",
      "|    value_loss         | 1.38      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 69        |\n",
      "|    iterations         | 18700     |\n",
      "|    time_elapsed       | 1346      |\n",
      "|    total_timesteps    | 93500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.8     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18699     |\n",
      "|    policy_loss        | -8.45     |\n",
      "|    reward             | 1.3864874 |\n",
      "|    std                | 1.12      |\n",
      "|    value_loss         | 0.753     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 69         |\n",
      "|    iterations         | 18800      |\n",
      "|    time_elapsed       | 1353       |\n",
      "|    total_timesteps    | 94000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 18799      |\n",
      "|    policy_loss        | 5.3        |\n",
      "|    reward             | -3.1542196 |\n",
      "|    std                | 1.12       |\n",
      "|    value_loss         | 4.27       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 69         |\n",
      "|    iterations         | 18900      |\n",
      "|    time_elapsed       | 1359       |\n",
      "|    total_timesteps    | 94500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 18899      |\n",
      "|    policy_loss        | 17         |\n",
      "|    reward             | 0.40741864 |\n",
      "|    std                | 1.12       |\n",
      "|    value_loss         | 29.6       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 69        |\n",
      "|    iterations         | 19000     |\n",
      "|    time_elapsed       | 1365      |\n",
      "|    total_timesteps    | 95000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.8     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18999     |\n",
      "|    policy_loss        | -4.34     |\n",
      "|    reward             | 0.9353864 |\n",
      "|    std                | 1.12      |\n",
      "|    value_loss         | 0.518     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 69        |\n",
      "|    iterations         | 19100     |\n",
      "|    time_elapsed       | 1372      |\n",
      "|    total_timesteps    | 95500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.8     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19099     |\n",
      "|    policy_loss        | -76.2     |\n",
      "|    reward             | 0.6453306 |\n",
      "|    std                | 1.12      |\n",
      "|    value_loss         | 33.1      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 69         |\n",
      "|    iterations         | 19200      |\n",
      "|    time_elapsed       | 1378       |\n",
      "|    total_timesteps    | 96000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 19199      |\n",
      "|    policy_loss        | -0.296     |\n",
      "|    reward             | 0.43444276 |\n",
      "|    std                | 1.13       |\n",
      "|    value_loss         | 2.09       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 69         |\n",
      "|    iterations         | 19300      |\n",
      "|    time_elapsed       | 1384       |\n",
      "|    total_timesteps    | 96500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.9      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 19299      |\n",
      "|    policy_loss        | -3.91      |\n",
      "|    reward             | 0.32945034 |\n",
      "|    std                | 1.13       |\n",
      "|    value_loss         | 0.33       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 69          |\n",
      "|    iterations         | 19400       |\n",
      "|    time_elapsed       | 1391        |\n",
      "|    total_timesteps    | 97000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.9       |\n",
      "|    explained_variance | 5.96e-08    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 19399       |\n",
      "|    policy_loss        | 94.3        |\n",
      "|    reward             | 0.104024015 |\n",
      "|    std                | 1.13        |\n",
      "|    value_loss         | 56.8        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 69         |\n",
      "|    iterations         | 19500      |\n",
      "|    time_elapsed       | 1397       |\n",
      "|    total_timesteps    | 97500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.9      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 19499      |\n",
      "|    policy_loss        | -92.9      |\n",
      "|    reward             | -1.0202163 |\n",
      "|    std                | 1.13       |\n",
      "|    value_loss         | 43.7       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 69        |\n",
      "|    iterations         | 19600     |\n",
      "|    time_elapsed       | 1404      |\n",
      "|    total_timesteps    | 98000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.9     |\n",
      "|    explained_variance | 1.79e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19599     |\n",
      "|    policy_loss        | 26.5      |\n",
      "|    reward             | 0.5823236 |\n",
      "|    std                | 1.13      |\n",
      "|    value_loss         | 9.65      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 69        |\n",
      "|    iterations         | 19700     |\n",
      "|    time_elapsed       | 1410      |\n",
      "|    total_timesteps    | 98500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.9     |\n",
      "|    explained_variance | -0.0294   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19699     |\n",
      "|    policy_loss        | -25.2     |\n",
      "|    reward             | 0.5278645 |\n",
      "|    std                | 1.13      |\n",
      "|    value_loss         | 5.72      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 69        |\n",
      "|    iterations         | 19800     |\n",
      "|    time_elapsed       | 1416      |\n",
      "|    total_timesteps    | 99000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.9     |\n",
      "|    explained_variance | 0.00139   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19799     |\n",
      "|    policy_loss        | 21.9      |\n",
      "|    reward             | -9.885806 |\n",
      "|    std                | 1.14      |\n",
      "|    value_loss         | 3.57      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 69         |\n",
      "|    iterations         | 19900      |\n",
      "|    time_elapsed       | 1423       |\n",
      "|    total_timesteps    | 99500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.9      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 19899      |\n",
      "|    policy_loss        | -64.3      |\n",
      "|    reward             | -3.8734045 |\n",
      "|    std                | 1.14       |\n",
      "|    value_loss         | 21.7       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 69         |\n",
      "|    iterations         | 20000      |\n",
      "|    time_elapsed       | 1429       |\n",
      "|    total_timesteps    | 100000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.9      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 19999      |\n",
      "|    policy_loss        | -37.4      |\n",
      "|    reward             | -9.4909315 |\n",
      "|    std                | 1.14       |\n",
      "|    value_loss         | 30.2       |\n",
      "--------------------------------------\n",
      "hit end!\n",
      "==============Get Baseline Stats===========\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Shape of DataFrame:  (400, 8)\n",
      "Annual return         -0.022145\n",
      "Cumulative returns    -0.034921\n",
      "Annual volatility      0.176822\n",
      "Sharpe ratio          -0.038668\n",
      "Calmar ratio          -0.100930\n",
      "Stability              0.215672\n",
      "Max drawdown          -0.219408\n",
      "Omega ratio            0.993578\n",
      "Sortino ratio         -0.053987\n",
      "Skew                        NaN\n",
      "Kurtosis                    NaN\n",
      "Tail ratio             1.003436\n",
      "Daily value at risk   -0.022305\n",
      "dtype: float64\n",
      "result:                       a2c           dji\n",
      "date                                  \n",
      "2021-10-01  1.000000e+06  1.000000e+06\n",
      "2021-10-04  9.994950e+05  9.905746e+05\n",
      "2021-10-05  1.000391e+06  9.996566e+05\n",
      "2021-10-06  1.000824e+06  1.002637e+06\n",
      "2021-10-07  1.002381e+06  1.012483e+06\n",
      "...                  ...           ...\n",
      "2023-04-27  9.951443e+05  9.854252e+05\n",
      "2023-04-28  1.004238e+06  9.933491e+05\n",
      "2023-05-01  1.007107e+06  9.919956e+05\n",
      "2023-05-02  1.000040e+06  9.812993e+05\n",
      "2023-05-03  9.953403e+05  9.734251e+05\n",
      "\n",
      "[399 rows x 2 columns]\n",
      "A2C로 얻은 투자 수익률>> -0.47 %\n",
      "{'n_steps': 5, 'ent_coef': 0.01, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to results/a2c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\se99a\\anaconda3\\lib\\site-packages\\pyfolio\\timeseries.py:724: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  stats = pd.Series()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 74         |\n",
      "|    iterations         | 100        |\n",
      "|    time_elapsed       | 6          |\n",
      "|    total_timesteps    | 500        |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -12.9      |\n",
      "|    explained_variance | 0.0769     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 99         |\n",
      "|    policy_loss        | -21.2      |\n",
      "|    reward             | 0.11584427 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 3.75       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 13        |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13       |\n",
      "|    explained_variance | -0.0208   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | -1.29     |\n",
      "|    reward             | 0.5939163 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 1.16      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 300       |\n",
      "|    time_elapsed       | 19        |\n",
      "|    total_timesteps    | 1500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -12.9     |\n",
      "|    explained_variance | 0.445     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 299       |\n",
      "|    policy_loss        | -11.9     |\n",
      "|    reward             | -1.814556 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 1.44      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 400       |\n",
      "|    time_elapsed       | 26        |\n",
      "|    total_timesteps    | 2000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13       |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 399       |\n",
      "|    policy_loss        | -14.3     |\n",
      "|    reward             | -1.941031 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 2.52      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 77          |\n",
      "|    iterations         | 500         |\n",
      "|    time_elapsed       | 32          |\n",
      "|    total_timesteps    | 2500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -12.9       |\n",
      "|    explained_variance | -0.207      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 499         |\n",
      "|    policy_loss        | -21.2       |\n",
      "|    reward             | -0.58324695 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 2.87        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 77          |\n",
      "|    iterations         | 600         |\n",
      "|    time_elapsed       | 38          |\n",
      "|    total_timesteps    | 3000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -12.9       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 599         |\n",
      "|    policy_loss        | 2.08        |\n",
      "|    reward             | 0.037930485 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 0.0328      |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 77          |\n",
      "|    iterations         | 700         |\n",
      "|    time_elapsed       | 44          |\n",
      "|    total_timesteps    | 3500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -12.9       |\n",
      "|    explained_variance | 0.025       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 699         |\n",
      "|    policy_loss        | 11.6        |\n",
      "|    reward             | -0.99594164 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 1.21        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 78        |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 51        |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -12.9     |\n",
      "|    explained_variance | 0.332     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | 7.6       |\n",
      "|    reward             | -0.559373 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 1.15      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 78       |\n",
      "|    iterations         | 900      |\n",
      "|    time_elapsed       | 57       |\n",
      "|    total_timesteps    | 4500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -12.9    |\n",
      "|    explained_variance | -0.0218  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 899      |\n",
      "|    policy_loss        | 14.8     |\n",
      "|    reward             | 2.710953 |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 3.84     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 78        |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 64        |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -12.9     |\n",
      "|    explained_variance | 0.00866   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | -75       |\n",
      "|    reward             | 3.5509784 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 67        |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 77        |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 70        |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -12.9     |\n",
      "|    explained_variance | 0.0494    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | 39.5      |\n",
      "|    reward             | 0.8757587 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 10.2      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 77         |\n",
      "|    iterations         | 1200       |\n",
      "|    time_elapsed       | 76         |\n",
      "|    total_timesteps    | 6000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -12.9      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1199       |\n",
      "|    policy_loss        | -9.38      |\n",
      "|    reward             | -0.7045185 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 0.834      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 85        |\n",
      "|    total_timesteps    | 6500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -12.9     |\n",
      "|    explained_variance | 0.221     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1299      |\n",
      "|    policy_loss        | -14.5     |\n",
      "|    reward             | 0.8784621 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 2.15      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 74        |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 93        |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -12.9     |\n",
      "|    explained_variance | -0.0806   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | 33        |\n",
      "|    reward             | 1.0956681 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 9.55      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 73        |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 101       |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -12.9     |\n",
      "|    explained_variance | -8.77e-05 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | 19.8      |\n",
      "|    reward             | 1.8219119 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 7.4       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 73         |\n",
      "|    iterations         | 1600       |\n",
      "|    time_elapsed       | 108        |\n",
      "|    total_timesteps    | 8000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -12.9      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1599       |\n",
      "|    policy_loss        | 37.2       |\n",
      "|    reward             | 0.97940654 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 12.1       |\n",
      "--------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 73            |\n",
      "|    iterations         | 1700          |\n",
      "|    time_elapsed       | 114           |\n",
      "|    total_timesteps    | 8500          |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -13           |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 1699          |\n",
      "|    policy_loss        | 110           |\n",
      "|    reward             | -0.0031410838 |\n",
      "|    std                | 1.02          |\n",
      "|    value_loss         | 72.6          |\n",
      "-----------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 74        |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 121       |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -12.9     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | -23       |\n",
      "|    reward             | 0.3577144 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 3.11      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 74         |\n",
      "|    iterations         | 1900       |\n",
      "|    time_elapsed       | 127        |\n",
      "|    total_timesteps    | 9500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13        |\n",
      "|    explained_variance | -0.048     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1899       |\n",
      "|    policy_loss        | -52.8      |\n",
      "|    reward             | 0.17173584 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 20.8       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 74          |\n",
      "|    iterations         | 2000        |\n",
      "|    time_elapsed       | 134         |\n",
      "|    total_timesteps    | 10000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13         |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1999        |\n",
      "|    policy_loss        | 37          |\n",
      "|    reward             | 0.042655386 |\n",
      "|    std                | 1.03        |\n",
      "|    value_loss         | 7.18        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 74        |\n",
      "|    iterations         | 2100      |\n",
      "|    time_elapsed       | 140       |\n",
      "|    total_timesteps    | 10500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13       |\n",
      "|    explained_variance | -0.00398  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2099      |\n",
      "|    policy_loss        | -26.6     |\n",
      "|    reward             | 2.5471075 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 5.97      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 74          |\n",
      "|    iterations         | 2200        |\n",
      "|    time_elapsed       | 147         |\n",
      "|    total_timesteps    | 11000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13         |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 2199        |\n",
      "|    policy_loss        | -1.06       |\n",
      "|    reward             | -0.44333112 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 2.11        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 74        |\n",
      "|    iterations         | 2300      |\n",
      "|    time_elapsed       | 153       |\n",
      "|    total_timesteps    | 11500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13       |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2299      |\n",
      "|    policy_loss        | 155       |\n",
      "|    reward             | 1.0572547 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 220       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 74         |\n",
      "|    iterations         | 2400       |\n",
      "|    time_elapsed       | 160        |\n",
      "|    total_timesteps    | 12000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13        |\n",
      "|    explained_variance | 0.0688     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2399       |\n",
      "|    policy_loss        | 29.1       |\n",
      "|    reward             | 0.35621795 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 5.93       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 74        |\n",
      "|    iterations         | 2500      |\n",
      "|    time_elapsed       | 167       |\n",
      "|    total_timesteps    | 12500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13       |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2499      |\n",
      "|    policy_loss        | -6.12     |\n",
      "|    reward             | 0.8730047 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 0.616     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 74        |\n",
      "|    iterations         | 2600      |\n",
      "|    time_elapsed       | 173       |\n",
      "|    total_timesteps    | 13000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13       |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2599      |\n",
      "|    policy_loss        | 3.15      |\n",
      "|    reward             | 0.7021101 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 1.14      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 2700       |\n",
      "|    time_elapsed       | 179        |\n",
      "|    total_timesteps    | 13500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13        |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2699       |\n",
      "|    policy_loss        | 12.3       |\n",
      "|    reward             | -0.6091718 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 1.76       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 75        |\n",
      "|    iterations         | 2800      |\n",
      "|    time_elapsed       | 186       |\n",
      "|    total_timesteps    | 14000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13       |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2799      |\n",
      "|    policy_loss        | -39.2     |\n",
      "|    reward             | 1.3308241 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 11.1      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 75        |\n",
      "|    iterations         | 2900      |\n",
      "|    time_elapsed       | 193       |\n",
      "|    total_timesteps    | 14500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.1     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2899      |\n",
      "|    policy_loss        | 48.2      |\n",
      "|    reward             | 0.547382  |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 16.9      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 74         |\n",
      "|    iterations         | 3000       |\n",
      "|    time_elapsed       | 200        |\n",
      "|    total_timesteps    | 15000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2999       |\n",
      "|    policy_loss        | 6.43       |\n",
      "|    reward             | -0.7483093 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 0.293      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 74         |\n",
      "|    iterations         | 3100       |\n",
      "|    time_elapsed       | 206        |\n",
      "|    total_timesteps    | 15500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.1      |\n",
      "|    explained_variance | -0.0289    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3099       |\n",
      "|    policy_loss        | -36.2      |\n",
      "|    reward             | 0.42747566 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 8.7        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 74        |\n",
      "|    iterations         | 3200      |\n",
      "|    time_elapsed       | 213       |\n",
      "|    total_timesteps    | 16000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.1     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3199      |\n",
      "|    policy_loss        | 53.2      |\n",
      "|    reward             | 0.7106738 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 24.8      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 3300       |\n",
      "|    time_elapsed       | 219        |\n",
      "|    total_timesteps    | 16500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3299       |\n",
      "|    policy_loss        | -8.77      |\n",
      "|    reward             | -1.6970884 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 1.65       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 75        |\n",
      "|    iterations         | 3400      |\n",
      "|    time_elapsed       | 226       |\n",
      "|    total_timesteps    | 17000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.1     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3399      |\n",
      "|    policy_loss        | -101      |\n",
      "|    reward             | -1.987412 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 104       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 75        |\n",
      "|    iterations         | 3500      |\n",
      "|    time_elapsed       | 232       |\n",
      "|    total_timesteps    | 17500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3499      |\n",
      "|    policy_loss        | 82        |\n",
      "|    reward             | 1.5333307 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 50.9      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 75        |\n",
      "|    iterations         | 3600      |\n",
      "|    time_elapsed       | 238       |\n",
      "|    total_timesteps    | 18000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.2     |\n",
      "|    explained_variance | -0.383    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3599      |\n",
      "|    policy_loss        | 0.535     |\n",
      "|    reward             | 0.3789775 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 0.174     |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 75          |\n",
      "|    iterations         | 3700        |\n",
      "|    time_elapsed       | 245         |\n",
      "|    total_timesteps    | 18500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.1       |\n",
      "|    explained_variance | 0.314       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 3699        |\n",
      "|    policy_loss        | 10.4        |\n",
      "|    reward             | -0.56458926 |\n",
      "|    std                | 1.04        |\n",
      "|    value_loss         | 1.44        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 3800       |\n",
      "|    time_elapsed       | 252        |\n",
      "|    total_timesteps    | 19000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3799       |\n",
      "|    policy_loss        | 35.2       |\n",
      "|    reward             | -1.4517219 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 12.5       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 75        |\n",
      "|    iterations         | 3900      |\n",
      "|    time_elapsed       | 259       |\n",
      "|    total_timesteps    | 19500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3899      |\n",
      "|    policy_loss        | -19.5     |\n",
      "|    reward             | 1.0084647 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 3.5       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 74         |\n",
      "|    iterations         | 4000       |\n",
      "|    time_elapsed       | 267        |\n",
      "|    total_timesteps    | 20000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.1      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3999       |\n",
      "|    policy_loss        | -68.4      |\n",
      "|    reward             | -7.0642786 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 27.4       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 74       |\n",
      "|    iterations         | 4100     |\n",
      "|    time_elapsed       | 274      |\n",
      "|    total_timesteps    | 20500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13.1    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4099     |\n",
      "|    policy_loss        | -28.6    |\n",
      "|    reward             | 3.453231 |\n",
      "|    std                | 1.04     |\n",
      "|    value_loss         | 5.04     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 74         |\n",
      "|    iterations         | 4200       |\n",
      "|    time_elapsed       | 281        |\n",
      "|    total_timesteps    | 21000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.1      |\n",
      "|    explained_variance | 0.0132     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4199       |\n",
      "|    policy_loss        | 4.01       |\n",
      "|    reward             | -1.0663433 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 0.734      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 74          |\n",
      "|    iterations         | 4300        |\n",
      "|    time_elapsed       | 287         |\n",
      "|    total_timesteps    | 21500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.1       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 4299        |\n",
      "|    policy_loss        | 36.3        |\n",
      "|    reward             | -0.57640684 |\n",
      "|    std                | 1.04        |\n",
      "|    value_loss         | 9.27        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 74        |\n",
      "|    iterations         | 4400      |\n",
      "|    time_elapsed       | 293       |\n",
      "|    total_timesteps    | 22000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.1     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4399      |\n",
      "|    policy_loss        | -6.59     |\n",
      "|    reward             | -2.720649 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 0.685     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 74       |\n",
      "|    iterations         | 4500     |\n",
      "|    time_elapsed       | 300      |\n",
      "|    total_timesteps    | 22500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13.1    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4499     |\n",
      "|    policy_loss        | 13.1     |\n",
      "|    reward             | 4.058865 |\n",
      "|    std                | 1.04     |\n",
      "|    value_loss         | 1.54     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 74         |\n",
      "|    iterations         | 4600       |\n",
      "|    time_elapsed       | 307        |\n",
      "|    total_timesteps    | 23000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4599       |\n",
      "|    policy_loss        | 61         |\n",
      "|    reward             | 0.08438226 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 24.9       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 74         |\n",
      "|    iterations         | 4700       |\n",
      "|    time_elapsed       | 313        |\n",
      "|    total_timesteps    | 23500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.2      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4699       |\n",
      "|    policy_loss        | -55.6      |\n",
      "|    reward             | -1.4945296 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 29.8       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 74          |\n",
      "|    iterations         | 4800        |\n",
      "|    time_elapsed       | 320         |\n",
      "|    total_timesteps    | 24000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.2       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 4799        |\n",
      "|    policy_loss        | -8.63       |\n",
      "|    reward             | -0.89468646 |\n",
      "|    std                | 1.05        |\n",
      "|    value_loss         | 1.07        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 74         |\n",
      "|    iterations         | 4900       |\n",
      "|    time_elapsed       | 326        |\n",
      "|    total_timesteps    | 24500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4899       |\n",
      "|    policy_loss        | 28.5       |\n",
      "|    reward             | 0.10068633 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 6.04       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 74        |\n",
      "|    iterations         | 5000      |\n",
      "|    time_elapsed       | 333       |\n",
      "|    total_timesteps    | 25000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4999      |\n",
      "|    policy_loss        | -54.5     |\n",
      "|    reward             | 1.2287625 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 18.4      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 74        |\n",
      "|    iterations         | 5100      |\n",
      "|    time_elapsed       | 340       |\n",
      "|    total_timesteps    | 25500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.2     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5099      |\n",
      "|    policy_loss        | 69.1      |\n",
      "|    reward             | 0.4791037 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 24.9      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 74        |\n",
      "|    iterations         | 5200      |\n",
      "|    time_elapsed       | 346       |\n",
      "|    total_timesteps    | 26000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5199      |\n",
      "|    policy_loss        | -5.72     |\n",
      "|    reward             | 0.1286072 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 0.601     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 5300       |\n",
      "|    time_elapsed       | 353        |\n",
      "|    total_timesteps    | 26500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5299       |\n",
      "|    policy_loss        | 54.4       |\n",
      "|    reward             | -4.5227833 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 22.6       |\n",
      "--------------------------------------\n",
      "day: 2956, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 5189383.18\n",
      "total_reward: 4189383.18\n",
      "total_cost: 6180.87\n",
      "total_trades: 15952\n",
      "Sharpe: 0.887\n",
      "=================================\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 5400       |\n",
      "|    time_elapsed       | 359        |\n",
      "|    total_timesteps    | 27000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5399       |\n",
      "|    policy_loss        | -35.4      |\n",
      "|    reward             | -1.1749339 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 8          |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 75          |\n",
      "|    iterations         | 5500        |\n",
      "|    time_elapsed       | 366         |\n",
      "|    total_timesteps    | 27500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.2       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 5499        |\n",
      "|    policy_loss        | 36.3        |\n",
      "|    reward             | -0.14681344 |\n",
      "|    std                | 1.06        |\n",
      "|    value_loss         | 9.98        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 75        |\n",
      "|    iterations         | 5600      |\n",
      "|    time_elapsed       | 372       |\n",
      "|    total_timesteps    | 28000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5599      |\n",
      "|    policy_loss        | -2.1      |\n",
      "|    reward             | 1.1518798 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 6.73      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 5700       |\n",
      "|    time_elapsed       | 379        |\n",
      "|    total_timesteps    | 28500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.3      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5699       |\n",
      "|    policy_loss        | -5.83      |\n",
      "|    reward             | 0.44590604 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 1.67       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 75        |\n",
      "|    iterations         | 5800      |\n",
      "|    time_elapsed       | 386       |\n",
      "|    total_timesteps    | 29000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.2     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5799      |\n",
      "|    policy_loss        | 30.1      |\n",
      "|    reward             | 1.6195419 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 10.4      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 75        |\n",
      "|    iterations         | 5900      |\n",
      "|    time_elapsed       | 393       |\n",
      "|    total_timesteps    | 29500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5899      |\n",
      "|    policy_loss        | -19.9     |\n",
      "|    reward             | 2.4142692 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 4.42      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 6000       |\n",
      "|    time_elapsed       | 399        |\n",
      "|    total_timesteps    | 30000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5999       |\n",
      "|    policy_loss        | -46        |\n",
      "|    reward             | 0.96417725 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 15.1       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 75        |\n",
      "|    iterations         | 6100      |\n",
      "|    time_elapsed       | 406       |\n",
      "|    total_timesteps    | 30500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6099      |\n",
      "|    policy_loss        | 10.4      |\n",
      "|    reward             | 0.5033939 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 1.69      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 6200       |\n",
      "|    time_elapsed       | 412        |\n",
      "|    total_timesteps    | 31000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.3      |\n",
      "|    explained_variance | -0.0592    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6199       |\n",
      "|    policy_loss        | -76.2      |\n",
      "|    reward             | -2.9083984 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 50.8       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 75        |\n",
      "|    iterations         | 6300      |\n",
      "|    time_elapsed       | 419       |\n",
      "|    total_timesteps    | 31500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.3     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6299      |\n",
      "|    policy_loss        | 0.383     |\n",
      "|    reward             | 0.5315494 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 1.73      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 75        |\n",
      "|    iterations         | 6400      |\n",
      "|    time_elapsed       | 425       |\n",
      "|    total_timesteps    | 32000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.3     |\n",
      "|    explained_variance | 0.215     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6399      |\n",
      "|    policy_loss        | 32.3      |\n",
      "|    reward             | 3.8220127 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 13.1      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 75          |\n",
      "|    iterations         | 6500        |\n",
      "|    time_elapsed       | 432         |\n",
      "|    total_timesteps    | 32500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.3       |\n",
      "|    explained_variance | 5.96e-08    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 6499        |\n",
      "|    policy_loss        | 78.6        |\n",
      "|    reward             | -0.10268232 |\n",
      "|    std                | 1.06        |\n",
      "|    value_loss         | 42.2        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 6600       |\n",
      "|    time_elapsed       | 439        |\n",
      "|    total_timesteps    | 33000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.3      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6599       |\n",
      "|    policy_loss        | -37.4      |\n",
      "|    reward             | -2.1771545 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 10.4       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 75        |\n",
      "|    iterations         | 6700      |\n",
      "|    time_elapsed       | 446       |\n",
      "|    total_timesteps    | 33500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6699      |\n",
      "|    policy_loss        | -5.31     |\n",
      "|    reward             | 1.3804752 |\n",
      "|    std                | 1.07      |\n",
      "|    value_loss         | 0.298     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 75        |\n",
      "|    iterations         | 6800      |\n",
      "|    time_elapsed       | 452       |\n",
      "|    total_timesteps    | 34000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6799      |\n",
      "|    policy_loss        | 51.8      |\n",
      "|    reward             | -2.066543 |\n",
      "|    std                | 1.07      |\n",
      "|    value_loss         | 14.1      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 6900       |\n",
      "|    time_elapsed       | 459        |\n",
      "|    total_timesteps    | 34500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6899       |\n",
      "|    policy_loss        | -23.5      |\n",
      "|    reward             | 0.39887977 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 4.79       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 75        |\n",
      "|    iterations         | 7000      |\n",
      "|    time_elapsed       | 465       |\n",
      "|    total_timesteps    | 35000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6999      |\n",
      "|    policy_loss        | 11.7      |\n",
      "|    reward             | 1.5009007 |\n",
      "|    std                | 1.08      |\n",
      "|    value_loss         | 2.05      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 75          |\n",
      "|    iterations         | 7100        |\n",
      "|    time_elapsed       | 472         |\n",
      "|    total_timesteps    | 35500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.4       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 7099        |\n",
      "|    policy_loss        | -3.67       |\n",
      "|    reward             | 0.057044316 |\n",
      "|    std                | 1.08        |\n",
      "|    value_loss         | 0.0808      |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 75          |\n",
      "|    iterations         | 7200        |\n",
      "|    time_elapsed       | 478         |\n",
      "|    total_timesteps    | 36000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.5       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 7199        |\n",
      "|    policy_loss        | 17.6        |\n",
      "|    reward             | -0.64771354 |\n",
      "|    std                | 1.08        |\n",
      "|    value_loss         | 1.35        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 75          |\n",
      "|    iterations         | 7300        |\n",
      "|    time_elapsed       | 485         |\n",
      "|    total_timesteps    | 36500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.4       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 7299        |\n",
      "|    policy_loss        | -3.11       |\n",
      "|    reward             | -0.91528744 |\n",
      "|    std                | 1.08        |\n",
      "|    value_loss         | 0.332       |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 75        |\n",
      "|    iterations         | 7400      |\n",
      "|    time_elapsed       | 491       |\n",
      "|    total_timesteps    | 37000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.4     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7399      |\n",
      "|    policy_loss        | -70.8     |\n",
      "|    reward             | 2.0638108 |\n",
      "|    std                | 1.08      |\n",
      "|    value_loss         | 31        |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 75        |\n",
      "|    iterations         | 7500      |\n",
      "|    time_elapsed       | 498       |\n",
      "|    total_timesteps    | 37500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7499      |\n",
      "|    policy_loss        | 7.63      |\n",
      "|    reward             | 2.1536303 |\n",
      "|    std                | 1.08      |\n",
      "|    value_loss         | 0.524     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 75        |\n",
      "|    iterations         | 7600      |\n",
      "|    time_elapsed       | 504       |\n",
      "|    total_timesteps    | 38000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.5     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7599      |\n",
      "|    policy_loss        | 9.77      |\n",
      "|    reward             | 5.3118234 |\n",
      "|    std                | 1.08      |\n",
      "|    value_loss         | 0.826     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 7700       |\n",
      "|    time_elapsed       | 510        |\n",
      "|    total_timesteps    | 38500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.5      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7699       |\n",
      "|    policy_loss        | 0.804      |\n",
      "|    reward             | 0.03498217 |\n",
      "|    std                | 1.09       |\n",
      "|    value_loss         | 0.0359     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 7800       |\n",
      "|    time_elapsed       | 517        |\n",
      "|    total_timesteps    | 39000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.5      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7799       |\n",
      "|    policy_loss        | 21.9       |\n",
      "|    reward             | -0.7758022 |\n",
      "|    std                | 1.08       |\n",
      "|    value_loss         | 3.32       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 75        |\n",
      "|    iterations         | 7900      |\n",
      "|    time_elapsed       | 523       |\n",
      "|    total_timesteps    | 39500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7899      |\n",
      "|    policy_loss        | -56.1     |\n",
      "|    reward             | 0.8319139 |\n",
      "|    std                | 1.09      |\n",
      "|    value_loss         | 19.2      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 75          |\n",
      "|    iterations         | 8000        |\n",
      "|    time_elapsed       | 530         |\n",
      "|    total_timesteps    | 40000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.5       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 7999        |\n",
      "|    policy_loss        | -42.6       |\n",
      "|    reward             | -0.19838594 |\n",
      "|    std                | 1.09        |\n",
      "|    value_loss         | 7.6         |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 75       |\n",
      "|    iterations         | 8100     |\n",
      "|    time_elapsed       | 536      |\n",
      "|    total_timesteps    | 40500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13.5    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8099     |\n",
      "|    policy_loss        | -95.1    |\n",
      "|    reward             | 5.800824 |\n",
      "|    std                | 1.09     |\n",
      "|    value_loss         | 67.3     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 8200       |\n",
      "|    time_elapsed       | 542        |\n",
      "|    total_timesteps    | 41000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.5      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8199       |\n",
      "|    policy_loss        | -514       |\n",
      "|    reward             | -15.255558 |\n",
      "|    std                | 1.09       |\n",
      "|    value_loss         | 1.27e+03   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 75        |\n",
      "|    iterations         | 8300      |\n",
      "|    time_elapsed       | 549       |\n",
      "|    total_timesteps    | 41500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8299      |\n",
      "|    policy_loss        | -28.1     |\n",
      "|    reward             | -1.632191 |\n",
      "|    std                | 1.09      |\n",
      "|    value_loss         | 6.16      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 8400       |\n",
      "|    time_elapsed       | 555        |\n",
      "|    total_timesteps    | 42000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.5      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8399       |\n",
      "|    policy_loss        | -29.3      |\n",
      "|    reward             | 0.47761676 |\n",
      "|    std                | 1.1        |\n",
      "|    value_loss         | 6.75       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 75        |\n",
      "|    iterations         | 8500      |\n",
      "|    time_elapsed       | 561       |\n",
      "|    total_timesteps    | 42500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8499      |\n",
      "|    policy_loss        | -16.1     |\n",
      "|    reward             | 2.1819937 |\n",
      "|    std                | 1.09      |\n",
      "|    value_loss         | 2.66      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 75        |\n",
      "|    iterations         | 8600      |\n",
      "|    time_elapsed       | 568       |\n",
      "|    total_timesteps    | 43000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8599      |\n",
      "|    policy_loss        | -12       |\n",
      "|    reward             | 1.6841316 |\n",
      "|    std                | 1.09      |\n",
      "|    value_loss         | 4.39      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 75        |\n",
      "|    iterations         | 8700      |\n",
      "|    time_elapsed       | 574       |\n",
      "|    total_timesteps    | 43500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.5     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8699      |\n",
      "|    policy_loss        | -62.4     |\n",
      "|    reward             | 1.6117307 |\n",
      "|    std                | 1.09      |\n",
      "|    value_loss         | 23.1      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 8800       |\n",
      "|    time_elapsed       | 581        |\n",
      "|    total_timesteps    | 44000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.5      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8799       |\n",
      "|    policy_loss        | 216        |\n",
      "|    reward             | -1.6540053 |\n",
      "|    std                | 1.09       |\n",
      "|    value_loss         | 284        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 75        |\n",
      "|    iterations         | 8900      |\n",
      "|    time_elapsed       | 587       |\n",
      "|    total_timesteps    | 44500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.5     |\n",
      "|    explained_variance | -1.44     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8899      |\n",
      "|    policy_loss        | 34.8      |\n",
      "|    reward             | 2.2842288 |\n",
      "|    std                | 1.09      |\n",
      "|    value_loss         | 8.74      |\n",
      "-------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 75           |\n",
      "|    iterations         | 9000         |\n",
      "|    time_elapsed       | 593          |\n",
      "|    total_timesteps    | 45000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -13.5        |\n",
      "|    explained_variance | 1.79e-07     |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 8999         |\n",
      "|    policy_loss        | 18.4         |\n",
      "|    reward             | 0.0049086628 |\n",
      "|    std                | 1.09         |\n",
      "|    value_loss         | 2.55         |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 9100       |\n",
      "|    time_elapsed       | 600        |\n",
      "|    total_timesteps    | 45500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.5      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9099       |\n",
      "|    policy_loss        | 1.5        |\n",
      "|    reward             | -0.2090941 |\n",
      "|    std                | 1.09       |\n",
      "|    value_loss         | 0.407      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 9200       |\n",
      "|    time_elapsed       | 606        |\n",
      "|    total_timesteps    | 46000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.5      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9199       |\n",
      "|    policy_loss        | 66.9       |\n",
      "|    reward             | 0.64316213 |\n",
      "|    std                | 1.09       |\n",
      "|    value_loss         | 26.6       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 75        |\n",
      "|    iterations         | 9300      |\n",
      "|    time_elapsed       | 613       |\n",
      "|    total_timesteps    | 46500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9299      |\n",
      "|    policy_loss        | 8.23      |\n",
      "|    reward             | 2.3158648 |\n",
      "|    std                | 1.09      |\n",
      "|    value_loss         | 1.28      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 75        |\n",
      "|    iterations         | 9400      |\n",
      "|    time_elapsed       | 619       |\n",
      "|    total_timesteps    | 47000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.5     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9399      |\n",
      "|    policy_loss        | 28        |\n",
      "|    reward             | 3.0738132 |\n",
      "|    std                | 1.09      |\n",
      "|    value_loss         | 37.8      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 75       |\n",
      "|    iterations         | 9500     |\n",
      "|    time_elapsed       | 626      |\n",
      "|    total_timesteps    | 47500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13.5    |\n",
      "|    explained_variance | -0.0705  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9499     |\n",
      "|    policy_loss        | 11.3     |\n",
      "|    reward             | 0.542357 |\n",
      "|    std                | 1.09     |\n",
      "|    value_loss         | 1.26     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 9600       |\n",
      "|    time_elapsed       | 632        |\n",
      "|    total_timesteps    | 48000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.5      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9599       |\n",
      "|    policy_loss        | -7.61      |\n",
      "|    reward             | -1.7174015 |\n",
      "|    std                | 1.09       |\n",
      "|    value_loss         | 1.19       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 9700       |\n",
      "|    time_elapsed       | 638        |\n",
      "|    total_timesteps    | 48500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.5      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9699       |\n",
      "|    policy_loss        | -2         |\n",
      "|    reward             | -1.6432642 |\n",
      "|    std                | 1.09       |\n",
      "|    value_loss         | 0.325      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 9800       |\n",
      "|    time_elapsed       | 645        |\n",
      "|    total_timesteps    | 49000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.6      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9799       |\n",
      "|    policy_loss        | -100       |\n",
      "|    reward             | -1.0874778 |\n",
      "|    std                | 1.1        |\n",
      "|    value_loss         | 49.4       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 9900       |\n",
      "|    time_elapsed       | 651        |\n",
      "|    total_timesteps    | 49500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.6      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9899       |\n",
      "|    policy_loss        | -23.7      |\n",
      "|    reward             | 0.08231932 |\n",
      "|    std                | 1.1        |\n",
      "|    value_loss         | 4          |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 10000      |\n",
      "|    time_elapsed       | 658        |\n",
      "|    total_timesteps    | 50000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.6      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9999       |\n",
      "|    policy_loss        | 3.98       |\n",
      "|    reward             | -6.4332957 |\n",
      "|    std                | 1.1        |\n",
      "|    value_loss         | 3.59       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 75        |\n",
      "|    iterations         | 10100     |\n",
      "|    time_elapsed       | 665       |\n",
      "|    total_timesteps    | 50500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.6     |\n",
      "|    explained_variance | 0.00494   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 10099     |\n",
      "|    policy_loss        | 0.805     |\n",
      "|    reward             | 1.5036052 |\n",
      "|    std                | 1.1       |\n",
      "|    value_loss         | 0.75      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 75        |\n",
      "|    iterations         | 10200     |\n",
      "|    time_elapsed       | 671       |\n",
      "|    total_timesteps    | 51000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 10199     |\n",
      "|    policy_loss        | 29.5      |\n",
      "|    reward             | 1.4118037 |\n",
      "|    std                | 1.1       |\n",
      "|    value_loss         | 6.15      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 10300      |\n",
      "|    time_elapsed       | 677        |\n",
      "|    total_timesteps    | 51500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.6      |\n",
      "|    explained_variance | -0.00183   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 10299      |\n",
      "|    policy_loss        | 16         |\n",
      "|    reward             | 0.66154456 |\n",
      "|    std                | 1.1        |\n",
      "|    value_loss         | 1.66       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 75        |\n",
      "|    iterations         | 10400     |\n",
      "|    time_elapsed       | 684       |\n",
      "|    total_timesteps    | 52000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.6     |\n",
      "|    explained_variance | 1.79e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 10399     |\n",
      "|    policy_loss        | 72.2      |\n",
      "|    reward             | 0.2827786 |\n",
      "|    std                | 1.1       |\n",
      "|    value_loss         | 33.3      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 10500      |\n",
      "|    time_elapsed       | 691        |\n",
      "|    total_timesteps    | 52500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.5      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 10499      |\n",
      "|    policy_loss        | 98.1       |\n",
      "|    reward             | -6.7189126 |\n",
      "|    std                | 1.09       |\n",
      "|    value_loss         | 86.6       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 10600      |\n",
      "|    time_elapsed       | 697        |\n",
      "|    total_timesteps    | 53000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.6      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 10599      |\n",
      "|    policy_loss        | -189       |\n",
      "|    reward             | -1.6464666 |\n",
      "|    std                | 1.1        |\n",
      "|    value_loss         | 347        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 75        |\n",
      "|    iterations         | 10700     |\n",
      "|    time_elapsed       | 704       |\n",
      "|    total_timesteps    | 53500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.6     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 10699     |\n",
      "|    policy_loss        | 7.79      |\n",
      "|    reward             | 0.6717859 |\n",
      "|    std                | 1.1       |\n",
      "|    value_loss         | 0.49      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 10800      |\n",
      "|    time_elapsed       | 710        |\n",
      "|    total_timesteps    | 54000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.6      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 10799      |\n",
      "|    policy_loss        | 14.2       |\n",
      "|    reward             | -0.5121781 |\n",
      "|    std                | 1.1        |\n",
      "|    value_loss         | 1.64       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 10900      |\n",
      "|    time_elapsed       | 717        |\n",
      "|    total_timesteps    | 54500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.6      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 10899      |\n",
      "|    policy_loss        | -21.3      |\n",
      "|    reward             | -2.0998354 |\n",
      "|    std                | 1.1        |\n",
      "|    value_loss         | 5.33       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 75        |\n",
      "|    iterations         | 11000     |\n",
      "|    time_elapsed       | 723       |\n",
      "|    total_timesteps    | 55000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.6     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 10999     |\n",
      "|    policy_loss        | -3.99     |\n",
      "|    reward             | 0.6192747 |\n",
      "|    std                | 1.1       |\n",
      "|    value_loss         | 0.426     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 11100     |\n",
      "|    time_elapsed       | 730       |\n",
      "|    total_timesteps    | 55500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 11099     |\n",
      "|    policy_loss        | 102       |\n",
      "|    reward             | 0.9341263 |\n",
      "|    std                | 1.1       |\n",
      "|    value_loss         | 98.4      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 76       |\n",
      "|    iterations         | 11200    |\n",
      "|    time_elapsed       | 736      |\n",
      "|    total_timesteps    | 56000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13.6    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 11199    |\n",
      "|    policy_loss        | -48.3    |\n",
      "|    reward             | -4.68881 |\n",
      "|    std                | 1.1      |\n",
      "|    value_loss         | 15.1     |\n",
      "------------------------------------\n",
      "day: 2956, episode: 20\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 5006837.62\n",
      "total_reward: 4006837.62\n",
      "total_cost: 3998.25\n",
      "total_trades: 10595\n",
      "Sharpe: 0.844\n",
      "=================================\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 11300      |\n",
      "|    time_elapsed       | 743        |\n",
      "|    total_timesteps    | 56500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.6      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 11299      |\n",
      "|    policy_loss        | 13.4       |\n",
      "|    reward             | 0.29080635 |\n",
      "|    std                | 1.1        |\n",
      "|    value_loss         | 1.15       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 11400     |\n",
      "|    time_elapsed       | 749       |\n",
      "|    total_timesteps    | 57000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 11399     |\n",
      "|    policy_loss        | -1.48     |\n",
      "|    reward             | 0.9406605 |\n",
      "|    std                | 1.11      |\n",
      "|    value_loss         | 0.375     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 76       |\n",
      "|    iterations         | 11500    |\n",
      "|    time_elapsed       | 756      |\n",
      "|    total_timesteps    | 57500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13.6    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 11499    |\n",
      "|    policy_loss        | 5.05     |\n",
      "|    reward             | 2.639221 |\n",
      "|    std                | 1.1      |\n",
      "|    value_loss         | 1.04     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 11600      |\n",
      "|    time_elapsed       | 763        |\n",
      "|    total_timesteps    | 58000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.5      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 11599      |\n",
      "|    policy_loss        | 3.32       |\n",
      "|    reward             | 0.40962586 |\n",
      "|    std                | 1.1        |\n",
      "|    value_loss         | 0.211      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 11700      |\n",
      "|    time_elapsed       | 769        |\n",
      "|    total_timesteps    | 58500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.6      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 11699      |\n",
      "|    policy_loss        | 36.3       |\n",
      "|    reward             | -1.3985709 |\n",
      "|    std                | 1.1        |\n",
      "|    value_loss         | 9.16       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 11800     |\n",
      "|    time_elapsed       | 776       |\n",
      "|    total_timesteps    | 59000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 11799     |\n",
      "|    policy_loss        | -67.3     |\n",
      "|    reward             | 4.6986585 |\n",
      "|    std                | 1.1       |\n",
      "|    value_loss         | 79.2      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 11900      |\n",
      "|    time_elapsed       | 782        |\n",
      "|    total_timesteps    | 59500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.6      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 11899      |\n",
      "|    policy_loss        | -31.9      |\n",
      "|    reward             | -0.9581499 |\n",
      "|    std                | 1.1        |\n",
      "|    value_loss         | 6.14       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 12000      |\n",
      "|    time_elapsed       | 789        |\n",
      "|    total_timesteps    | 60000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.6      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 11999      |\n",
      "|    policy_loss        | -18.5      |\n",
      "|    reward             | -2.7737744 |\n",
      "|    std                | 1.1        |\n",
      "|    value_loss         | 3.13       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 12100      |\n",
      "|    time_elapsed       | 795        |\n",
      "|    total_timesteps    | 60500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.6      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 12099      |\n",
      "|    policy_loss        | -6.72      |\n",
      "|    reward             | 0.62269235 |\n",
      "|    std                | 1.11       |\n",
      "|    value_loss         | 0.34       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 75        |\n",
      "|    iterations         | 12200     |\n",
      "|    time_elapsed       | 802       |\n",
      "|    total_timesteps    | 61000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 12199     |\n",
      "|    policy_loss        | -86       |\n",
      "|    reward             | 0.7148877 |\n",
      "|    std                | 1.11      |\n",
      "|    value_loss         | 42.9      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 75        |\n",
      "|    iterations         | 12300     |\n",
      "|    time_elapsed       | 809       |\n",
      "|    total_timesteps    | 61500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.7     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 12299     |\n",
      "|    policy_loss        | -80       |\n",
      "|    reward             | 2.8248792 |\n",
      "|    std                | 1.11      |\n",
      "|    value_loss         | 50.8      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 75       |\n",
      "|    iterations         | 12400    |\n",
      "|    time_elapsed       | 815      |\n",
      "|    total_timesteps    | 62000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13.7    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 12399    |\n",
      "|    policy_loss        | 57       |\n",
      "|    reward             | 8.721681 |\n",
      "|    std                | 1.11     |\n",
      "|    value_loss         | 19.8     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 76       |\n",
      "|    iterations         | 12500    |\n",
      "|    time_elapsed       | 822      |\n",
      "|    total_timesteps    | 62500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13.7    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 12499    |\n",
      "|    policy_loss        | -72.7    |\n",
      "|    reward             | 4.8651   |\n",
      "|    std                | 1.11     |\n",
      "|    value_loss         | 35.2     |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 76          |\n",
      "|    iterations         | 12600       |\n",
      "|    time_elapsed       | 828         |\n",
      "|    total_timesteps    | 63000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.7       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 12599       |\n",
      "|    policy_loss        | -21.6       |\n",
      "|    reward             | -0.96296847 |\n",
      "|    std                | 1.12        |\n",
      "|    value_loss         | 2.56        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 12700      |\n",
      "|    time_elapsed       | 835        |\n",
      "|    total_timesteps    | 63500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 12699      |\n",
      "|    policy_loss        | -25.8      |\n",
      "|    reward             | 0.45332214 |\n",
      "|    std                | 1.12       |\n",
      "|    value_loss         | 5.05       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 76          |\n",
      "|    iterations         | 12800       |\n",
      "|    time_elapsed       | 841         |\n",
      "|    total_timesteps    | 64000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.7       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 12799       |\n",
      "|    policy_loss        | 26.1        |\n",
      "|    reward             | -0.35387352 |\n",
      "|    std                | 1.12        |\n",
      "|    value_loss         | 3.27        |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 76       |\n",
      "|    iterations         | 12900    |\n",
      "|    time_elapsed       | 848      |\n",
      "|    total_timesteps    | 64500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13.7    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 12899    |\n",
      "|    policy_loss        | 40.7     |\n",
      "|    reward             | 2.901327 |\n",
      "|    std                | 1.12     |\n",
      "|    value_loss         | 14       |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 13000      |\n",
      "|    time_elapsed       | 854        |\n",
      "|    total_timesteps    | 65000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.7      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 12999      |\n",
      "|    policy_loss        | 55.7       |\n",
      "|    reward             | -6.9894443 |\n",
      "|    std                | 1.11       |\n",
      "|    value_loss         | 21.3       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 76       |\n",
      "|    iterations         | 13100    |\n",
      "|    time_elapsed       | 861      |\n",
      "|    total_timesteps    | 65500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13.7    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 13099    |\n",
      "|    policy_loss        | -76.2    |\n",
      "|    reward             | 3.171884 |\n",
      "|    std                | 1.12     |\n",
      "|    value_loss         | 33.4     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 13200      |\n",
      "|    time_elapsed       | 867        |\n",
      "|    total_timesteps    | 66000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 13199      |\n",
      "|    policy_loss        | -18.1      |\n",
      "|    reward             | -1.8378233 |\n",
      "|    std                | 1.12       |\n",
      "|    value_loss         | 2.53       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 13300     |\n",
      "|    time_elapsed       | 874       |\n",
      "|    total_timesteps    | 66500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 13299     |\n",
      "|    policy_loss        | -44.3     |\n",
      "|    reward             | 1.1297455 |\n",
      "|    std                | 1.12      |\n",
      "|    value_loss         | 16.8      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 76          |\n",
      "|    iterations         | 13400       |\n",
      "|    time_elapsed       | 881         |\n",
      "|    total_timesteps    | 67000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.7       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 13399       |\n",
      "|    policy_loss        | -9.68       |\n",
      "|    reward             | 0.007233167 |\n",
      "|    std                | 1.12        |\n",
      "|    value_loss         | 1.05        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 13500      |\n",
      "|    time_elapsed       | 887        |\n",
      "|    total_timesteps    | 67500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.7      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 13499      |\n",
      "|    policy_loss        | -42.8      |\n",
      "|    reward             | 0.95013845 |\n",
      "|    std                | 1.12       |\n",
      "|    value_loss         | 10.1       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 13600     |\n",
      "|    time_elapsed       | 893       |\n",
      "|    total_timesteps    | 68000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 13599     |\n",
      "|    policy_loss        | -14.4     |\n",
      "|    reward             | 1.4389671 |\n",
      "|    std                | 1.11      |\n",
      "|    value_loss         | 1.6       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 13700     |\n",
      "|    time_elapsed       | 900       |\n",
      "|    total_timesteps    | 68500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 13699     |\n",
      "|    policy_loss        | 36.4      |\n",
      "|    reward             | 1.8704686 |\n",
      "|    std                | 1.11      |\n",
      "|    value_loss         | 13.3      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 76       |\n",
      "|    iterations         | 13800    |\n",
      "|    time_elapsed       | 907      |\n",
      "|    total_timesteps    | 69000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13.7    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 13799    |\n",
      "|    policy_loss        | 1.67     |\n",
      "|    reward             | 2.07118  |\n",
      "|    std                | 1.12     |\n",
      "|    value_loss         | 0.195    |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 13900      |\n",
      "|    time_elapsed       | 913        |\n",
      "|    total_timesteps    | 69500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 13899      |\n",
      "|    policy_loss        | 12         |\n",
      "|    reward             | -1.9126241 |\n",
      "|    std                | 1.12       |\n",
      "|    value_loss         | 4.02       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 14000      |\n",
      "|    time_elapsed       | 920        |\n",
      "|    total_timesteps    | 70000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 13999      |\n",
      "|    policy_loss        | 8.18       |\n",
      "|    reward             | 0.18617079 |\n",
      "|    std                | 1.12       |\n",
      "|    value_loss         | 0.871      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 14100      |\n",
      "|    time_elapsed       | 926        |\n",
      "|    total_timesteps    | 70500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.8      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 14099      |\n",
      "|    policy_loss        | -3.22      |\n",
      "|    reward             | -1.3269366 |\n",
      "|    std                | 1.12       |\n",
      "|    value_loss         | 0.515      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 14200      |\n",
      "|    time_elapsed       | 933        |\n",
      "|    total_timesteps    | 71000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.8      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 14199      |\n",
      "|    policy_loss        | 5.84       |\n",
      "|    reward             | 0.13067745 |\n",
      "|    std                | 1.12       |\n",
      "|    value_loss         | 0.199      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 14300     |\n",
      "|    time_elapsed       | 939       |\n",
      "|    total_timesteps    | 71500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.8     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 14299     |\n",
      "|    policy_loss        | 14.6      |\n",
      "|    reward             | 0.9165654 |\n",
      "|    std                | 1.12      |\n",
      "|    value_loss         | 1.89      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 76          |\n",
      "|    iterations         | 14400       |\n",
      "|    time_elapsed       | 946         |\n",
      "|    total_timesteps    | 72000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.8       |\n",
      "|    explained_variance | 1.79e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 14399       |\n",
      "|    policy_loss        | -85         |\n",
      "|    reward             | 0.048696145 |\n",
      "|    std                | 1.13        |\n",
      "|    value_loss         | 38.7        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 76          |\n",
      "|    iterations         | 14500       |\n",
      "|    time_elapsed       | 952         |\n",
      "|    total_timesteps    | 72500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.8       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 14499       |\n",
      "|    policy_loss        | 16.7        |\n",
      "|    reward             | -0.73614377 |\n",
      "|    std                | 1.13        |\n",
      "|    value_loss         | 3.54        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 14600     |\n",
      "|    time_elapsed       | 959       |\n",
      "|    total_timesteps    | 73000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.8     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 14599     |\n",
      "|    policy_loss        | 64.3      |\n",
      "|    reward             | -2.132495 |\n",
      "|    std                | 1.13      |\n",
      "|    value_loss         | 26.5      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 14700      |\n",
      "|    time_elapsed       | 965        |\n",
      "|    total_timesteps    | 73500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 14699      |\n",
      "|    policy_loss        | 37.6       |\n",
      "|    reward             | -4.6349497 |\n",
      "|    std                | 1.13       |\n",
      "|    value_loss         | 9.55       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 14800      |\n",
      "|    time_elapsed       | 972        |\n",
      "|    total_timesteps    | 74000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.8      |\n",
      "|    explained_variance | -0.00389   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 14799      |\n",
      "|    policy_loss        | 1.32       |\n",
      "|    reward             | 0.20704296 |\n",
      "|    std                | 1.12       |\n",
      "|    value_loss         | 0.084      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 14900     |\n",
      "|    time_elapsed       | 978       |\n",
      "|    total_timesteps    | 74500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.8     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 14899     |\n",
      "|    policy_loss        | -32.4     |\n",
      "|    reward             | 0.0891951 |\n",
      "|    std                | 1.13      |\n",
      "|    value_loss         | 6.39      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 15000      |\n",
      "|    time_elapsed       | 988        |\n",
      "|    total_timesteps    | 75000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 14999      |\n",
      "|    policy_loss        | -24.2      |\n",
      "|    reward             | -2.0057395 |\n",
      "|    std                | 1.13       |\n",
      "|    value_loss         | 4.28       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 75        |\n",
      "|    iterations         | 15100     |\n",
      "|    time_elapsed       | 997       |\n",
      "|    total_timesteps    | 75500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.8     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15099     |\n",
      "|    policy_loss        | 18.2      |\n",
      "|    reward             | -2.751422 |\n",
      "|    std                | 1.13      |\n",
      "|    value_loss         | 3.78      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 75        |\n",
      "|    iterations         | 15200     |\n",
      "|    time_elapsed       | 1007      |\n",
      "|    total_timesteps    | 76000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.8     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15199     |\n",
      "|    policy_loss        | -24.2     |\n",
      "|    reward             | 3.7738245 |\n",
      "|    std                | 1.13      |\n",
      "|    value_loss         | 31.2      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 15300      |\n",
      "|    time_elapsed       | 1016       |\n",
      "|    total_timesteps    | 76500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.9      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 15299      |\n",
      "|    policy_loss        | -227       |\n",
      "|    reward             | -7.8535523 |\n",
      "|    std                | 1.13       |\n",
      "|    value_loss         | 540        |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 75          |\n",
      "|    iterations         | 15400       |\n",
      "|    time_elapsed       | 1023        |\n",
      "|    total_timesteps    | 77000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.9       |\n",
      "|    explained_variance | -0.523      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 15399       |\n",
      "|    policy_loss        | 22.1        |\n",
      "|    reward             | -0.39156508 |\n",
      "|    std                | 1.14        |\n",
      "|    value_loss         | 7.34        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 75        |\n",
      "|    iterations         | 15500     |\n",
      "|    time_elapsed       | 1030      |\n",
      "|    total_timesteps    | 77500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.9     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15499     |\n",
      "|    policy_loss        | 32.9      |\n",
      "|    reward             | 1.0329311 |\n",
      "|    std                | 1.14      |\n",
      "|    value_loss         | 7.72      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 75          |\n",
      "|    iterations         | 15600       |\n",
      "|    time_elapsed       | 1037        |\n",
      "|    total_timesteps    | 78000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.9       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 15599       |\n",
      "|    policy_loss        | 25.4        |\n",
      "|    reward             | 0.104982376 |\n",
      "|    std                | 1.14        |\n",
      "|    value_loss         | 4           |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 15700      |\n",
      "|    time_elapsed       | 1044       |\n",
      "|    total_timesteps    | 78500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -14        |\n",
      "|    explained_variance | 1.79e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 15699      |\n",
      "|    policy_loss        | 6.3        |\n",
      "|    reward             | 0.62970656 |\n",
      "|    std                | 1.15       |\n",
      "|    value_loss         | 0.597      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 75        |\n",
      "|    iterations         | 15800     |\n",
      "|    time_elapsed       | 1050      |\n",
      "|    total_timesteps    | 79000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -14       |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15799     |\n",
      "|    policy_loss        | 9.43      |\n",
      "|    reward             | 2.7345433 |\n",
      "|    std                | 1.15      |\n",
      "|    value_loss         | 0.913     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 15900      |\n",
      "|    time_elapsed       | 1057       |\n",
      "|    total_timesteps    | 79500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -14        |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 15899      |\n",
      "|    policy_loss        | 88.9       |\n",
      "|    reward             | 0.43832535 |\n",
      "|    std                | 1.15       |\n",
      "|    value_loss         | 85.3       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 16000      |\n",
      "|    time_elapsed       | 1063       |\n",
      "|    total_timesteps    | 80000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -14        |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 15999      |\n",
      "|    policy_loss        | -1.03      |\n",
      "|    reward             | -1.5228717 |\n",
      "|    std                | 1.15       |\n",
      "|    value_loss         | 2.44       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 16100      |\n",
      "|    time_elapsed       | 1070       |\n",
      "|    total_timesteps    | 80500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -14        |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 16099      |\n",
      "|    policy_loss        | 1.54       |\n",
      "|    reward             | 0.61875963 |\n",
      "|    std                | 1.16       |\n",
      "|    value_loss         | 0.808      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 75        |\n",
      "|    iterations         | 16200     |\n",
      "|    time_elapsed       | 1077      |\n",
      "|    total_timesteps    | 81000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -14       |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16199     |\n",
      "|    policy_loss        | -28.5     |\n",
      "|    reward             | 1.4830098 |\n",
      "|    std                | 1.16      |\n",
      "|    value_loss         | 5.18      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 75          |\n",
      "|    iterations         | 16300       |\n",
      "|    time_elapsed       | 1083        |\n",
      "|    total_timesteps    | 81500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -14         |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 16299       |\n",
      "|    policy_loss        | -30.3       |\n",
      "|    reward             | 0.004706867 |\n",
      "|    std                | 1.16        |\n",
      "|    value_loss         | 4.64        |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 75       |\n",
      "|    iterations         | 16400    |\n",
      "|    time_elapsed       | 1090     |\n",
      "|    total_timesteps    | 82000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -14      |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 16399    |\n",
      "|    policy_loss        | 4.87     |\n",
      "|    reward             | 1.560369 |\n",
      "|    std                | 1.16     |\n",
      "|    value_loss         | 1.48     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 16500      |\n",
      "|    time_elapsed       | 1097       |\n",
      "|    total_timesteps    | 82500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -14        |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 16499      |\n",
      "|    policy_loss        | 5.68       |\n",
      "|    reward             | -1.2168511 |\n",
      "|    std                | 1.16       |\n",
      "|    value_loss         | 6.04       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 75        |\n",
      "|    iterations         | 16600     |\n",
      "|    time_elapsed       | 1104      |\n",
      "|    total_timesteps    | 83000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -14       |\n",
      "|    explained_variance | 0.0211    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16599     |\n",
      "|    policy_loss        | 2.72      |\n",
      "|    reward             | 0.3668785 |\n",
      "|    std                | 1.16      |\n",
      "|    value_loss         | 0.0888    |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 75        |\n",
      "|    iterations         | 16700     |\n",
      "|    time_elapsed       | 1110      |\n",
      "|    total_timesteps    | 83500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -14.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16699     |\n",
      "|    policy_loss        | -34.9     |\n",
      "|    reward             | 0.7814096 |\n",
      "|    std                | 1.16      |\n",
      "|    value_loss         | 7.08      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 16800      |\n",
      "|    time_elapsed       | 1117       |\n",
      "|    total_timesteps    | 84000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -14.1      |\n",
      "|    explained_variance | 1.79e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 16799      |\n",
      "|    policy_loss        | 16.7       |\n",
      "|    reward             | -1.9222854 |\n",
      "|    std                | 1.16       |\n",
      "|    value_loss         | 3.5        |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 75       |\n",
      "|    iterations         | 16900    |\n",
      "|    time_elapsed       | 1123     |\n",
      "|    total_timesteps    | 84500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -14      |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 16899    |\n",
      "|    policy_loss        | 2.15     |\n",
      "|    reward             | 0.392867 |\n",
      "|    std                | 1.16     |\n",
      "|    value_loss         | 1.12     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 17000      |\n",
      "|    time_elapsed       | 1129       |\n",
      "|    total_timesteps    | 85000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -14        |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 16999      |\n",
      "|    policy_loss        | -12.5      |\n",
      "|    reward             | -3.5044694 |\n",
      "|    std                | 1.16       |\n",
      "|    value_loss         | 4.57       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 75        |\n",
      "|    iterations         | 17100     |\n",
      "|    time_elapsed       | 1136      |\n",
      "|    total_timesteps    | 85500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -14       |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17099     |\n",
      "|    policy_loss        | -115      |\n",
      "|    reward             | 3.6405673 |\n",
      "|    std                | 1.15      |\n",
      "|    value_loss         | 114       |\n",
      "-------------------------------------\n",
      "day: 2956, episode: 30\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 5192484.66\n",
      "total_reward: 4192484.66\n",
      "total_cost: 3772.75\n",
      "total_trades: 15439\n",
      "Sharpe: 0.936\n",
      "=================================\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 75          |\n",
      "|    iterations         | 17200       |\n",
      "|    time_elapsed       | 1142        |\n",
      "|    total_timesteps    | 86000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -14         |\n",
      "|    explained_variance | 0.101       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 17199       |\n",
      "|    policy_loss        | 1.64        |\n",
      "|    reward             | -0.19651026 |\n",
      "|    std                | 1.16        |\n",
      "|    value_loss         | 0.0777      |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 75        |\n",
      "|    iterations         | 17300     |\n",
      "|    time_elapsed       | 1149      |\n",
      "|    total_timesteps    | 86500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -14       |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17299     |\n",
      "|    policy_loss        | -12       |\n",
      "|    reward             | 0.5064195 |\n",
      "|    std                | 1.16      |\n",
      "|    value_loss         | 0.849     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 17400      |\n",
      "|    time_elapsed       | 1155       |\n",
      "|    total_timesteps    | 87000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -14.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 17399      |\n",
      "|    policy_loss        | -18.5      |\n",
      "|    reward             | -2.2953637 |\n",
      "|    std                | 1.16       |\n",
      "|    value_loss         | 1.78       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 75        |\n",
      "|    iterations         | 17500     |\n",
      "|    time_elapsed       | 1162      |\n",
      "|    total_timesteps    | 87500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -14.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17499     |\n",
      "|    policy_loss        | -29.6     |\n",
      "|    reward             | 1.7147244 |\n",
      "|    std                | 1.16      |\n",
      "|    value_loss         | 7.25      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 75        |\n",
      "|    iterations         | 17600     |\n",
      "|    time_elapsed       | 1169      |\n",
      "|    total_timesteps    | 88000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -14.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17599     |\n",
      "|    policy_loss        | 156       |\n",
      "|    reward             | 0.2770998 |\n",
      "|    std                | 1.16      |\n",
      "|    value_loss         | 130       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 75        |\n",
      "|    iterations         | 17700     |\n",
      "|    time_elapsed       | 1176      |\n",
      "|    total_timesteps    | 88500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -14.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17699     |\n",
      "|    policy_loss        | 14.9      |\n",
      "|    reward             | 3.2807126 |\n",
      "|    std                | 1.16      |\n",
      "|    value_loss         | 8.05      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 17800      |\n",
      "|    time_elapsed       | 1181       |\n",
      "|    total_timesteps    | 89000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -14        |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 17799      |\n",
      "|    policy_loss        | 11.4       |\n",
      "|    reward             | 0.84830517 |\n",
      "|    std                | 1.16       |\n",
      "|    value_loss         | 0.657      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 75        |\n",
      "|    iterations         | 17900     |\n",
      "|    time_elapsed       | 1187      |\n",
      "|    total_timesteps    | 89500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -14.1     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17899     |\n",
      "|    policy_loss        | -0.348    |\n",
      "|    reward             | -2.589642 |\n",
      "|    std                | 1.16      |\n",
      "|    value_loss         | 0.0274    |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 18000      |\n",
      "|    time_elapsed       | 1194       |\n",
      "|    total_timesteps    | 90000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -14.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 17999      |\n",
      "|    policy_loss        | 15.7       |\n",
      "|    reward             | 0.08467221 |\n",
      "|    std                | 1.16       |\n",
      "|    value_loss         | 2.03       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 75        |\n",
      "|    iterations         | 18100     |\n",
      "|    time_elapsed       | 1200      |\n",
      "|    total_timesteps    | 90500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -14.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18099     |\n",
      "|    policy_loss        | -13.4     |\n",
      "|    reward             | 0.7703773 |\n",
      "|    std                | 1.16      |\n",
      "|    value_loss         | 1.36      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 75          |\n",
      "|    iterations         | 18200       |\n",
      "|    time_elapsed       | 1206        |\n",
      "|    total_timesteps    | 91000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -14.1       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 18199       |\n",
      "|    policy_loss        | 20.9        |\n",
      "|    reward             | -0.49550286 |\n",
      "|    std                | 1.17        |\n",
      "|    value_loss         | 11.2        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 18300      |\n",
      "|    time_elapsed       | 1212       |\n",
      "|    total_timesteps    | 91500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -14.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 18299      |\n",
      "|    policy_loss        | -187       |\n",
      "|    reward             | -1.5377405 |\n",
      "|    std                | 1.16       |\n",
      "|    value_loss         | 184        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 18400      |\n",
      "|    time_elapsed       | 1219       |\n",
      "|    total_timesteps    | 92000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -14.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 18399      |\n",
      "|    policy_loss        | 13.3       |\n",
      "|    reward             | 0.09564175 |\n",
      "|    std                | 1.16       |\n",
      "|    value_loss         | 1.4        |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 75          |\n",
      "|    iterations         | 18500       |\n",
      "|    time_elapsed       | 1225        |\n",
      "|    total_timesteps    | 92500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -14.1       |\n",
      "|    explained_variance | 5.96e-08    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 18499       |\n",
      "|    policy_loss        | -2.85       |\n",
      "|    reward             | -0.35351178 |\n",
      "|    std                | 1.16        |\n",
      "|    value_loss         | 3.21        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 18600      |\n",
      "|    time_elapsed       | 1231       |\n",
      "|    total_timesteps    | 93000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -14.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 18599      |\n",
      "|    policy_loss        | -2.2       |\n",
      "|    reward             | 0.44507915 |\n",
      "|    std                | 1.16       |\n",
      "|    value_loss         | 0.562      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 75        |\n",
      "|    iterations         | 18700     |\n",
      "|    time_elapsed       | 1237      |\n",
      "|    total_timesteps    | 93500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -14.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18699     |\n",
      "|    policy_loss        | -6.17     |\n",
      "|    reward             | 2.1434257 |\n",
      "|    std                | 1.17      |\n",
      "|    value_loss         | 0.445     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 18800      |\n",
      "|    time_elapsed       | 1245       |\n",
      "|    total_timesteps    | 94000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -14.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 18799      |\n",
      "|    policy_loss        | 6.24       |\n",
      "|    reward             | -0.6844628 |\n",
      "|    std                | 1.17       |\n",
      "|    value_loss         | 2.69       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 18900      |\n",
      "|    time_elapsed       | 1251       |\n",
      "|    total_timesteps    | 94500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -14.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 18899      |\n",
      "|    policy_loss        | 74.5       |\n",
      "|    reward             | -1.1964669 |\n",
      "|    std                | 1.16       |\n",
      "|    value_loss         | 64.7       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 75        |\n",
      "|    iterations         | 19000     |\n",
      "|    time_elapsed       | 1258      |\n",
      "|    total_timesteps    | 95000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -14.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18999     |\n",
      "|    policy_loss        | -7.58     |\n",
      "|    reward             | 0.9868038 |\n",
      "|    std                | 1.16      |\n",
      "|    value_loss         | 0.718     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 19100      |\n",
      "|    time_elapsed       | 1264       |\n",
      "|    total_timesteps    | 95500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -14.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 19099      |\n",
      "|    policy_loss        | -85        |\n",
      "|    reward             | 0.97446746 |\n",
      "|    std                | 1.16       |\n",
      "|    value_loss         | 37.1       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 75          |\n",
      "|    iterations         | 19200       |\n",
      "|    time_elapsed       | 1270        |\n",
      "|    total_timesteps    | 96000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -14.1       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 19199       |\n",
      "|    policy_loss        | -0.0501     |\n",
      "|    reward             | -0.17052439 |\n",
      "|    std                | 1.16        |\n",
      "|    value_loss         | 1.68        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 75          |\n",
      "|    iterations         | 19300       |\n",
      "|    time_elapsed       | 1277        |\n",
      "|    total_timesteps    | 96500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -14.1       |\n",
      "|    explained_variance | 1.79e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 19299       |\n",
      "|    policy_loss        | 0.78        |\n",
      "|    reward             | -0.47914827 |\n",
      "|    std                | 1.17        |\n",
      "|    value_loss         | 0.687       |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 75        |\n",
      "|    iterations         | 19400     |\n",
      "|    time_elapsed       | 1284      |\n",
      "|    total_timesteps    | 97000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -14.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19399     |\n",
      "|    policy_loss        | 93.2      |\n",
      "|    reward             | 0.4886387 |\n",
      "|    std                | 1.16      |\n",
      "|    value_loss         | 56.7      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 19500      |\n",
      "|    time_elapsed       | 1291       |\n",
      "|    total_timesteps    | 97500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -14.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 19499      |\n",
      "|    policy_loss        | -43        |\n",
      "|    reward             | -1.3560994 |\n",
      "|    std                | 1.17       |\n",
      "|    value_loss         | 8.26       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 75       |\n",
      "|    iterations         | 19600    |\n",
      "|    time_elapsed       | 1298     |\n",
      "|    total_timesteps    | 98000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -14.1    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 19599    |\n",
      "|    policy_loss        | 41.1     |\n",
      "|    reward             | 0.725391 |\n",
      "|    std                | 1.17     |\n",
      "|    value_loss         | 10       |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 19700      |\n",
      "|    time_elapsed       | 1305       |\n",
      "|    total_timesteps    | 98500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -14.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 19699      |\n",
      "|    policy_loss        | -32.9      |\n",
      "|    reward             | 0.09384803 |\n",
      "|    std                | 1.17       |\n",
      "|    value_loss         | 7.96       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 75        |\n",
      "|    iterations         | 19800     |\n",
      "|    time_elapsed       | 1311      |\n",
      "|    total_timesteps    | 99000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -14.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19799     |\n",
      "|    policy_loss        | 16.8      |\n",
      "|    reward             | -9.406724 |\n",
      "|    std                | 1.17      |\n",
      "|    value_loss         | 2.33      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 19900      |\n",
      "|    time_elapsed       | 1318       |\n",
      "|    total_timesteps    | 99500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -14.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 19899      |\n",
      "|    policy_loss        | -53.7      |\n",
      "|    reward             | -3.6928453 |\n",
      "|    std                | 1.18       |\n",
      "|    value_loss         | 14.7       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 20000      |\n",
      "|    time_elapsed       | 1325       |\n",
      "|    total_timesteps    | 100000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -14.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 19999      |\n",
      "|    policy_loss        | -30.8      |\n",
      "|    reward             | -8.7030325 |\n",
      "|    std                | 1.18       |\n",
      "|    value_loss         | 23         |\n",
      "--------------------------------------\n",
      "hit end!\n",
      "==============Get Baseline Stats===========\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Shape of DataFrame:  (400, 8)\n",
      "Annual return         -0.022145\n",
      "Cumulative returns    -0.034921\n",
      "Annual volatility      0.176822\n",
      "Sharpe ratio          -0.038668\n",
      "Calmar ratio          -0.100930\n",
      "Stability              0.215672\n",
      "Max drawdown          -0.219408\n",
      "Omega ratio            0.993578\n",
      "Sortino ratio         -0.053987\n",
      "Skew                        NaN\n",
      "Kurtosis                    NaN\n",
      "Tail ratio             1.003436\n",
      "Daily value at risk   -0.022305\n",
      "dtype: float64\n",
      "result:                       a2c           dji\n",
      "date                                  \n",
      "2021-10-01  1.000000e+06  1.000000e+06\n",
      "2021-10-04  9.996705e+05  9.905746e+05\n",
      "2021-10-05  1.000250e+06  9.996566e+05\n",
      "2021-10-06  1.000477e+06  1.002637e+06\n",
      "2021-10-07  1.001644e+06  1.012483e+06\n",
      "...                  ...           ...\n",
      "2023-04-27  1.108366e+06  9.854252e+05\n",
      "2023-04-28  1.118390e+06  9.933491e+05\n",
      "2023-05-01  1.118714e+06  9.919956e+05\n",
      "2023-05-02  1.105133e+06  9.812993e+05\n",
      "2023-05-03  1.097614e+06  9.734251e+05\n",
      "\n",
      "[399 rows x 2 columns]\n",
      "A2C로 얻은 투자 수익률>> 9.76 %\n",
      "{'n_steps': 5, 'ent_coef': 0.01, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to results/a2c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\se99a\\anaconda3\\lib\\site-packages\\pyfolio\\timeseries.py:724: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  stats = pd.Series()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 73        |\n",
      "|    iterations         | 100       |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 500       |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -12.9     |\n",
      "|    explained_variance | 0.113     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 99        |\n",
      "|    policy_loss        | -20.9     |\n",
      "|    reward             | 0.0668218 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 3.41      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 74        |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 13        |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -12.9     |\n",
      "|    explained_variance | 0.22      |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | -2.52     |\n",
      "|    reward             | 1.0188936 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 1.1       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 68         |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 21         |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -12.9      |\n",
      "|    explained_variance | 0.0135     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | -14.7      |\n",
      "|    reward             | -2.8838403 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 3.53       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 69          |\n",
      "|    iterations         | 400         |\n",
      "|    time_elapsed       | 28          |\n",
      "|    total_timesteps    | 2000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -12.9       |\n",
      "|    explained_variance | 0.38        |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 399         |\n",
      "|    policy_loss        | -41.6       |\n",
      "|    reward             | -0.03772307 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 12.3        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 71         |\n",
      "|    iterations         | 500        |\n",
      "|    time_elapsed       | 35         |\n",
      "|    total_timesteps    | 2500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -12.9      |\n",
      "|    explained_variance | 0.0544     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 499        |\n",
      "|    policy_loss        | -60.6      |\n",
      "|    reward             | -1.7510525 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 20.8       |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 71           |\n",
      "|    iterations         | 600          |\n",
      "|    time_elapsed       | 41           |\n",
      "|    total_timesteps    | 3000         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -12.9        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 599          |\n",
      "|    policy_loss        | -1.36        |\n",
      "|    reward             | 0.0059576547 |\n",
      "|    std                | 1.01         |\n",
      "|    value_loss         | 0.0265       |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 72         |\n",
      "|    iterations         | 700        |\n",
      "|    time_elapsed       | 48         |\n",
      "|    total_timesteps    | 3500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -12.9      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 699        |\n",
      "|    policy_loss        | 2.46       |\n",
      "|    reward             | -0.6244517 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 0.486      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 72         |\n",
      "|    iterations         | 800        |\n",
      "|    time_elapsed       | 55         |\n",
      "|    total_timesteps    | 4000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -12.9      |\n",
      "|    explained_variance | 0.122      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 799        |\n",
      "|    policy_loss        | -10.2      |\n",
      "|    reward             | 0.10350753 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 1.11       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 72        |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 62        |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -12.9     |\n",
      "|    explained_variance | 0.12      |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | 9.12      |\n",
      "|    reward             | 2.8259103 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 3.57      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 72        |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 68        |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13       |\n",
      "|    explained_variance | -0.00879  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | -93       |\n",
      "|    reward             | 4.7074575 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 112       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 72        |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 75        |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13       |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | 32.9      |\n",
      "|    reward             | 0.8948513 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 17.5      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 72         |\n",
      "|    iterations         | 1200       |\n",
      "|    time_elapsed       | 82         |\n",
      "|    total_timesteps    | 6000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -12.9      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1199       |\n",
      "|    policy_loss        | -14.8      |\n",
      "|    reward             | -1.0036278 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 1.43       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 73         |\n",
      "|    iterations         | 1300       |\n",
      "|    time_elapsed       | 88         |\n",
      "|    total_timesteps    | 6500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -12.9      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1299       |\n",
      "|    policy_loss        | -11.7      |\n",
      "|    reward             | 0.52360404 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 2.13       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 73        |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 95        |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -12.9     |\n",
      "|    explained_variance | -0.0237   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | 26.4      |\n",
      "|    reward             | 0.8565179 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 7.07      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 73          |\n",
      "|    iterations         | 1500        |\n",
      "|    time_elapsed       | 101         |\n",
      "|    total_timesteps    | 7500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -12.9       |\n",
      "|    explained_variance | 0.0141      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1499        |\n",
      "|    policy_loss        | 30.6        |\n",
      "|    reward             | 0.009818496 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 9.03        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 73        |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 108       |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -12.9     |\n",
      "|    explained_variance | -0.00105  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | -15.6     |\n",
      "|    reward             | 2.0273376 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 11.1      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 74       |\n",
      "|    iterations         | 1700     |\n",
      "|    time_elapsed       | 114      |\n",
      "|    total_timesteps    | 8500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -12.9    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1699     |\n",
      "|    policy_loss        | 131      |\n",
      "|    reward             | 4.04603  |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 127      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 74       |\n",
      "|    iterations         | 1800     |\n",
      "|    time_elapsed       | 120      |\n",
      "|    total_timesteps    | 9000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -12.9    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1799     |\n",
      "|    policy_loss        | -40.2    |\n",
      "|    reward             | 1.119485 |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 11       |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 74          |\n",
      "|    iterations         | 1900        |\n",
      "|    time_elapsed       | 127         |\n",
      "|    total_timesteps    | 9500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -12.9       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1899        |\n",
      "|    policy_loss        | -45.7       |\n",
      "|    reward             | -0.69893074 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 14.2        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 74         |\n",
      "|    iterations         | 2000       |\n",
      "|    time_elapsed       | 134        |\n",
      "|    total_timesteps    | 10000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -12.9      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1999       |\n",
      "|    policy_loss        | 7.09       |\n",
      "|    reward             | -0.2227163 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 1.05       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 74       |\n",
      "|    iterations         | 2100     |\n",
      "|    time_elapsed       | 140      |\n",
      "|    total_timesteps    | 10500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -12.9    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2099     |\n",
      "|    policy_loss        | -18      |\n",
      "|    reward             | 2.662647 |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 5.81     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 74         |\n",
      "|    iterations         | 2200       |\n",
      "|    time_elapsed       | 147        |\n",
      "|    total_timesteps    | 11000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -12.9      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2199       |\n",
      "|    policy_loss        | 10.9       |\n",
      "|    reward             | -4.6386786 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 3.27       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 74        |\n",
      "|    iterations         | 2300      |\n",
      "|    time_elapsed       | 153       |\n",
      "|    total_timesteps    | 11500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -12.9     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2299      |\n",
      "|    policy_loss        | 180       |\n",
      "|    reward             | 2.676675  |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 309       |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 74       |\n",
      "|    iterations         | 2400     |\n",
      "|    time_elapsed       | 160      |\n",
      "|    total_timesteps    | 12000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -12.9    |\n",
      "|    explained_variance | 0.00191  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2399     |\n",
      "|    policy_loss        | 34.6     |\n",
      "|    reward             | 0.14174  |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 6.12     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 75        |\n",
      "|    iterations         | 2500      |\n",
      "|    time_elapsed       | 166       |\n",
      "|    total_timesteps    | 12500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13       |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2499      |\n",
      "|    policy_loss        | -13.4     |\n",
      "|    reward             | 0.9733149 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 1.65      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 2600       |\n",
      "|    time_elapsed       | 172        |\n",
      "|    total_timesteps    | 13000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13        |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2599       |\n",
      "|    policy_loss        | 5.79       |\n",
      "|    reward             | 0.53261757 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 1.29       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 2700       |\n",
      "|    time_elapsed       | 179        |\n",
      "|    total_timesteps    | 13500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13        |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2699       |\n",
      "|    policy_loss        | -6.06      |\n",
      "|    reward             | -1.3218145 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 0.542      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 2800       |\n",
      "|    time_elapsed       | 185        |\n",
      "|    total_timesteps    | 14000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13        |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2799       |\n",
      "|    policy_loss        | -62.6      |\n",
      "|    reward             | 0.99432033 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 25.2       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 2900       |\n",
      "|    time_elapsed       | 191        |\n",
      "|    total_timesteps    | 14500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13        |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2899       |\n",
      "|    policy_loss        | 52.1       |\n",
      "|    reward             | 0.39456162 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 27.3       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 3000       |\n",
      "|    time_elapsed       | 198        |\n",
      "|    total_timesteps    | 15000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13        |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2999       |\n",
      "|    policy_loss        | 6.23       |\n",
      "|    reward             | -1.0887085 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 0.405      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 3100       |\n",
      "|    time_elapsed       | 205        |\n",
      "|    total_timesteps    | 15500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13        |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3099       |\n",
      "|    policy_loss        | -31.3      |\n",
      "|    reward             | 0.57408106 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 8.15       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 3200       |\n",
      "|    time_elapsed       | 211        |\n",
      "|    total_timesteps    | 16000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13        |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3199       |\n",
      "|    policy_loss        | 68.8       |\n",
      "|    reward             | 0.93479496 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 27.8       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 3300       |\n",
      "|    time_elapsed       | 218        |\n",
      "|    total_timesteps    | 16500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13        |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3299       |\n",
      "|    policy_loss        | 1.99       |\n",
      "|    reward             | -0.7276246 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 0.169      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 75          |\n",
      "|    iterations         | 3400        |\n",
      "|    time_elapsed       | 224         |\n",
      "|    total_timesteps    | 17000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13         |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 3399        |\n",
      "|    policy_loss        | -113        |\n",
      "|    reward             | -0.75983423 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 164         |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 3500       |\n",
      "|    time_elapsed       | 230        |\n",
      "|    total_timesteps    | 17500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -12.9      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3499       |\n",
      "|    policy_loss        | 113        |\n",
      "|    reward             | -0.2525111 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 78.5       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 3600       |\n",
      "|    time_elapsed       | 237        |\n",
      "|    total_timesteps    | 18000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13        |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3599       |\n",
      "|    policy_loss        | 2.17       |\n",
      "|    reward             | 0.42883295 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 0.137      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 76          |\n",
      "|    iterations         | 3700        |\n",
      "|    time_elapsed       | 243         |\n",
      "|    total_timesteps    | 18500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13         |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 3699        |\n",
      "|    policy_loss        | 5.08        |\n",
      "|    reward             | -0.41829804 |\n",
      "|    std                | 1.03        |\n",
      "|    value_loss         | 0.991       |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 76           |\n",
      "|    iterations         | 3800         |\n",
      "|    time_elapsed       | 249          |\n",
      "|    total_timesteps    | 19000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -13.1        |\n",
      "|    explained_variance | -1.19e-07    |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 3799         |\n",
      "|    policy_loss        | 26.7         |\n",
      "|    reward             | -0.085940145 |\n",
      "|    std                | 1.03         |\n",
      "|    value_loss         | 11.4         |\n",
      "----------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 75        |\n",
      "|    iterations         | 3900      |\n",
      "|    time_elapsed       | 256       |\n",
      "|    total_timesteps    | 19500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.1     |\n",
      "|    explained_variance | 0.000486  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3899      |\n",
      "|    policy_loss        | -9.34     |\n",
      "|    reward             | 0.8633609 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 1.03      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 75        |\n",
      "|    iterations         | 4000      |\n",
      "|    time_elapsed       | 263       |\n",
      "|    total_timesteps    | 20000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3999      |\n",
      "|    policy_loss        | -53.2     |\n",
      "|    reward             | -5.395122 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 20.1      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 75        |\n",
      "|    iterations         | 4100      |\n",
      "|    time_elapsed       | 269       |\n",
      "|    total_timesteps    | 20500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4099      |\n",
      "|    policy_loss        | -28.9     |\n",
      "|    reward             | 1.2071984 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 10.8      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 4200       |\n",
      "|    time_elapsed       | 276        |\n",
      "|    total_timesteps    | 21000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4199       |\n",
      "|    policy_loss        | 1.95       |\n",
      "|    reward             | -0.9695253 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 1.17       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 4300       |\n",
      "|    time_elapsed       | 282        |\n",
      "|    total_timesteps    | 21500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.1      |\n",
      "|    explained_variance | -0.0032    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4299       |\n",
      "|    policy_loss        | 17.2       |\n",
      "|    reward             | -0.3694503 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 2.73       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 4400       |\n",
      "|    time_elapsed       | 288        |\n",
      "|    total_timesteps    | 22000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.1      |\n",
      "|    explained_variance | -0.155     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4399       |\n",
      "|    policy_loss        | -15.5      |\n",
      "|    reward             | -2.3345904 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 1.68       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 4500      |\n",
      "|    time_elapsed       | 295       |\n",
      "|    total_timesteps    | 22500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4499      |\n",
      "|    policy_loss        | 11.6      |\n",
      "|    reward             | 4.4307446 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 0.902     |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 76          |\n",
      "|    iterations         | 4600        |\n",
      "|    time_elapsed       | 301         |\n",
      "|    total_timesteps    | 23000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.2       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 4599        |\n",
      "|    policy_loss        | 52.3        |\n",
      "|    reward             | -0.50491923 |\n",
      "|    std                | 1.05        |\n",
      "|    value_loss         | 16.5        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 4700      |\n",
      "|    time_elapsed       | 307       |\n",
      "|    total_timesteps    | 23500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.2     |\n",
      "|    explained_variance | -0.0346   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4699      |\n",
      "|    policy_loss        | 35.2      |\n",
      "|    reward             | 2.9998612 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 14.5      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 4800       |\n",
      "|    time_elapsed       | 313        |\n",
      "|    total_timesteps    | 24000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.2      |\n",
      "|    explained_variance | -0.00178   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4799       |\n",
      "|    policy_loss        | -6.72      |\n",
      "|    reward             | -0.8822363 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 1.99       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 76          |\n",
      "|    iterations         | 4900        |\n",
      "|    time_elapsed       | 320         |\n",
      "|    total_timesteps    | 24500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.2       |\n",
      "|    explained_variance | 0.00133     |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 4899        |\n",
      "|    policy_loss        | 19.8        |\n",
      "|    reward             | -0.66398346 |\n",
      "|    std                | 1.05        |\n",
      "|    value_loss         | 3.49        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 5000      |\n",
      "|    time_elapsed       | 326       |\n",
      "|    total_timesteps    | 25000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.2     |\n",
      "|    explained_variance | 0.0196    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4999      |\n",
      "|    policy_loss        | -19.9     |\n",
      "|    reward             | 1.0429996 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 3.22      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 5100       |\n",
      "|    time_elapsed       | 332        |\n",
      "|    total_timesteps    | 25500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.2      |\n",
      "|    explained_variance | 0.109      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5099       |\n",
      "|    policy_loss        | 43.3       |\n",
      "|    reward             | 0.40039003 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 11.5       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 5200       |\n",
      "|    time_elapsed       | 338        |\n",
      "|    total_timesteps    | 26000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.2      |\n",
      "|    explained_variance | -1.15      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5199       |\n",
      "|    policy_loss        | -35        |\n",
      "|    reward             | -0.4633149 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 11.1       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 76       |\n",
      "|    iterations         | 5300     |\n",
      "|    time_elapsed       | 345      |\n",
      "|    total_timesteps    | 26500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13.2    |\n",
      "|    explained_variance | -0.0115  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5299     |\n",
      "|    policy_loss        | 70.1     |\n",
      "|    reward             | -4.89521 |\n",
      "|    std                | 1.05     |\n",
      "|    value_loss         | 39.3     |\n",
      "------------------------------------\n",
      "day: 2956, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3964466.76\n",
      "total_reward: 2964466.76\n",
      "total_cost: 41471.78\n",
      "total_trades: 20794\n",
      "Sharpe: 0.731\n",
      "=================================\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 5400      |\n",
      "|    time_elapsed       | 351       |\n",
      "|    total_timesteps    | 27000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.3     |\n",
      "|    explained_variance | 0.0189    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5399      |\n",
      "|    policy_loss        | -33.5     |\n",
      "|    reward             | -1.186669 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 7.32      |\n",
      "-------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 76            |\n",
      "|    iterations         | 5500          |\n",
      "|    time_elapsed       | 357           |\n",
      "|    total_timesteps    | 27500         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -13.2         |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 5499          |\n",
      "|    policy_loss        | 27.8          |\n",
      "|    reward             | -0.0017971804 |\n",
      "|    std                | 1.06          |\n",
      "|    value_loss         | 5.27          |\n",
      "-----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 5600       |\n",
      "|    time_elapsed       | 364        |\n",
      "|    total_timesteps    | 28000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5599       |\n",
      "|    policy_loss        | -3.22      |\n",
      "|    reward             | 0.68456495 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 5.14       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 5700       |\n",
      "|    time_elapsed       | 370        |\n",
      "|    total_timesteps    | 28500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.3      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5699       |\n",
      "|    policy_loss        | -7.37      |\n",
      "|    reward             | -0.1966003 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 2.08       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 77        |\n",
      "|    iterations         | 5800      |\n",
      "|    time_elapsed       | 376       |\n",
      "|    total_timesteps    | 29000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5799      |\n",
      "|    policy_loss        | 22.3      |\n",
      "|    reward             | 1.0093496 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 6.85      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 5900      |\n",
      "|    time_elapsed       | 383       |\n",
      "|    total_timesteps    | 29500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5899      |\n",
      "|    policy_loss        | -30.1     |\n",
      "|    reward             | 2.2924917 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 8.59      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 6000      |\n",
      "|    time_elapsed       | 390       |\n",
      "|    total_timesteps    | 30000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.3     |\n",
      "|    explained_variance | 0.00623   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5999      |\n",
      "|    policy_loss        | -42.5     |\n",
      "|    reward             | 0.9176576 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 15.2      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 6100      |\n",
      "|    time_elapsed       | 396       |\n",
      "|    total_timesteps    | 30500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6099      |\n",
      "|    policy_loss        | 9.73      |\n",
      "|    reward             | 0.6970243 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 0.904     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 77        |\n",
      "|    iterations         | 6200      |\n",
      "|    time_elapsed       | 402       |\n",
      "|    total_timesteps    | 31000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.3     |\n",
      "|    explained_variance | 0.00144   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6199      |\n",
      "|    policy_loss        | -43.5     |\n",
      "|    reward             | -1.876082 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 32.3      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 77        |\n",
      "|    iterations         | 6300      |\n",
      "|    time_elapsed       | 408       |\n",
      "|    total_timesteps    | 31500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6299      |\n",
      "|    policy_loss        | 2.49      |\n",
      "|    reward             | 1.1831834 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 1.49      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 77        |\n",
      "|    iterations         | 6400      |\n",
      "|    time_elapsed       | 414       |\n",
      "|    total_timesteps    | 32000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.3     |\n",
      "|    explained_variance | 1.79e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6399      |\n",
      "|    policy_loss        | 0.621     |\n",
      "|    reward             | 3.9941466 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 6.03      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 77        |\n",
      "|    iterations         | 6500      |\n",
      "|    time_elapsed       | 420       |\n",
      "|    total_timesteps    | 32500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6499      |\n",
      "|    policy_loss        | -55.1     |\n",
      "|    reward             | 1.7069342 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 20.4      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 77         |\n",
      "|    iterations         | 6600       |\n",
      "|    time_elapsed       | 426        |\n",
      "|    total_timesteps    | 33000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.3      |\n",
      "|    explained_variance | 0.000802   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6599       |\n",
      "|    policy_loss        | -46        |\n",
      "|    reward             | -2.0751786 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 13.1       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 77        |\n",
      "|    iterations         | 6700      |\n",
      "|    time_elapsed       | 434       |\n",
      "|    total_timesteps    | 33500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6699      |\n",
      "|    policy_loss        | 5.17      |\n",
      "|    reward             | 1.4892348 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 0.24      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 6800       |\n",
      "|    time_elapsed       | 441        |\n",
      "|    total_timesteps    | 34000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.3      |\n",
      "|    explained_variance | 0.000756   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6799       |\n",
      "|    policy_loss        | 19.2       |\n",
      "|    reward             | -2.2336924 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 2.6        |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 76          |\n",
      "|    iterations         | 6900        |\n",
      "|    time_elapsed       | 450         |\n",
      "|    total_timesteps    | 34500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.3       |\n",
      "|    explained_variance | 0.0168      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 6899        |\n",
      "|    policy_loss        | -2.06       |\n",
      "|    reward             | -0.97238374 |\n",
      "|    std                | 1.06        |\n",
      "|    value_loss         | 0.465       |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 7000      |\n",
      "|    time_elapsed       | 456       |\n",
      "|    total_timesteps    | 35000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.3     |\n",
      "|    explained_variance | -0.322    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6999      |\n",
      "|    policy_loss        | -0.264    |\n",
      "|    reward             | 1.1817423 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 0.345     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 7100       |\n",
      "|    time_elapsed       | 462        |\n",
      "|    total_timesteps    | 35500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7099       |\n",
      "|    policy_loss        | -3.33      |\n",
      "|    reward             | 0.03780213 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 0.0606     |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 76          |\n",
      "|    iterations         | 7200        |\n",
      "|    time_elapsed       | 469         |\n",
      "|    total_timesteps    | 36000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.3       |\n",
      "|    explained_variance | 0.0172      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 7199        |\n",
      "|    policy_loss        | 4.87        |\n",
      "|    reward             | -0.36302328 |\n",
      "|    std                | 1.06        |\n",
      "|    value_loss         | 0.293       |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 7300      |\n",
      "|    time_elapsed       | 475       |\n",
      "|    total_timesteps    | 36500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.3     |\n",
      "|    explained_variance | 1.79e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7299      |\n",
      "|    policy_loss        | -10.9     |\n",
      "|    reward             | 0.0932186 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 0.884     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 76       |\n",
      "|    iterations         | 7400     |\n",
      "|    time_elapsed       | 482      |\n",
      "|    total_timesteps    | 37000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13.3    |\n",
      "|    explained_variance | -0.0177  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7399     |\n",
      "|    policy_loss        | -42.8    |\n",
      "|    reward             | 1.28519  |\n",
      "|    std                | 1.06     |\n",
      "|    value_loss         | 18.1     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 7500      |\n",
      "|    time_elapsed       | 488       |\n",
      "|    total_timesteps    | 37500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7499      |\n",
      "|    policy_loss        | -4.92     |\n",
      "|    reward             | 1.8438351 |\n",
      "|    std                | 1.07      |\n",
      "|    value_loss         | 0.186     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 76       |\n",
      "|    iterations         | 7600     |\n",
      "|    time_elapsed       | 494      |\n",
      "|    total_timesteps    | 38000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13.3    |\n",
      "|    explained_variance | -0.00279 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7599     |\n",
      "|    policy_loss        | 16.8     |\n",
      "|    reward             | 4.091442 |\n",
      "|    std                | 1.06     |\n",
      "|    value_loss         | 2.96     |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 76          |\n",
      "|    iterations         | 7700        |\n",
      "|    time_elapsed       | 501         |\n",
      "|    total_timesteps    | 38500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.3       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 7699        |\n",
      "|    policy_loss        | 5.31        |\n",
      "|    reward             | 0.014818347 |\n",
      "|    std                | 1.07        |\n",
      "|    value_loss         | 0.167       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 7800       |\n",
      "|    time_elapsed       | 508        |\n",
      "|    total_timesteps    | 39000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.3      |\n",
      "|    explained_variance | -0.0659    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7799       |\n",
      "|    policy_loss        | 15.9       |\n",
      "|    reward             | -0.4752591 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 2.19       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 7900       |\n",
      "|    time_elapsed       | 514        |\n",
      "|    total_timesteps    | 39500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7899       |\n",
      "|    policy_loss        | -44.1      |\n",
      "|    reward             | 0.87171596 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 14.7       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 8000       |\n",
      "|    time_elapsed       | 520        |\n",
      "|    total_timesteps    | 40000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.3      |\n",
      "|    explained_variance | 0.00201    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7999       |\n",
      "|    policy_loss        | -28.1      |\n",
      "|    reward             | 0.32467663 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 4.63       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 76       |\n",
      "|    iterations         | 8100     |\n",
      "|    time_elapsed       | 527      |\n",
      "|    total_timesteps    | 40500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13.4    |\n",
      "|    explained_variance | -0.00061 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8099     |\n",
      "|    policy_loss        | -73.7    |\n",
      "|    reward             | 6.432984 |\n",
      "|    std                | 1.07     |\n",
      "|    value_loss         | 46.1     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 8200       |\n",
      "|    time_elapsed       | 533        |\n",
      "|    total_timesteps    | 41000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.4      |\n",
      "|    explained_variance | 0.000939   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8199       |\n",
      "|    policy_loss        | -503       |\n",
      "|    reward             | -14.373388 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 1.91e+03   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 8300       |\n",
      "|    time_elapsed       | 540        |\n",
      "|    total_timesteps    | 41500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8299       |\n",
      "|    policy_loss        | -26.3      |\n",
      "|    reward             | -1.2684872 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 6.27       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 8400       |\n",
      "|    time_elapsed       | 546        |\n",
      "|    total_timesteps    | 42000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.4      |\n",
      "|    explained_variance | 0.0323     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8399       |\n",
      "|    policy_loss        | -32.6      |\n",
      "|    reward             | 0.10150606 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 7.48       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 8500       |\n",
      "|    time_elapsed       | 553        |\n",
      "|    total_timesteps    | 42500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.4      |\n",
      "|    explained_variance | -0.195     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8499       |\n",
      "|    policy_loss        | -3.51      |\n",
      "|    reward             | 0.74243253 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 0.72       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 76       |\n",
      "|    iterations         | 8600     |\n",
      "|    time_elapsed       | 561      |\n",
      "|    total_timesteps    | 43000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13.4    |\n",
      "|    explained_variance | 0.142    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8599     |\n",
      "|    policy_loss        | -2.37    |\n",
      "|    reward             | 1.757238 |\n",
      "|    std                | 1.07     |\n",
      "|    value_loss         | 0.354    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 8700      |\n",
      "|    time_elapsed       | 570       |\n",
      "|    total_timesteps    | 43500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.4     |\n",
      "|    explained_variance | -0.147    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8699      |\n",
      "|    policy_loss        | -17.2     |\n",
      "|    reward             | 2.6634753 |\n",
      "|    std                | 1.07      |\n",
      "|    value_loss         | 4.67      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 8800       |\n",
      "|    time_elapsed       | 579        |\n",
      "|    total_timesteps    | 44000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.4      |\n",
      "|    explained_variance | 0.129      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8799       |\n",
      "|    policy_loss        | 117        |\n",
      "|    reward             | -0.8832799 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 71         |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 75        |\n",
      "|    iterations         | 8900      |\n",
      "|    time_elapsed       | 586       |\n",
      "|    total_timesteps    | 44500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.4     |\n",
      "|    explained_variance | -2.59     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8899      |\n",
      "|    policy_loss        | 69        |\n",
      "|    reward             | 2.0950553 |\n",
      "|    std                | 1.07      |\n",
      "|    value_loss         | 28        |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 75        |\n",
      "|    iterations         | 9000      |\n",
      "|    time_elapsed       | 595       |\n",
      "|    total_timesteps    | 45000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8999      |\n",
      "|    policy_loss        | 19        |\n",
      "|    reward             | -0.284769 |\n",
      "|    std                | 1.07      |\n",
      "|    value_loss         | 2.42      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 9100       |\n",
      "|    time_elapsed       | 604        |\n",
      "|    total_timesteps    | 45500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.4      |\n",
      "|    explained_variance | 0.0556     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9099       |\n",
      "|    policy_loss        | 12.9       |\n",
      "|    reward             | 0.17491302 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 1.13       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 74        |\n",
      "|    iterations         | 9200      |\n",
      "|    time_elapsed       | 615       |\n",
      "|    total_timesteps    | 46000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.4     |\n",
      "|    explained_variance | -0.0015   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9199      |\n",
      "|    policy_loss        | 33.4      |\n",
      "|    reward             | 0.9432962 |\n",
      "|    std                | 1.07      |\n",
      "|    value_loss         | 7.44      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 74        |\n",
      "|    iterations         | 9300      |\n",
      "|    time_elapsed       | 626       |\n",
      "|    total_timesteps    | 46500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.4     |\n",
      "|    explained_variance | -0.0116   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9299      |\n",
      "|    policy_loss        | 42.2      |\n",
      "|    reward             | 2.9428964 |\n",
      "|    std                | 1.07      |\n",
      "|    value_loss         | 10.1      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 74        |\n",
      "|    iterations         | 9400      |\n",
      "|    time_elapsed       | 632       |\n",
      "|    total_timesteps    | 47000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9399      |\n",
      "|    policy_loss        | 47.8      |\n",
      "|    reward             | 4.6937943 |\n",
      "|    std                | 1.07      |\n",
      "|    value_loss         | 40        |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 74         |\n",
      "|    iterations         | 9500       |\n",
      "|    time_elapsed       | 639        |\n",
      "|    total_timesteps    | 47500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9499       |\n",
      "|    policy_loss        | 11.8       |\n",
      "|    reward             | 0.48416367 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 1.39       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 74         |\n",
      "|    iterations         | 9600       |\n",
      "|    time_elapsed       | 646        |\n",
      "|    total_timesteps    | 48000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.4      |\n",
      "|    explained_variance | 0.352      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9599       |\n",
      "|    policy_loss        | -1.75      |\n",
      "|    reward             | -1.4516454 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 0.508      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 74         |\n",
      "|    iterations         | 9700       |\n",
      "|    time_elapsed       | 653        |\n",
      "|    total_timesteps    | 48500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.4      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9699       |\n",
      "|    policy_loss        | 3.69       |\n",
      "|    reward             | -1.0596128 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 0.443      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 74          |\n",
      "|    iterations         | 9800        |\n",
      "|    time_elapsed       | 660         |\n",
      "|    total_timesteps    | 49000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.4       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 9799        |\n",
      "|    policy_loss        | -96.4       |\n",
      "|    reward             | -0.74707675 |\n",
      "|    std                | 1.08        |\n",
      "|    value_loss         | 49.9        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 74          |\n",
      "|    iterations         | 9900        |\n",
      "|    time_elapsed       | 666         |\n",
      "|    total_timesteps    | 49500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.4       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 9899        |\n",
      "|    policy_loss        | -30.7       |\n",
      "|    reward             | -0.27072978 |\n",
      "|    std                | 1.08        |\n",
      "|    value_loss         | 5.89        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 74         |\n",
      "|    iterations         | 10000      |\n",
      "|    time_elapsed       | 673        |\n",
      "|    total_timesteps    | 50000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9999       |\n",
      "|    policy_loss        | 86.2       |\n",
      "|    reward             | -16.336918 |\n",
      "|    std                | 1.08       |\n",
      "|    value_loss         | 51.6       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 74        |\n",
      "|    iterations         | 10100     |\n",
      "|    time_elapsed       | 680       |\n",
      "|    total_timesteps    | 50500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.4     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 10099     |\n",
      "|    policy_loss        | 1.54      |\n",
      "|    reward             | 1.2754896 |\n",
      "|    std                | 1.08      |\n",
      "|    value_loss         | 0.614     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 74        |\n",
      "|    iterations         | 10200     |\n",
      "|    time_elapsed       | 687       |\n",
      "|    total_timesteps    | 51000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 10199     |\n",
      "|    policy_loss        | 12.6      |\n",
      "|    reward             | 0.7993743 |\n",
      "|    std                | 1.08      |\n",
      "|    value_loss         | 1.79      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 74        |\n",
      "|    iterations         | 10300     |\n",
      "|    time_elapsed       | 693       |\n",
      "|    total_timesteps    | 51500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 10299     |\n",
      "|    policy_loss        | -14.4     |\n",
      "|    reward             | 0.5821072 |\n",
      "|    std                | 1.08      |\n",
      "|    value_loss         | 1.45      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 74         |\n",
      "|    iterations         | 10400      |\n",
      "|    time_elapsed       | 700        |\n",
      "|    total_timesteps    | 52000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 10399      |\n",
      "|    policy_loss        | 10.8       |\n",
      "|    reward             | 0.15018807 |\n",
      "|    std                | 1.08       |\n",
      "|    value_loss         | 5.17       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 74        |\n",
      "|    iterations         | 10500     |\n",
      "|    time_elapsed       | 707       |\n",
      "|    total_timesteps    | 52500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 10499     |\n",
      "|    policy_loss        | 8.9       |\n",
      "|    reward             | -7.480763 |\n",
      "|    std                | 1.08      |\n",
      "|    value_loss         | 8.8       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 74        |\n",
      "|    iterations         | 10600     |\n",
      "|    time_elapsed       | 713       |\n",
      "|    total_timesteps    | 53000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.4     |\n",
      "|    explained_variance | -0.00397  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 10599     |\n",
      "|    policy_loss        | -237      |\n",
      "|    reward             | 0.9093465 |\n",
      "|    std                | 1.08      |\n",
      "|    value_loss         | 323       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 74         |\n",
      "|    iterations         | 10700      |\n",
      "|    time_elapsed       | 721        |\n",
      "|    total_timesteps    | 53500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.4      |\n",
      "|    explained_variance | 0.0115     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 10699      |\n",
      "|    policy_loss        | 7.68       |\n",
      "|    reward             | 0.39078203 |\n",
      "|    std                | 1.08       |\n",
      "|    value_loss         | 0.661      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 74          |\n",
      "|    iterations         | 10800       |\n",
      "|    time_elapsed       | 728         |\n",
      "|    total_timesteps    | 54000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.4       |\n",
      "|    explained_variance | -0.913      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 10799       |\n",
      "|    policy_loss        | 7.81        |\n",
      "|    reward             | -0.17817038 |\n",
      "|    std                | 1.08        |\n",
      "|    value_loss         | 0.795       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 74         |\n",
      "|    iterations         | 10900      |\n",
      "|    time_elapsed       | 734        |\n",
      "|    total_timesteps    | 54500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.4      |\n",
      "|    explained_variance | -0.139     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 10899      |\n",
      "|    policy_loss        | 18.7       |\n",
      "|    reward             | -2.3132646 |\n",
      "|    std                | 1.08       |\n",
      "|    value_loss         | 2.92       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 74        |\n",
      "|    iterations         | 11000     |\n",
      "|    time_elapsed       | 741       |\n",
      "|    total_timesteps    | 55000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 10999     |\n",
      "|    policy_loss        | -11       |\n",
      "|    reward             | 1.1366616 |\n",
      "|    std                | 1.08      |\n",
      "|    value_loss         | 1.14      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 74         |\n",
      "|    iterations         | 11100      |\n",
      "|    time_elapsed       | 749        |\n",
      "|    total_timesteps    | 55500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.4      |\n",
      "|    explained_variance | 0.0134     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 11099      |\n",
      "|    policy_loss        | 70.4       |\n",
      "|    reward             | -0.2628408 |\n",
      "|    std                | 1.08       |\n",
      "|    value_loss         | 39.9       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 74        |\n",
      "|    iterations         | 11200     |\n",
      "|    time_elapsed       | 756       |\n",
      "|    total_timesteps    | 56000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.4     |\n",
      "|    explained_variance | 0.0419    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 11199     |\n",
      "|    policy_loss        | -50.5     |\n",
      "|    reward             | -4.139358 |\n",
      "|    std                | 1.08      |\n",
      "|    value_loss         | 18.1      |\n",
      "-------------------------------------\n",
      "day: 2956, episode: 20\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4458784.57\n",
      "total_reward: 3458784.57\n",
      "total_cost: 52099.20\n",
      "total_trades: 21583\n",
      "Sharpe: 0.858\n",
      "=================================\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 74         |\n",
      "|    iterations         | 11300      |\n",
      "|    time_elapsed       | 763        |\n",
      "|    total_timesteps    | 56500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.5      |\n",
      "|    explained_variance | -0.36      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 11299      |\n",
      "|    policy_loss        | 17.4       |\n",
      "|    reward             | 0.19141881 |\n",
      "|    std                | 1.08       |\n",
      "|    value_loss         | 2.14       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 74        |\n",
      "|    iterations         | 11400     |\n",
      "|    time_elapsed       | 769       |\n",
      "|    total_timesteps    | 57000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 11399     |\n",
      "|    policy_loss        | 7.7       |\n",
      "|    reward             | 0.5442897 |\n",
      "|    std                | 1.08      |\n",
      "|    value_loss         | 0.624     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 74        |\n",
      "|    iterations         | 11500     |\n",
      "|    time_elapsed       | 776       |\n",
      "|    total_timesteps    | 57500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.4     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 11499     |\n",
      "|    policy_loss        | 18.4      |\n",
      "|    reward             | 2.3308363 |\n",
      "|    std                | 1.08      |\n",
      "|    value_loss         | 3.43      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 74          |\n",
      "|    iterations         | 11600       |\n",
      "|    time_elapsed       | 783         |\n",
      "|    total_timesteps    | 58000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.4       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 11599       |\n",
      "|    policy_loss        | 8.15        |\n",
      "|    reward             | -0.69626623 |\n",
      "|    std                | 1.08        |\n",
      "|    value_loss         | 1.69        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 74         |\n",
      "|    iterations         | 11700      |\n",
      "|    time_elapsed       | 790        |\n",
      "|    total_timesteps    | 58500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.4      |\n",
      "|    explained_variance | -0.335     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 11699      |\n",
      "|    policy_loss        | 35.9       |\n",
      "|    reward             | 0.34773564 |\n",
      "|    std                | 1.08       |\n",
      "|    value_loss         | 10.4       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 74        |\n",
      "|    iterations         | 11800     |\n",
      "|    time_elapsed       | 797       |\n",
      "|    total_timesteps    | 59000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.4     |\n",
      "|    explained_variance | -0.0192   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 11799     |\n",
      "|    policy_loss        | -31.5     |\n",
      "|    reward             | 1.9770246 |\n",
      "|    std                | 1.08      |\n",
      "|    value_loss         | 18.6      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 74          |\n",
      "|    iterations         | 11900       |\n",
      "|    time_elapsed       | 803         |\n",
      "|    total_timesteps    | 59500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.4       |\n",
      "|    explained_variance | -2.24       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 11899       |\n",
      "|    policy_loss        | -8.35       |\n",
      "|    reward             | 0.010328789 |\n",
      "|    std                | 1.08        |\n",
      "|    value_loss         | 2.04        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 74         |\n",
      "|    iterations         | 12000      |\n",
      "|    time_elapsed       | 810        |\n",
      "|    total_timesteps    | 60000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.5      |\n",
      "|    explained_variance | -0.0449    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 11999      |\n",
      "|    policy_loss        | -29.2      |\n",
      "|    reward             | -1.8221413 |\n",
      "|    std                | 1.08       |\n",
      "|    value_loss         | 5.35       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 74        |\n",
      "|    iterations         | 12100     |\n",
      "|    time_elapsed       | 817       |\n",
      "|    total_timesteps    | 60500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.5     |\n",
      "|    explained_variance | 0.246     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 12099     |\n",
      "|    policy_loss        | -7.52     |\n",
      "|    reward             | 0.3691487 |\n",
      "|    std                | 1.09      |\n",
      "|    value_loss         | 0.3       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 73        |\n",
      "|    iterations         | 12200     |\n",
      "|    time_elapsed       | 825       |\n",
      "|    total_timesteps    | 61000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.5     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 12199     |\n",
      "|    policy_loss        | -58.2     |\n",
      "|    reward             | 0.7408714 |\n",
      "|    std                | 1.09      |\n",
      "|    value_loss         | 16.5      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 73        |\n",
      "|    iterations         | 12300     |\n",
      "|    time_elapsed       | 832       |\n",
      "|    total_timesteps    | 61500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.5     |\n",
      "|    explained_variance | -0.0679   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 12299     |\n",
      "|    policy_loss        | -38.4     |\n",
      "|    reward             | 2.4904718 |\n",
      "|    std                | 1.09      |\n",
      "|    value_loss         | 16.5      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 73       |\n",
      "|    iterations         | 12400    |\n",
      "|    time_elapsed       | 839      |\n",
      "|    total_timesteps    | 62000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13.5    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 12399    |\n",
      "|    policy_loss        | 77.3     |\n",
      "|    reward             | 7.160681 |\n",
      "|    std                | 1.09     |\n",
      "|    value_loss         | 42.1     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 73       |\n",
      "|    iterations         | 12500    |\n",
      "|    time_elapsed       | 845      |\n",
      "|    total_timesteps    | 62500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13.5    |\n",
      "|    explained_variance | -0.0112  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 12499    |\n",
      "|    policy_loss        | -63.8    |\n",
      "|    reward             | 5.464859 |\n",
      "|    std                | 1.09     |\n",
      "|    value_loss         | 26.5     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 73        |\n",
      "|    iterations         | 12600     |\n",
      "|    time_elapsed       | 852       |\n",
      "|    total_timesteps    | 63000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.5     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 12599     |\n",
      "|    policy_loss        | -2.71     |\n",
      "|    reward             | -1.084931 |\n",
      "|    std                | 1.09      |\n",
      "|    value_loss         | 0.0733    |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 73          |\n",
      "|    iterations         | 12700       |\n",
      "|    time_elapsed       | 860         |\n",
      "|    total_timesteps    | 63500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.5       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 12699       |\n",
      "|    policy_loss        | -66.5       |\n",
      "|    reward             | -0.57665455 |\n",
      "|    std                | 1.09        |\n",
      "|    value_loss         | 28.7        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 73          |\n",
      "|    iterations         | 12800       |\n",
      "|    time_elapsed       | 867         |\n",
      "|    total_timesteps    | 64000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.5       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 12799       |\n",
      "|    policy_loss        | 23.6        |\n",
      "|    reward             | -0.12770781 |\n",
      "|    std                | 1.09        |\n",
      "|    value_loss         | 3.61        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 73        |\n",
      "|    iterations         | 12900     |\n",
      "|    time_elapsed       | 873       |\n",
      "|    total_timesteps    | 64500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 12899     |\n",
      "|    policy_loss        | 3.87      |\n",
      "|    reward             | 2.6668103 |\n",
      "|    std                | 1.09      |\n",
      "|    value_loss         | 3.54      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 73         |\n",
      "|    iterations         | 13000      |\n",
      "|    time_elapsed       | 880        |\n",
      "|    total_timesteps    | 65000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.5      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 12999      |\n",
      "|    policy_loss        | 53.6       |\n",
      "|    reward             | -5.7197857 |\n",
      "|    std                | 1.08       |\n",
      "|    value_loss         | 27.4       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 73        |\n",
      "|    iterations         | 13100     |\n",
      "|    time_elapsed       | 886       |\n",
      "|    total_timesteps    | 65500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.5     |\n",
      "|    explained_variance | 0.000903  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 13099     |\n",
      "|    policy_loss        | -82.8     |\n",
      "|    reward             | 3.5485928 |\n",
      "|    std                | 1.09      |\n",
      "|    value_loss         | 39.1      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 73         |\n",
      "|    iterations         | 13200      |\n",
      "|    time_elapsed       | 893        |\n",
      "|    total_timesteps    | 66000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.5      |\n",
      "|    explained_variance | -0.0156    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 13199      |\n",
      "|    policy_loss        | -32.6      |\n",
      "|    reward             | -0.9282165 |\n",
      "|    std                | 1.09       |\n",
      "|    value_loss         | 5.92       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 73         |\n",
      "|    iterations         | 13300      |\n",
      "|    time_elapsed       | 899        |\n",
      "|    total_timesteps    | 66500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.5      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 13299      |\n",
      "|    policy_loss        | -28        |\n",
      "|    reward             | 0.33554587 |\n",
      "|    std                | 1.09       |\n",
      "|    value_loss         | 8.1        |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 73          |\n",
      "|    iterations         | 13400       |\n",
      "|    time_elapsed       | 906         |\n",
      "|    total_timesteps    | 67000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.5       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 13399       |\n",
      "|    policy_loss        | 14          |\n",
      "|    reward             | 0.067825824 |\n",
      "|    std                | 1.09        |\n",
      "|    value_loss         | 1.5         |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 73         |\n",
      "|    iterations         | 13500      |\n",
      "|    time_elapsed       | 913        |\n",
      "|    total_timesteps    | 67500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.5      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 13499      |\n",
      "|    policy_loss        | 19.7       |\n",
      "|    reward             | 0.28325218 |\n",
      "|    std                | 1.09       |\n",
      "|    value_loss         | 2.55       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 73         |\n",
      "|    iterations         | 13600      |\n",
      "|    time_elapsed       | 921        |\n",
      "|    total_timesteps    | 68000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.5      |\n",
      "|    explained_variance | 7.15e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 13599      |\n",
      "|    policy_loss        | -27.5      |\n",
      "|    reward             | -1.5329349 |\n",
      "|    std                | 1.09       |\n",
      "|    value_loss         | 6.73       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 73        |\n",
      "|    iterations         | 13700     |\n",
      "|    time_elapsed       | 931       |\n",
      "|    total_timesteps    | 68500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.5     |\n",
      "|    explained_variance | -6.53e-05 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 13699     |\n",
      "|    policy_loss        | 31.9      |\n",
      "|    reward             | 1.5602993 |\n",
      "|    std                | 1.09      |\n",
      "|    value_loss         | 14.1      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 73       |\n",
      "|    iterations         | 13800    |\n",
      "|    time_elapsed       | 940      |\n",
      "|    total_timesteps    | 69000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13.5    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 13799    |\n",
      "|    policy_loss        | -9.08    |\n",
      "|    reward             | 1.823146 |\n",
      "|    std                | 1.08     |\n",
      "|    value_loss         | 0.555    |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 73         |\n",
      "|    iterations         | 13900      |\n",
      "|    time_elapsed       | 948        |\n",
      "|    total_timesteps    | 69500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.5      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 13899      |\n",
      "|    policy_loss        | 16.8       |\n",
      "|    reward             | -1.8821951 |\n",
      "|    std                | 1.09       |\n",
      "|    value_loss         | 2.38       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 73          |\n",
      "|    iterations         | 14000       |\n",
      "|    time_elapsed       | 957         |\n",
      "|    total_timesteps    | 70000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.5       |\n",
      "|    explained_variance | 0.13        |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 13999       |\n",
      "|    policy_loss        | -2.38       |\n",
      "|    reward             | -0.39691326 |\n",
      "|    std                | 1.09        |\n",
      "|    value_loss         | 0.819       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 72         |\n",
      "|    iterations         | 14100      |\n",
      "|    time_elapsed       | 966        |\n",
      "|    total_timesteps    | 70500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.5      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 14099      |\n",
      "|    policy_loss        | 7.41       |\n",
      "|    reward             | -0.6547193 |\n",
      "|    std                | 1.09       |\n",
      "|    value_loss         | 0.418      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 72         |\n",
      "|    iterations         | 14200      |\n",
      "|    time_elapsed       | 975        |\n",
      "|    total_timesteps    | 71000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.5      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 14199      |\n",
      "|    policy_loss        | 3.23       |\n",
      "|    reward             | 0.10133051 |\n",
      "|    std                | 1.09       |\n",
      "|    value_loss         | 0.0608     |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 72        |\n",
      "|    iterations         | 14300     |\n",
      "|    time_elapsed       | 984       |\n",
      "|    total_timesteps    | 71500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.5     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 14299     |\n",
      "|    policy_loss        | 14.7      |\n",
      "|    reward             | 0.6544616 |\n",
      "|    std                | 1.09      |\n",
      "|    value_loss         | 2.17      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 72         |\n",
      "|    iterations         | 14400      |\n",
      "|    time_elapsed       | 990        |\n",
      "|    total_timesteps    | 72000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.5      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 14399      |\n",
      "|    policy_loss        | -55.9      |\n",
      "|    reward             | 0.35214254 |\n",
      "|    std                | 1.09       |\n",
      "|    value_loss         | 15.4       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 72          |\n",
      "|    iterations         | 14500       |\n",
      "|    time_elapsed       | 997         |\n",
      "|    total_timesteps    | 72500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.5       |\n",
      "|    explained_variance | 0.0434      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 14499       |\n",
      "|    policy_loss        | 16.7        |\n",
      "|    reward             | -0.25473267 |\n",
      "|    std                | 1.09        |\n",
      "|    value_loss         | 4.35        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 72         |\n",
      "|    iterations         | 14600      |\n",
      "|    time_elapsed       | 1003       |\n",
      "|    total_timesteps    | 73000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.5      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 14599      |\n",
      "|    policy_loss        | 47.9       |\n",
      "|    reward             | -2.8120396 |\n",
      "|    std                | 1.09       |\n",
      "|    value_loss         | 17         |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 72        |\n",
      "|    iterations         | 14700     |\n",
      "|    time_elapsed       | 1010      |\n",
      "|    total_timesteps    | 73500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 14699     |\n",
      "|    policy_loss        | 63        |\n",
      "|    reward             | -7.416311 |\n",
      "|    std                | 1.09      |\n",
      "|    value_loss         | 25.2      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 72        |\n",
      "|    iterations         | 14800     |\n",
      "|    time_elapsed       | 1016      |\n",
      "|    total_timesteps    | 74000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 14799     |\n",
      "|    policy_loss        | 5.93      |\n",
      "|    reward             | 0.3046606 |\n",
      "|    std                | 1.09      |\n",
      "|    value_loss         | 0.195     |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 72          |\n",
      "|    iterations         | 14900       |\n",
      "|    time_elapsed       | 1023        |\n",
      "|    total_timesteps    | 74500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.5       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 14899       |\n",
      "|    policy_loss        | -37.1       |\n",
      "|    reward             | -0.16600567 |\n",
      "|    std                | 1.09        |\n",
      "|    value_loss         | 9.33        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 72        |\n",
      "|    iterations         | 15000     |\n",
      "|    time_elapsed       | 1029      |\n",
      "|    total_timesteps    | 75000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 14999     |\n",
      "|    policy_loss        | -28.3     |\n",
      "|    reward             | -1.844156 |\n",
      "|    std                | 1.09      |\n",
      "|    value_loss         | 5.4       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 72        |\n",
      "|    iterations         | 15100     |\n",
      "|    time_elapsed       | 1036      |\n",
      "|    total_timesteps    | 75500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15099     |\n",
      "|    policy_loss        | 9.48      |\n",
      "|    reward             | -2.830651 |\n",
      "|    std                | 1.09      |\n",
      "|    value_loss         | 1.71      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 71        |\n",
      "|    iterations         | 15200     |\n",
      "|    time_elapsed       | 1061      |\n",
      "|    total_timesteps    | 76000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.5     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15199     |\n",
      "|    policy_loss        | -38       |\n",
      "|    reward             | 3.1360393 |\n",
      "|    std                | 1.09      |\n",
      "|    value_loss         | 46.3      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 71         |\n",
      "|    iterations         | 15300      |\n",
      "|    time_elapsed       | 1068       |\n",
      "|    total_timesteps    | 76500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.5      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 15299      |\n",
      "|    policy_loss        | -315       |\n",
      "|    reward             | -12.754807 |\n",
      "|    std                | 1.09       |\n",
      "|    value_loss         | 592        |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 71          |\n",
      "|    iterations         | 15400       |\n",
      "|    time_elapsed       | 1074        |\n",
      "|    total_timesteps    | 77000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.5       |\n",
      "|    explained_variance | -0.287      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 15399       |\n",
      "|    policy_loss        | 26.1        |\n",
      "|    reward             | -0.19104461 |\n",
      "|    std                | 1.08        |\n",
      "|    value_loss         | 6.02        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 71        |\n",
      "|    iterations         | 15500     |\n",
      "|    time_elapsed       | 1081      |\n",
      "|    total_timesteps    | 77500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.5     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15499     |\n",
      "|    policy_loss        | 37.3      |\n",
      "|    reward             | 1.2974149 |\n",
      "|    std                | 1.08      |\n",
      "|    value_loss         | 8.27      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 71        |\n",
      "|    iterations         | 15600     |\n",
      "|    time_elapsed       | 1087      |\n",
      "|    total_timesteps    | 78000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.5     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15599     |\n",
      "|    policy_loss        | 18.4      |\n",
      "|    reward             | 0.7760436 |\n",
      "|    std                | 1.09      |\n",
      "|    value_loss         | 2.36      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 71        |\n",
      "|    iterations         | 15700     |\n",
      "|    time_elapsed       | 1094      |\n",
      "|    total_timesteps    | 78500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15699     |\n",
      "|    policy_loss        | -2.43     |\n",
      "|    reward             | 0.6777496 |\n",
      "|    std                | 1.09      |\n",
      "|    value_loss         | 0.33      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 71        |\n",
      "|    iterations         | 15800     |\n",
      "|    time_elapsed       | 1100      |\n",
      "|    total_timesteps    | 79000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15799     |\n",
      "|    policy_loss        | -11.3     |\n",
      "|    reward             | 1.9473292 |\n",
      "|    std                | 1.09      |\n",
      "|    value_loss         | 1.27      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 71        |\n",
      "|    iterations         | 15900     |\n",
      "|    time_elapsed       | 1107      |\n",
      "|    total_timesteps    | 79500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15899     |\n",
      "|    policy_loss        | 50.1      |\n",
      "|    reward             | 4.1360235 |\n",
      "|    std                | 1.09      |\n",
      "|    value_loss         | 53        |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 71         |\n",
      "|    iterations         | 16000      |\n",
      "|    time_elapsed       | 1113       |\n",
      "|    total_timesteps    | 80000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.6      |\n",
      "|    explained_variance | 1.79e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 15999      |\n",
      "|    policy_loss        | 2.87       |\n",
      "|    reward             | -1.5640818 |\n",
      "|    std                | 1.1        |\n",
      "|    value_loss         | 2.7        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 71        |\n",
      "|    iterations         | 16100     |\n",
      "|    time_elapsed       | 1121      |\n",
      "|    total_timesteps    | 80500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16099     |\n",
      "|    policy_loss        | 19.9      |\n",
      "|    reward             | 1.6022257 |\n",
      "|    std                | 1.1       |\n",
      "|    value_loss         | 3.16      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 71         |\n",
      "|    iterations         | 16200      |\n",
      "|    time_elapsed       | 1127       |\n",
      "|    total_timesteps    | 81000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.6      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 16199      |\n",
      "|    policy_loss        | -26.1      |\n",
      "|    reward             | 0.46504804 |\n",
      "|    std                | 1.1        |\n",
      "|    value_loss         | 5.14       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 71         |\n",
      "|    iterations         | 16300      |\n",
      "|    time_elapsed       | 1134       |\n",
      "|    total_timesteps    | 81500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.6      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 16299      |\n",
      "|    policy_loss        | -19.6      |\n",
      "|    reward             | 0.29457837 |\n",
      "|    std                | 1.11       |\n",
      "|    value_loss         | 2.71       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 71        |\n",
      "|    iterations         | 16400     |\n",
      "|    time_elapsed       | 1140      |\n",
      "|    total_timesteps    | 82000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16399     |\n",
      "|    policy_loss        | -61.8     |\n",
      "|    reward             | 1.1850511 |\n",
      "|    std                | 1.1       |\n",
      "|    value_loss         | 24.4      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 71       |\n",
      "|    iterations         | 16500    |\n",
      "|    time_elapsed       | 1147     |\n",
      "|    total_timesteps    | 82500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13.6    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 16499    |\n",
      "|    policy_loss        | -75.1    |\n",
      "|    reward             | 1.067065 |\n",
      "|    std                | 1.1      |\n",
      "|    value_loss         | 52.4     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 71         |\n",
      "|    iterations         | 16600      |\n",
      "|    time_elapsed       | 1153       |\n",
      "|    total_timesteps    | 83000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.6      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 16599      |\n",
      "|    policy_loss        | 4.28       |\n",
      "|    reward             | 0.85624665 |\n",
      "|    std                | 1.1        |\n",
      "|    value_loss         | 0.288      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 71        |\n",
      "|    iterations         | 16700     |\n",
      "|    time_elapsed       | 1160      |\n",
      "|    total_timesteps    | 83500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16699     |\n",
      "|    policy_loss        | -43.7     |\n",
      "|    reward             | 0.3627367 |\n",
      "|    std                | 1.1       |\n",
      "|    value_loss         | 8.8       |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 72          |\n",
      "|    iterations         | 16800       |\n",
      "|    time_elapsed       | 1166        |\n",
      "|    total_timesteps    | 84000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.6       |\n",
      "|    explained_variance | 5.36e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 16799       |\n",
      "|    policy_loss        | 5.11        |\n",
      "|    reward             | -0.86546344 |\n",
      "|    std                | 1.1         |\n",
      "|    value_loss         | 1.97        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 72        |\n",
      "|    iterations         | 16900     |\n",
      "|    time_elapsed       | 1172      |\n",
      "|    total_timesteps    | 84500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.6     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16899     |\n",
      "|    policy_loss        | -2.99     |\n",
      "|    reward             | 1.2721897 |\n",
      "|    std                | 1.1       |\n",
      "|    value_loss         | 0.811     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 72         |\n",
      "|    iterations         | 17000      |\n",
      "|    time_elapsed       | 1180       |\n",
      "|    total_timesteps    | 85000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.6      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 16999      |\n",
      "|    policy_loss        | -24.6      |\n",
      "|    reward             | -4.1125283 |\n",
      "|    std                | 1.1        |\n",
      "|    value_loss         | 6.02       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 72       |\n",
      "|    iterations         | 17100    |\n",
      "|    time_elapsed       | 1186     |\n",
      "|    total_timesteps    | 85500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13.6    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 17099    |\n",
      "|    policy_loss        | -172     |\n",
      "|    reward             | 4.177122 |\n",
      "|    std                | 1.1      |\n",
      "|    value_loss         | 183      |\n",
      "------------------------------------\n",
      "day: 2956, episode: 30\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 6213190.53\n",
      "total_reward: 5213190.53\n",
      "total_cost: 4359.24\n",
      "total_trades: 15911\n",
      "Sharpe: 0.895\n",
      "=================================\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 72         |\n",
      "|    iterations         | 17200      |\n",
      "|    time_elapsed       | 1193       |\n",
      "|    total_timesteps    | 86000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.7      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 17199      |\n",
      "|    policy_loss        | 0.0726     |\n",
      "|    reward             | 0.30797976 |\n",
      "|    std                | 1.11       |\n",
      "|    value_loss         | 0.183      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 72        |\n",
      "|    iterations         | 17300     |\n",
      "|    time_elapsed       | 1199      |\n",
      "|    total_timesteps    | 86500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17299     |\n",
      "|    policy_loss        | -6.57     |\n",
      "|    reward             | 0.7975358 |\n",
      "|    std                | 1.11      |\n",
      "|    value_loss         | 0.442     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 72         |\n",
      "|    iterations         | 17400      |\n",
      "|    time_elapsed       | 1205       |\n",
      "|    total_timesteps    | 87000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 17399      |\n",
      "|    policy_loss        | -21.9      |\n",
      "|    reward             | -1.9660933 |\n",
      "|    std                | 1.11       |\n",
      "|    value_loss         | 3.38       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 72        |\n",
      "|    iterations         | 17500     |\n",
      "|    time_elapsed       | 1212      |\n",
      "|    total_timesteps    | 87500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.7     |\n",
      "|    explained_variance | 5.69e-05  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17499     |\n",
      "|    policy_loss        | -34.4     |\n",
      "|    reward             | 1.0930126 |\n",
      "|    std                | 1.11      |\n",
      "|    value_loss         | 9.02      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 72         |\n",
      "|    iterations         | 17600      |\n",
      "|    time_elapsed       | 1219       |\n",
      "|    total_timesteps    | 88000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 17599      |\n",
      "|    policy_loss        | 138        |\n",
      "|    reward             | -0.6632637 |\n",
      "|    std                | 1.12       |\n",
      "|    value_loss         | 114        |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 72       |\n",
      "|    iterations         | 17700    |\n",
      "|    time_elapsed       | 1225     |\n",
      "|    total_timesteps    | 88500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13.7    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 17699    |\n",
      "|    policy_loss        | -52.3    |\n",
      "|    reward             | 5.847029 |\n",
      "|    std                | 1.11     |\n",
      "|    value_loss         | 20.8     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 72         |\n",
      "|    iterations         | 17800      |\n",
      "|    time_elapsed       | 1232       |\n",
      "|    total_timesteps    | 89000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 17799      |\n",
      "|    policy_loss        | -3.99      |\n",
      "|    reward             | 0.77687794 |\n",
      "|    std                | 1.12       |\n",
      "|    value_loss         | 0.175      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 72         |\n",
      "|    iterations         | 17900      |\n",
      "|    time_elapsed       | 1238       |\n",
      "|    total_timesteps    | 89500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 17899      |\n",
      "|    policy_loss        | -7.06      |\n",
      "|    reward             | -2.5702617 |\n",
      "|    std                | 1.11       |\n",
      "|    value_loss         | 0.249      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 72         |\n",
      "|    iterations         | 18000      |\n",
      "|    time_elapsed       | 1245       |\n",
      "|    total_timesteps    | 90000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 17999      |\n",
      "|    policy_loss        | 17         |\n",
      "|    reward             | 0.41815573 |\n",
      "|    std                | 1.12       |\n",
      "|    value_loss         | 2.28       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 72        |\n",
      "|    iterations         | 18100     |\n",
      "|    time_elapsed       | 1251      |\n",
      "|    total_timesteps    | 90500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.8     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18099     |\n",
      "|    policy_loss        | 11.2      |\n",
      "|    reward             | 1.7320381 |\n",
      "|    std                | 1.12      |\n",
      "|    value_loss         | 1.2       |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 72       |\n",
      "|    iterations         | 18200    |\n",
      "|    time_elapsed       | 1258     |\n",
      "|    total_timesteps    | 91000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13.8    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 18199    |\n",
      "|    policy_loss        | 67.3     |\n",
      "|    reward             | 1.143579 |\n",
      "|    std                | 1.12     |\n",
      "|    value_loss         | 28       |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 72         |\n",
      "|    iterations         | 18300      |\n",
      "|    time_elapsed       | 1264       |\n",
      "|    total_timesteps    | 91500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 18299      |\n",
      "|    policy_loss        | -151       |\n",
      "|    reward             | -0.9927367 |\n",
      "|    std                | 1.12       |\n",
      "|    value_loss         | 126        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 72        |\n",
      "|    iterations         | 18400     |\n",
      "|    time_elapsed       | 1270      |\n",
      "|    total_timesteps    | 92000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.8     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18399     |\n",
      "|    policy_loss        | 18.3      |\n",
      "|    reward             | 0.2695268 |\n",
      "|    std                | 1.12      |\n",
      "|    value_loss         | 2.07      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 72         |\n",
      "|    iterations         | 18500      |\n",
      "|    time_elapsed       | 1277       |\n",
      "|    total_timesteps    | 92500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.8      |\n",
      "|    explained_variance | -0.000923  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 18499      |\n",
      "|    policy_loss        | -24.7      |\n",
      "|    reward             | -0.2537322 |\n",
      "|    std                | 1.12       |\n",
      "|    value_loss         | 6.64       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 72        |\n",
      "|    iterations         | 18600     |\n",
      "|    time_elapsed       | 1283      |\n",
      "|    total_timesteps    | 93000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.8     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18599     |\n",
      "|    policy_loss        | 2.56      |\n",
      "|    reward             | 1.2824944 |\n",
      "|    std                | 1.12      |\n",
      "|    value_loss         | 0.414     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 72        |\n",
      "|    iterations         | 18700     |\n",
      "|    time_elapsed       | 1290      |\n",
      "|    total_timesteps    | 93500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.8     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18699     |\n",
      "|    policy_loss        | -11.4     |\n",
      "|    reward             | 2.4640975 |\n",
      "|    std                | 1.13      |\n",
      "|    value_loss         | 0.923     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 72        |\n",
      "|    iterations         | 18800     |\n",
      "|    time_elapsed       | 1296      |\n",
      "|    total_timesteps    | 94000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.9     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18799     |\n",
      "|    policy_loss        | 37.7      |\n",
      "|    reward             | 0.8079894 |\n",
      "|    std                | 1.13      |\n",
      "|    value_loss         | 17.7      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 72        |\n",
      "|    iterations         | 18900     |\n",
      "|    time_elapsed       | 1303      |\n",
      "|    total_timesteps    | 94500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.9     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18899     |\n",
      "|    policy_loss        | 31.3      |\n",
      "|    reward             | 1.0882292 |\n",
      "|    std                | 1.13      |\n",
      "|    value_loss         | 36.9      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 72        |\n",
      "|    iterations         | 19000     |\n",
      "|    time_elapsed       | 1309      |\n",
      "|    total_timesteps    | 95000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.8     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18999     |\n",
      "|    policy_loss        | -4.17     |\n",
      "|    reward             | 1.3636538 |\n",
      "|    std                | 1.13      |\n",
      "|    value_loss         | 0.477     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 72        |\n",
      "|    iterations         | 19100     |\n",
      "|    time_elapsed       | 1315      |\n",
      "|    total_timesteps    | 95500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.9     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19099     |\n",
      "|    policy_loss        | -62.8     |\n",
      "|    reward             | 0.9692339 |\n",
      "|    std                | 1.13      |\n",
      "|    value_loss         | 25.2      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 72          |\n",
      "|    iterations         | 19200       |\n",
      "|    time_elapsed       | 1322        |\n",
      "|    total_timesteps    | 96000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.9       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 19199       |\n",
      "|    policy_loss        | -11.5       |\n",
      "|    reward             | -0.24094844 |\n",
      "|    std                | 1.14        |\n",
      "|    value_loss         | 2.45        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 72          |\n",
      "|    iterations         | 19300       |\n",
      "|    time_elapsed       | 1328        |\n",
      "|    total_timesteps    | 96500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.9       |\n",
      "|    explained_variance | 5.96e-08    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 19299       |\n",
      "|    policy_loss        | -46.9       |\n",
      "|    reward             | -0.21430701 |\n",
      "|    std                | 1.13        |\n",
      "|    value_loss         | 13.8        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 72        |\n",
      "|    iterations         | 19400     |\n",
      "|    time_elapsed       | 1334      |\n",
      "|    total_timesteps    | 97000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.9     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19399     |\n",
      "|    policy_loss        | 115       |\n",
      "|    reward             | 1.0443178 |\n",
      "|    std                | 1.14      |\n",
      "|    value_loss         | 93.8      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 72         |\n",
      "|    iterations         | 19500      |\n",
      "|    time_elapsed       | 1341       |\n",
      "|    total_timesteps    | 97500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.9      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 19499      |\n",
      "|    policy_loss        | -21.8      |\n",
      "|    reward             | -0.7144671 |\n",
      "|    std                | 1.14       |\n",
      "|    value_loss         | 3.2        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 72         |\n",
      "|    iterations         | 19600      |\n",
      "|    time_elapsed       | 1347       |\n",
      "|    total_timesteps    | 98000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.9      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 19599      |\n",
      "|    policy_loss        | 27.9       |\n",
      "|    reward             | 0.44056746 |\n",
      "|    std                | 1.14       |\n",
      "|    value_loss         | 10.7       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 72         |\n",
      "|    iterations         | 19700      |\n",
      "|    time_elapsed       | 1353       |\n",
      "|    total_timesteps    | 98500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -14        |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 19699      |\n",
      "|    policy_loss        | -29.3      |\n",
      "|    reward             | 0.15702532 |\n",
      "|    std                | 1.15       |\n",
      "|    value_loss         | 6          |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 72         |\n",
      "|    iterations         | 19800      |\n",
      "|    time_elapsed       | 1360       |\n",
      "|    total_timesteps    | 99000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.9      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 19799      |\n",
      "|    policy_loss        | 13.7       |\n",
      "|    reward             | -6.9234395 |\n",
      "|    std                | 1.14       |\n",
      "|    value_loss         | 2.08       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 72        |\n",
      "|    iterations         | 19900     |\n",
      "|    time_elapsed       | 1366      |\n",
      "|    total_timesteps    | 99500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.9     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19899     |\n",
      "|    policy_loss        | -60       |\n",
      "|    reward             | -4.663621 |\n",
      "|    std                | 1.14      |\n",
      "|    value_loss         | 19.3      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 72        |\n",
      "|    iterations         | 20000     |\n",
      "|    time_elapsed       | 1372      |\n",
      "|    total_timesteps    | 100000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.9     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19999     |\n",
      "|    policy_loss        | -63.9     |\n",
      "|    reward             | -9.792703 |\n",
      "|    std                | 1.14      |\n",
      "|    value_loss         | 67.7      |\n",
      "-------------------------------------\n",
      "hit end!\n",
      "==============Get Baseline Stats===========\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Shape of DataFrame:  (400, 8)\n",
      "Annual return         -0.022145\n",
      "Cumulative returns    -0.034921\n",
      "Annual volatility      0.176822\n",
      "Sharpe ratio          -0.038668\n",
      "Calmar ratio          -0.100930\n",
      "Stability              0.215672\n",
      "Max drawdown          -0.219408\n",
      "Omega ratio            0.993578\n",
      "Sortino ratio         -0.053987\n",
      "Skew                        NaN\n",
      "Kurtosis                    NaN\n",
      "Tail ratio             1.003436\n",
      "Daily value at risk   -0.022305\n",
      "dtype: float64\n",
      "result:                       a2c           dji\n",
      "date                                  \n",
      "2021-10-01  1.000000e+06  1.000000e+06\n",
      "2021-10-04  9.995433e+05  9.905746e+05\n",
      "2021-10-05  1.000444e+06  9.996566e+05\n",
      "2021-10-06  1.001233e+06  1.002637e+06\n",
      "2021-10-07  1.002522e+06  1.012483e+06\n",
      "...                  ...           ...\n",
      "2023-04-27  1.000175e+06  9.854252e+05\n",
      "2023-04-28  1.009115e+06  9.933491e+05\n",
      "2023-05-01  1.010679e+06  9.919956e+05\n",
      "2023-05-02  1.001235e+06  9.812993e+05\n",
      "2023-05-03  9.940464e+05  9.734251e+05\n",
      "\n",
      "[399 rows x 2 columns]\n",
      "A2C로 얻은 투자 수익률>> -0.6 %\n",
      "{'n_steps': 5, 'ent_coef': 0.01, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to results/a2c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\se99a\\anaconda3\\lib\\site-packages\\pyfolio\\timeseries.py:724: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  stats = pd.Series()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 72         |\n",
      "|    iterations         | 100        |\n",
      "|    time_elapsed       | 6          |\n",
      "|    total_timesteps    | 500        |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -12.8      |\n",
      "|    explained_variance | 0.557      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 99         |\n",
      "|    policy_loss        | -22.9      |\n",
      "|    reward             | 0.25826782 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 4.17       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 75        |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 13        |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -12.8     |\n",
      "|    explained_variance | -0.142    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | 4.47      |\n",
      "|    reward             | 0.7409554 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 1.62      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 19         |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -12.8      |\n",
      "|    explained_variance | 0.121      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | -9.01      |\n",
      "|    reward             | -3.1677368 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 2.02       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 74         |\n",
      "|    iterations         | 400        |\n",
      "|    time_elapsed       | 26         |\n",
      "|    total_timesteps    | 2000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -12.8      |\n",
      "|    explained_variance | -0.215     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 399        |\n",
      "|    policy_loss        | 7.82       |\n",
      "|    reward             | 0.05912972 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 2.01       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 72         |\n",
      "|    iterations         | 500        |\n",
      "|    time_elapsed       | 34         |\n",
      "|    total_timesteps    | 2500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -12.8      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 499        |\n",
      "|    policy_loss        | -19.5      |\n",
      "|    reward             | -0.4015711 |\n",
      "|    std                | 0.999      |\n",
      "|    value_loss         | 3.63       |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 73           |\n",
      "|    iterations         | 600          |\n",
      "|    time_elapsed       | 40           |\n",
      "|    total_timesteps    | 3000         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -12.8        |\n",
      "|    explained_variance | -1.19e-07    |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 599          |\n",
      "|    policy_loss        | 0.122        |\n",
      "|    reward             | -0.013615431 |\n",
      "|    std                | 0.998        |\n",
      "|    value_loss         | 0.0097       |\n",
      "----------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 73       |\n",
      "|    iterations         | 700      |\n",
      "|    time_elapsed       | 47       |\n",
      "|    total_timesteps    | 3500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -12.8    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 699      |\n",
      "|    policy_loss        | 5.52     |\n",
      "|    reward             | -1.12438 |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 0.558    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 72        |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 55        |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -12.8     |\n",
      "|    explained_variance | 0.059     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | 1.04      |\n",
      "|    reward             | 0.9298243 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 0.753     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 71        |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 63        |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -12.8     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | 9.39      |\n",
      "|    reward             | 2.1539633 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 3.02      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 70        |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 70        |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -12.8     |\n",
      "|    explained_variance | 0.0276    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | -60       |\n",
      "|    reward             | 1.8048987 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 51.1      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 69         |\n",
      "|    iterations         | 1100       |\n",
      "|    time_elapsed       | 79         |\n",
      "|    total_timesteps    | 5500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -12.8      |\n",
      "|    explained_variance | -0.277     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1099       |\n",
      "|    policy_loss        | 15.2       |\n",
      "|    reward             | 0.09828056 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 3.61       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 67          |\n",
      "|    iterations         | 1200        |\n",
      "|    time_elapsed       | 89          |\n",
      "|    total_timesteps    | 6000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -12.9       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1199        |\n",
      "|    policy_loss        | -4.2        |\n",
      "|    reward             | -0.18441802 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 0.144       |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 65        |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 98        |\n",
      "|    total_timesteps    | 6500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -12.9     |\n",
      "|    explained_variance | -0.21     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1299      |\n",
      "|    policy_loss        | -12.2     |\n",
      "|    reward             | 0.5668723 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 1.38      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 64         |\n",
      "|    iterations         | 1400       |\n",
      "|    time_elapsed       | 109        |\n",
      "|    total_timesteps    | 7000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -12.8      |\n",
      "|    explained_variance | 0.0904     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1399       |\n",
      "|    policy_loss        | 13.5       |\n",
      "|    reward             | 0.22580564 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 2.16       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 63        |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 118       |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -12.9     |\n",
      "|    explained_variance | -0.023    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | 11.4      |\n",
      "|    reward             | 1.1451013 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 1.23      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 64        |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 124       |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -12.9     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | -10.5     |\n",
      "|    reward             | -1.088984 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 1.71      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 64       |\n",
      "|    iterations         | 1700     |\n",
      "|    time_elapsed       | 131      |\n",
      "|    total_timesteps    | 8500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -12.9    |\n",
      "|    explained_variance | -0.0102  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1699     |\n",
      "|    policy_loss        | 143      |\n",
      "|    reward             | 8.90256  |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 129      |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 64         |\n",
      "|    iterations         | 1800       |\n",
      "|    time_elapsed       | 138        |\n",
      "|    total_timesteps    | 9000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -12.9      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1799       |\n",
      "|    policy_loss        | -20.6      |\n",
      "|    reward             | 0.45451513 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 2.71       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 65        |\n",
      "|    iterations         | 1900      |\n",
      "|    time_elapsed       | 145       |\n",
      "|    total_timesteps    | 9500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13       |\n",
      "|    explained_variance | -0.014    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1899      |\n",
      "|    policy_loss        | -34       |\n",
      "|    reward             | 0.4502512 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 8.54      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 65         |\n",
      "|    iterations         | 2000       |\n",
      "|    time_elapsed       | 151        |\n",
      "|    total_timesteps    | 10000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -12.9      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1999       |\n",
      "|    policy_loss        | 7.66       |\n",
      "|    reward             | 0.28919283 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 2.27       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 66        |\n",
      "|    iterations         | 2100      |\n",
      "|    time_elapsed       | 158       |\n",
      "|    total_timesteps    | 10500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -12.9     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2099      |\n",
      "|    policy_loss        | -0.399    |\n",
      "|    reward             | 1.0596085 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 0.482     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 66        |\n",
      "|    iterations         | 2200      |\n",
      "|    time_elapsed       | 164       |\n",
      "|    total_timesteps    | 11000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13       |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2199      |\n",
      "|    policy_loss        | 18.3      |\n",
      "|    reward             | 1.7379335 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 4.41      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 66        |\n",
      "|    iterations         | 2300      |\n",
      "|    time_elapsed       | 171       |\n",
      "|    total_timesteps    | 11500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13       |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2299      |\n",
      "|    policy_loss        | 101       |\n",
      "|    reward             | 1.7969406 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 63.7      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 67         |\n",
      "|    iterations         | 2400       |\n",
      "|    time_elapsed       | 178        |\n",
      "|    total_timesteps    | 12000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13        |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2399       |\n",
      "|    policy_loss        | 23.6       |\n",
      "|    reward             | 0.22926448 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 3.89       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 67         |\n",
      "|    iterations         | 2500       |\n",
      "|    time_elapsed       | 184        |\n",
      "|    total_timesteps    | 12500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13        |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2499       |\n",
      "|    policy_loss        | -10.6      |\n",
      "|    reward             | 0.55890507 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 1.28       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 67         |\n",
      "|    iterations         | 2600       |\n",
      "|    time_elapsed       | 192        |\n",
      "|    total_timesteps    | 13000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13        |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2599       |\n",
      "|    policy_loss        | 5.04       |\n",
      "|    reward             | 0.76000524 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 0.829      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 67          |\n",
      "|    iterations         | 2700        |\n",
      "|    time_elapsed       | 199         |\n",
      "|    total_timesteps    | 13500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13         |\n",
      "|    explained_variance | 1.79e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 2699        |\n",
      "|    policy_loss        | -11.4       |\n",
      "|    reward             | -0.25509813 |\n",
      "|    std                | 1.03        |\n",
      "|    value_loss         | 3.77        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 67         |\n",
      "|    iterations         | 2800       |\n",
      "|    time_elapsed       | 206        |\n",
      "|    total_timesteps    | 14000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13        |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2799       |\n",
      "|    policy_loss        | -17.3      |\n",
      "|    reward             | -0.6450804 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 3.12       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 68         |\n",
      "|    iterations         | 2900       |\n",
      "|    time_elapsed       | 212        |\n",
      "|    total_timesteps    | 14500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13        |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2899       |\n",
      "|    policy_loss        | 46.7       |\n",
      "|    reward             | -1.8096751 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 16.9       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 68         |\n",
      "|    iterations         | 3000       |\n",
      "|    time_elapsed       | 219        |\n",
      "|    total_timesteps    | 15000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13        |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2999       |\n",
      "|    policy_loss        | 7.4        |\n",
      "|    reward             | -1.0080005 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 0.41       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 68          |\n",
      "|    iterations         | 3100        |\n",
      "|    time_elapsed       | 225         |\n",
      "|    total_timesteps    | 15500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13         |\n",
      "|    explained_variance | 1.19e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 3099        |\n",
      "|    policy_loss        | -17.1       |\n",
      "|    reward             | -0.85379386 |\n",
      "|    std                | 1.03        |\n",
      "|    value_loss         | 3.3         |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 68        |\n",
      "|    iterations         | 3200      |\n",
      "|    time_elapsed       | 232       |\n",
      "|    total_timesteps    | 16000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13       |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3199      |\n",
      "|    policy_loss        | 22.8      |\n",
      "|    reward             | 2.171949  |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 7.25      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 69         |\n",
      "|    iterations         | 3300       |\n",
      "|    time_elapsed       | 238        |\n",
      "|    total_timesteps    | 16500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13        |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3299       |\n",
      "|    policy_loss        | 11.7       |\n",
      "|    reward             | 0.95958906 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 1.46       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 69        |\n",
      "|    iterations         | 3400      |\n",
      "|    time_elapsed       | 245       |\n",
      "|    total_timesteps    | 17000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13       |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3399      |\n",
      "|    policy_loss        | -60.2     |\n",
      "|    reward             | 4.7048006 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 28.4      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 69          |\n",
      "|    iterations         | 3500        |\n",
      "|    time_elapsed       | 251         |\n",
      "|    total_timesteps    | 17500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.1       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 3499        |\n",
      "|    policy_loss        | 66.1        |\n",
      "|    reward             | 0.032258667 |\n",
      "|    std                | 1.03        |\n",
      "|    value_loss         | 36          |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 69        |\n",
      "|    iterations         | 3600      |\n",
      "|    time_elapsed       | 258       |\n",
      "|    total_timesteps    | 18000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.1     |\n",
      "|    explained_variance | -0.612    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3599      |\n",
      "|    policy_loss        | 0.211     |\n",
      "|    reward             | 0.4675048 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 0.117     |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 69          |\n",
      "|    iterations         | 3700        |\n",
      "|    time_elapsed       | 265         |\n",
      "|    total_timesteps    | 18500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.1       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 3699        |\n",
      "|    policy_loss        | 0.598       |\n",
      "|    reward             | -0.46106002 |\n",
      "|    std                | 1.04        |\n",
      "|    value_loss         | 1.16        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 69          |\n",
      "|    iterations         | 3800        |\n",
      "|    time_elapsed       | 272         |\n",
      "|    total_timesteps    | 19000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.1       |\n",
      "|    explained_variance | 0.0223      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 3799        |\n",
      "|    policy_loss        | 40.8        |\n",
      "|    reward             | -0.16385213 |\n",
      "|    std                | 1.04        |\n",
      "|    value_loss         | 13.1        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 69        |\n",
      "|    iterations         | 3900      |\n",
      "|    time_elapsed       | 278       |\n",
      "|    total_timesteps    | 19500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.2     |\n",
      "|    explained_variance | 0.0304    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3899      |\n",
      "|    policy_loss        | 11        |\n",
      "|    reward             | 0.5704173 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 1.15      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 70         |\n",
      "|    iterations         | 4000       |\n",
      "|    time_elapsed       | 285        |\n",
      "|    total_timesteps    | 20000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.1      |\n",
      "|    explained_variance | -0.0121    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3999       |\n",
      "|    policy_loss        | -64.1      |\n",
      "|    reward             | -5.2271543 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 26.5       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 69        |\n",
      "|    iterations         | 4100      |\n",
      "|    time_elapsed       | 292       |\n",
      "|    total_timesteps    | 20500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.2     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4099      |\n",
      "|    policy_loss        | -38.2     |\n",
      "|    reward             | 1.6443115 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 14.8      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 70         |\n",
      "|    iterations         | 4200       |\n",
      "|    time_elapsed       | 299        |\n",
      "|    total_timesteps    | 21000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.2      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4199       |\n",
      "|    policy_loss        | 2.47       |\n",
      "|    reward             | -0.8688841 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 0.701      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 70         |\n",
      "|    iterations         | 4300       |\n",
      "|    time_elapsed       | 306        |\n",
      "|    total_timesteps    | 21500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.2      |\n",
      "|    explained_variance | -0.0129    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4299       |\n",
      "|    policy_loss        | 25.9       |\n",
      "|    reward             | 0.10598524 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 4.48       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 70         |\n",
      "|    iterations         | 4400       |\n",
      "|    time_elapsed       | 313        |\n",
      "|    total_timesteps    | 22000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.2      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4399       |\n",
      "|    policy_loss        | -12.6      |\n",
      "|    reward             | -3.8803012 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 0.869      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 70        |\n",
      "|    iterations         | 4500      |\n",
      "|    time_elapsed       | 320       |\n",
      "|    total_timesteps    | 22500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.2     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4499      |\n",
      "|    policy_loss        | 18        |\n",
      "|    reward             | 3.1065147 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 2.63      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 70          |\n",
      "|    iterations         | 4600        |\n",
      "|    time_elapsed       | 326         |\n",
      "|    total_timesteps    | 23000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.1       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 4599        |\n",
      "|    policy_loss        | 55.2        |\n",
      "|    reward             | -0.36547327 |\n",
      "|    std                | 1.04        |\n",
      "|    value_loss         | 24.3        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 70         |\n",
      "|    iterations         | 4700       |\n",
      "|    time_elapsed       | 333        |\n",
      "|    total_timesteps    | 23500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4699       |\n",
      "|    policy_loss        | 59.7       |\n",
      "|    reward             | -3.5020978 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 23.9       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 70          |\n",
      "|    iterations         | 4800        |\n",
      "|    time_elapsed       | 340         |\n",
      "|    total_timesteps    | 24000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.1       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 4799        |\n",
      "|    policy_loss        | -1.49       |\n",
      "|    reward             | -0.56470525 |\n",
      "|    std                | 1.04        |\n",
      "|    value_loss         | 0.924       |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 70       |\n",
      "|    iterations         | 4900     |\n",
      "|    time_elapsed       | 346      |\n",
      "|    total_timesteps    | 24500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13.2    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4899     |\n",
      "|    policy_loss        | 8.34     |\n",
      "|    reward             | 0.596619 |\n",
      "|    std                | 1.04     |\n",
      "|    value_loss         | 1.22     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 70       |\n",
      "|    iterations         | 5000     |\n",
      "|    time_elapsed       | 353      |\n",
      "|    total_timesteps    | 25000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13.2    |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4999     |\n",
      "|    policy_loss        | -68.4    |\n",
      "|    reward             | 1.324633 |\n",
      "|    std                | 1.05     |\n",
      "|    value_loss         | 31.4     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 70         |\n",
      "|    iterations         | 5100       |\n",
      "|    time_elapsed       | 359        |\n",
      "|    total_timesteps    | 25500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5099       |\n",
      "|    policy_loss        | 42.3       |\n",
      "|    reward             | 0.68412364 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 11.7       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 70          |\n",
      "|    iterations         | 5200        |\n",
      "|    time_elapsed       | 366         |\n",
      "|    total_timesteps    | 26000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.1       |\n",
      "|    explained_variance | 1.19e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 5199        |\n",
      "|    policy_loss        | -37.3       |\n",
      "|    reward             | -0.27550668 |\n",
      "|    std                | 1.04        |\n",
      "|    value_loss         | 20.2        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 71        |\n",
      "|    iterations         | 5300      |\n",
      "|    time_elapsed       | 373       |\n",
      "|    total_timesteps    | 26500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5299      |\n",
      "|    policy_loss        | 146       |\n",
      "|    reward             | -5.014915 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 97.9      |\n",
      "-------------------------------------\n",
      "day: 2956, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 6236829.61\n",
      "total_reward: 5236829.61\n",
      "total_cost: 5014.00\n",
      "total_trades: 18841\n",
      "Sharpe: 0.987\n",
      "=================================\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 71          |\n",
      "|    iterations         | 5400        |\n",
      "|    time_elapsed       | 379         |\n",
      "|    total_timesteps    | 27000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.1       |\n",
      "|    explained_variance | -0.018      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 5399        |\n",
      "|    policy_loss        | -28.3       |\n",
      "|    reward             | -0.77084816 |\n",
      "|    std                | 1.04        |\n",
      "|    value_loss         | 4.51        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 71          |\n",
      "|    iterations         | 5500        |\n",
      "|    time_elapsed       | 386         |\n",
      "|    total_timesteps    | 27500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.1       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 5499        |\n",
      "|    policy_loss        | 28.3        |\n",
      "|    reward             | 0.020375514 |\n",
      "|    std                | 1.04        |\n",
      "|    value_loss         | 5.33        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 71        |\n",
      "|    iterations         | 5600      |\n",
      "|    time_elapsed       | 393       |\n",
      "|    total_timesteps    | 28000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5599      |\n",
      "|    policy_loss        | -22.3     |\n",
      "|    reward             | 0.3668838 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 9.37      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 71         |\n",
      "|    iterations         | 5700       |\n",
      "|    time_elapsed       | 399        |\n",
      "|    total_timesteps    | 28500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5699       |\n",
      "|    policy_loss        | 1.9        |\n",
      "|    reward             | -0.7280471 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 0.652      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 71        |\n",
      "|    iterations         | 5800      |\n",
      "|    time_elapsed       | 406       |\n",
      "|    total_timesteps    | 29000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5799      |\n",
      "|    policy_loss        | 51.7      |\n",
      "|    reward             | 0.6477628 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 31.5      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 71        |\n",
      "|    iterations         | 5900      |\n",
      "|    time_elapsed       | 412       |\n",
      "|    total_timesteps    | 29500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.1     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5899      |\n",
      "|    policy_loss        | -10.7     |\n",
      "|    reward             | 1.2560468 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 12.7      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 71       |\n",
      "|    iterations         | 6000     |\n",
      "|    time_elapsed       | 419      |\n",
      "|    total_timesteps    | 30000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13.1    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5999     |\n",
      "|    policy_loss        | -38.6    |\n",
      "|    reward             | 1.12778  |\n",
      "|    std                | 1.04     |\n",
      "|    value_loss         | 8.58     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 71        |\n",
      "|    iterations         | 6100      |\n",
      "|    time_elapsed       | 426       |\n",
      "|    total_timesteps    | 30500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.1     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6099      |\n",
      "|    policy_loss        | 7.18      |\n",
      "|    reward             | 0.3291316 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 0.775     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 71        |\n",
      "|    iterations         | 6200      |\n",
      "|    time_elapsed       | 432       |\n",
      "|    total_timesteps    | 31000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.2     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6199      |\n",
      "|    policy_loss        | -53.5     |\n",
      "|    reward             | -2.574027 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 43.4      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 71        |\n",
      "|    iterations         | 6300      |\n",
      "|    time_elapsed       | 439       |\n",
      "|    total_timesteps    | 31500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6299      |\n",
      "|    policy_loss        | 0.611     |\n",
      "|    reward             | 0.5029629 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 1.75      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 71        |\n",
      "|    iterations         | 6400      |\n",
      "|    time_elapsed       | 446       |\n",
      "|    total_timesteps    | 32000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.2     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6399      |\n",
      "|    policy_loss        | 74.8      |\n",
      "|    reward             | 6.511492  |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 69.2      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 71         |\n",
      "|    iterations         | 6500       |\n",
      "|    time_elapsed       | 452        |\n",
      "|    total_timesteps    | 32500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6499       |\n",
      "|    policy_loss        | -73.7      |\n",
      "|    reward             | 0.92201746 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 35.7       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 71         |\n",
      "|    iterations         | 6600       |\n",
      "|    time_elapsed       | 459        |\n",
      "|    total_timesteps    | 33000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.2      |\n",
      "|    explained_variance | -0.00621   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6599       |\n",
      "|    policy_loss        | -36.1      |\n",
      "|    reward             | -1.6255112 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 8.16       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 71        |\n",
      "|    iterations         | 6700      |\n",
      "|    time_elapsed       | 465       |\n",
      "|    total_timesteps    | 33500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6699      |\n",
      "|    policy_loss        | 1.47      |\n",
      "|    reward             | 1.9207145 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 0.0602    |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 71         |\n",
      "|    iterations         | 6800       |\n",
      "|    time_elapsed       | 472        |\n",
      "|    total_timesteps    | 34000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.2      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6799       |\n",
      "|    policy_loss        | 32.6       |\n",
      "|    reward             | -2.4781482 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 6.95       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 71          |\n",
      "|    iterations         | 6900        |\n",
      "|    time_elapsed       | 479         |\n",
      "|    total_timesteps    | 34500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.2       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 6899        |\n",
      "|    policy_loss        | -23.3       |\n",
      "|    reward             | -0.85683024 |\n",
      "|    std                | 1.05        |\n",
      "|    value_loss         | 4.24        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 71        |\n",
      "|    iterations         | 7000      |\n",
      "|    time_elapsed       | 486       |\n",
      "|    total_timesteps    | 35000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6999      |\n",
      "|    policy_loss        | -25.6     |\n",
      "|    reward             | 2.4669063 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 6.7       |\n",
      "-------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 72           |\n",
      "|    iterations         | 7100         |\n",
      "|    time_elapsed       | 492          |\n",
      "|    total_timesteps    | 35500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -13.2        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 7099         |\n",
      "|    policy_loss        | -1.51        |\n",
      "|    reward             | -0.013160724 |\n",
      "|    std                | 1.05         |\n",
      "|    value_loss         | 0.0155       |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 72         |\n",
      "|    iterations         | 7200       |\n",
      "|    time_elapsed       | 499        |\n",
      "|    total_timesteps    | 36000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7199       |\n",
      "|    policy_loss        | 6.19       |\n",
      "|    reward             | -0.3904432 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 0.286      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 72          |\n",
      "|    iterations         | 7300        |\n",
      "|    time_elapsed       | 506         |\n",
      "|    total_timesteps    | 36500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.2       |\n",
      "|    explained_variance | 1.19e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 7299        |\n",
      "|    policy_loss        | -3.36       |\n",
      "|    reward             | -0.40884888 |\n",
      "|    std                | 1.05        |\n",
      "|    value_loss         | 0.37        |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 72       |\n",
      "|    iterations         | 7400     |\n",
      "|    time_elapsed       | 513      |\n",
      "|    total_timesteps    | 37000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13.2    |\n",
      "|    explained_variance | 0.00111  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7399     |\n",
      "|    policy_loss        | -77.8    |\n",
      "|    reward             | 2.537419 |\n",
      "|    std                | 1.04     |\n",
      "|    value_loss         | 34.8     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 72        |\n",
      "|    iterations         | 7500      |\n",
      "|    time_elapsed       | 519       |\n",
      "|    total_timesteps    | 37500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7499      |\n",
      "|    policy_loss        | -6.18     |\n",
      "|    reward             | 2.8685172 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 0.637     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 72        |\n",
      "|    iterations         | 7600      |\n",
      "|    time_elapsed       | 526       |\n",
      "|    total_timesteps    | 38000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7599      |\n",
      "|    policy_loss        | 18.2      |\n",
      "|    reward             | 6.4693046 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 4.6       |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 72          |\n",
      "|    iterations         | 7700        |\n",
      "|    time_elapsed       | 532         |\n",
      "|    total_timesteps    | 38500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.2       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 7699        |\n",
      "|    policy_loss        | 5.24        |\n",
      "|    reward             | 0.056207057 |\n",
      "|    std                | 1.05        |\n",
      "|    value_loss         | 0.193       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 72         |\n",
      "|    iterations         | 7800       |\n",
      "|    time_elapsed       | 539        |\n",
      "|    total_timesteps    | 39000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7799       |\n",
      "|    policy_loss        | 22.3       |\n",
      "|    reward             | -0.5935071 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 3.53       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 72        |\n",
      "|    iterations         | 7900      |\n",
      "|    time_elapsed       | 545       |\n",
      "|    total_timesteps    | 39500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7899      |\n",
      "|    policy_loss        | -48.9     |\n",
      "|    reward             | 0.9553145 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 14.4      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 72         |\n",
      "|    iterations         | 8000       |\n",
      "|    time_elapsed       | 552        |\n",
      "|    total_timesteps    | 40000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.1      |\n",
      "|    explained_variance | 0.391      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7999       |\n",
      "|    policy_loss        | -29.2      |\n",
      "|    reward             | 0.11830288 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 6.14       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 72       |\n",
      "|    iterations         | 8100     |\n",
      "|    time_elapsed       | 558      |\n",
      "|    total_timesteps    | 40500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13.1    |\n",
      "|    explained_variance | 0.00336  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8099     |\n",
      "|    policy_loss        | -57.7    |\n",
      "|    reward             | 5.725108 |\n",
      "|    std                | 1.04     |\n",
      "|    value_loss         | 39.4     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 72        |\n",
      "|    iterations         | 8200      |\n",
      "|    time_elapsed       | 566       |\n",
      "|    total_timesteps    | 41000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8199      |\n",
      "|    policy_loss        | -412      |\n",
      "|    reward             | -14.15416 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 1.31e+03  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 72         |\n",
      "|    iterations         | 8300       |\n",
      "|    time_elapsed       | 572        |\n",
      "|    total_timesteps    | 41500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.2      |\n",
      "|    explained_variance | 0.183      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8299       |\n",
      "|    policy_loss        | -41.9      |\n",
      "|    reward             | -2.1514919 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 13.5       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 72        |\n",
      "|    iterations         | 8400      |\n",
      "|    time_elapsed       | 579       |\n",
      "|    total_timesteps    | 42000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8399      |\n",
      "|    policy_loss        | -30.7     |\n",
      "|    reward             | 0.4932687 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 7.3       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 72        |\n",
      "|    iterations         | 8500      |\n",
      "|    time_elapsed       | 585       |\n",
      "|    total_timesteps    | 42500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.2     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8499      |\n",
      "|    policy_loss        | -7.25     |\n",
      "|    reward             | 1.9548683 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 1.76      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 72        |\n",
      "|    iterations         | 8600      |\n",
      "|    time_elapsed       | 592       |\n",
      "|    total_timesteps    | 43000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.2     |\n",
      "|    explained_variance | 1.79e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8599      |\n",
      "|    policy_loss        | -4.66     |\n",
      "|    reward             | 2.0419188 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 2.63      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 72       |\n",
      "|    iterations         | 8700     |\n",
      "|    time_elapsed       | 598      |\n",
      "|    total_timesteps    | 43500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13.2    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8699     |\n",
      "|    policy_loss        | -86.5    |\n",
      "|    reward             | 2.415062 |\n",
      "|    std                | 1.05     |\n",
      "|    value_loss         | 37.2     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 72         |\n",
      "|    iterations         | 8800       |\n",
      "|    time_elapsed       | 605        |\n",
      "|    total_timesteps    | 44000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.2      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8799       |\n",
      "|    policy_loss        | 116        |\n",
      "|    reward             | -2.0354633 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 77.2       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 72       |\n",
      "|    iterations         | 8900     |\n",
      "|    time_elapsed       | 611      |\n",
      "|    total_timesteps    | 44500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13.3    |\n",
      "|    explained_variance | -0.0496  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8899     |\n",
      "|    policy_loss        | 18.3     |\n",
      "|    reward             | 2.110533 |\n",
      "|    std                | 1.06     |\n",
      "|    value_loss         | 3.63     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 72         |\n",
      "|    iterations         | 9000       |\n",
      "|    time_elapsed       | 618        |\n",
      "|    total_timesteps    | 45000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.2      |\n",
      "|    explained_variance | 0.258      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8999       |\n",
      "|    policy_loss        | 18.7       |\n",
      "|    reward             | 0.31698227 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 2.55       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 72         |\n",
      "|    iterations         | 9100       |\n",
      "|    time_elapsed       | 625        |\n",
      "|    total_timesteps    | 45500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9099       |\n",
      "|    policy_loss        | 4.17       |\n",
      "|    reward             | 0.06547546 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 0.247      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 72         |\n",
      "|    iterations         | 9200       |\n",
      "|    time_elapsed       | 632        |\n",
      "|    total_timesteps    | 46000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9199       |\n",
      "|    policy_loss        | 45.9       |\n",
      "|    reward             | 0.21437299 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 15         |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 72       |\n",
      "|    iterations         | 9300     |\n",
      "|    time_elapsed       | 638      |\n",
      "|    total_timesteps    | 46500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13.3    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9299     |\n",
      "|    policy_loss        | 25.9     |\n",
      "|    reward             | 3.825673 |\n",
      "|    std                | 1.06     |\n",
      "|    value_loss         | 4.53     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 72        |\n",
      "|    iterations         | 9400      |\n",
      "|    time_elapsed       | 645       |\n",
      "|    total_timesteps    | 47000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.3     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9399      |\n",
      "|    policy_loss        | 72.1      |\n",
      "|    reward             | 1.4634874 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 42.7      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 72         |\n",
      "|    iterations         | 9500       |\n",
      "|    time_elapsed       | 652        |\n",
      "|    total_timesteps    | 47500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.3      |\n",
      "|    explained_variance | -0.131     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9499       |\n",
      "|    policy_loss        | 9.46       |\n",
      "|    reward             | 0.44693422 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 1.04       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 72        |\n",
      "|    iterations         | 9600      |\n",
      "|    time_elapsed       | 658       |\n",
      "|    total_timesteps    | 48000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.3     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9599      |\n",
      "|    policy_loss        | 5.51      |\n",
      "|    reward             | -1.161286 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 0.782     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 72         |\n",
      "|    iterations         | 9700       |\n",
      "|    time_elapsed       | 664        |\n",
      "|    total_timesteps    | 48500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9699       |\n",
      "|    policy_loss        | 3.03       |\n",
      "|    reward             | -1.1757271 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 0.725      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 72          |\n",
      "|    iterations         | 9800        |\n",
      "|    time_elapsed       | 671         |\n",
      "|    total_timesteps    | 49000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.3       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 9799        |\n",
      "|    policy_loss        | -78.2       |\n",
      "|    reward             | -0.87654454 |\n",
      "|    std                | 1.06        |\n",
      "|    value_loss         | 35.6        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 72         |\n",
      "|    iterations         | 9900       |\n",
      "|    time_elapsed       | 678        |\n",
      "|    total_timesteps    | 49500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9899       |\n",
      "|    policy_loss        | -25.1      |\n",
      "|    reward             | 0.31208235 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 3.83       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 72         |\n",
      "|    iterations         | 10000      |\n",
      "|    time_elapsed       | 685        |\n",
      "|    total_timesteps    | 50000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9999       |\n",
      "|    policy_loss        | 56.3       |\n",
      "|    reward             | -12.160019 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 23.7       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 72        |\n",
      "|    iterations         | 10100     |\n",
      "|    time_elapsed       | 692       |\n",
      "|    total_timesteps    | 50500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.3     |\n",
      "|    explained_variance | 0.14      |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 10099     |\n",
      "|    policy_loss        | 3.98      |\n",
      "|    reward             | 1.3558804 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 0.801     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 72        |\n",
      "|    iterations         | 10200     |\n",
      "|    time_elapsed       | 699       |\n",
      "|    total_timesteps    | 51000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 10199     |\n",
      "|    policy_loss        | 18.8      |\n",
      "|    reward             | 0.9506353 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 2.97      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 72        |\n",
      "|    iterations         | 10300     |\n",
      "|    time_elapsed       | 705       |\n",
      "|    total_timesteps    | 51500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 10299     |\n",
      "|    policy_loss        | 0.499     |\n",
      "|    reward             | 1.0174537 |\n",
      "|    std                | 1.07      |\n",
      "|    value_loss         | 0.175     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 73         |\n",
      "|    iterations         | 10400      |\n",
      "|    time_elapsed       | 712        |\n",
      "|    total_timesteps    | 52000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 10399      |\n",
      "|    policy_loss        | 112        |\n",
      "|    reward             | 0.10060353 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 63.4       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 72        |\n",
      "|    iterations         | 10500     |\n",
      "|    time_elapsed       | 720       |\n",
      "|    total_timesteps    | 52500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 10499     |\n",
      "|    policy_loss        | 35.1      |\n",
      "|    reward             | -8.542449 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 25.8      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 72        |\n",
      "|    iterations         | 10600     |\n",
      "|    time_elapsed       | 727       |\n",
      "|    total_timesteps    | 53000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.3     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 10599     |\n",
      "|    policy_loss        | -192      |\n",
      "|    reward             | 0.9996357 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 379       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 72         |\n",
      "|    iterations         | 10700      |\n",
      "|    time_elapsed       | 733        |\n",
      "|    total_timesteps    | 53500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.4      |\n",
      "|    explained_variance | -0.129     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 10699      |\n",
      "|    policy_loss        | 3.88       |\n",
      "|    reward             | 0.24727666 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 0.211      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 72          |\n",
      "|    iterations         | 10800       |\n",
      "|    time_elapsed       | 742         |\n",
      "|    total_timesteps    | 54000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.3       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 10799       |\n",
      "|    policy_loss        | -8.64       |\n",
      "|    reward             | 0.009155809 |\n",
      "|    std                | 1.07        |\n",
      "|    value_loss         | 0.5         |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 72         |\n",
      "|    iterations         | 10900      |\n",
      "|    time_elapsed       | 749        |\n",
      "|    total_timesteps    | 54500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.3      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 10899      |\n",
      "|    policy_loss        | 6.56       |\n",
      "|    reward             | -4.3016634 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 2.66       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 72        |\n",
      "|    iterations         | 11000     |\n",
      "|    time_elapsed       | 755       |\n",
      "|    total_timesteps    | 55000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.3     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 10999     |\n",
      "|    policy_loss        | -7.96     |\n",
      "|    reward             | 0.6850325 |\n",
      "|    std                | 1.07      |\n",
      "|    value_loss         | 0.455     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 72         |\n",
      "|    iterations         | 11100      |\n",
      "|    time_elapsed       | 762        |\n",
      "|    total_timesteps    | 55500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 11099      |\n",
      "|    policy_loss        | 104        |\n",
      "|    reward             | 0.33868793 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 90.1       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 72        |\n",
      "|    iterations         | 11200     |\n",
      "|    time_elapsed       | 769       |\n",
      "|    total_timesteps    | 56000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.3     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 11199     |\n",
      "|    policy_loss        | -79.2     |\n",
      "|    reward             | -2.372308 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 36.7      |\n",
      "-------------------------------------\n",
      "day: 2956, episode: 20\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 6538162.79\n",
      "total_reward: 5538162.79\n",
      "total_cost: 7674.94\n",
      "total_trades: 17147\n",
      "Sharpe: 0.988\n",
      "=================================\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 72        |\n",
      "|    iterations         | 11300     |\n",
      "|    time_elapsed       | 777       |\n",
      "|    total_timesteps    | 56500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.3     |\n",
      "|    explained_variance | -0.0111   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 11299     |\n",
      "|    policy_loss        | 13        |\n",
      "|    reward             | 0.7384844 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 1.05      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 72         |\n",
      "|    iterations         | 11400      |\n",
      "|    time_elapsed       | 783        |\n",
      "|    total_timesteps    | 57000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 11399      |\n",
      "|    policy_loss        | 5.82       |\n",
      "|    reward             | 0.63977164 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 0.574      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 72        |\n",
      "|    iterations         | 11500     |\n",
      "|    time_elapsed       | 789       |\n",
      "|    total_timesteps    | 57500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.4     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 11499     |\n",
      "|    policy_loss        | 19        |\n",
      "|    reward             | 2.4426837 |\n",
      "|    std                | 1.07      |\n",
      "|    value_loss         | 4.94      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 72         |\n",
      "|    iterations         | 11600      |\n",
      "|    time_elapsed       | 796        |\n",
      "|    total_timesteps    | 58000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 11599      |\n",
      "|    policy_loss        | -3.19      |\n",
      "|    reward             | -0.7288055 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 1.12       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 72        |\n",
      "|    iterations         | 11700     |\n",
      "|    time_elapsed       | 803       |\n",
      "|    total_timesteps    | 58500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 11699     |\n",
      "|    policy_loss        | 78.6      |\n",
      "|    reward             | -2.063593 |\n",
      "|    std                | 1.07      |\n",
      "|    value_loss         | 35.8      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 72        |\n",
      "|    iterations         | 11800     |\n",
      "|    time_elapsed       | 810       |\n",
      "|    total_timesteps    | 59000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 11799     |\n",
      "|    policy_loss        | -3.46     |\n",
      "|    reward             | 2.9746933 |\n",
      "|    std                | 1.07      |\n",
      "|    value_loss         | 19.1      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 72         |\n",
      "|    iterations         | 11900      |\n",
      "|    time_elapsed       | 816        |\n",
      "|    total_timesteps    | 59500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 11899      |\n",
      "|    policy_loss        | -17.4      |\n",
      "|    reward             | -0.3005147 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 2.12       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 72        |\n",
      "|    iterations         | 12000     |\n",
      "|    time_elapsed       | 823       |\n",
      "|    total_timesteps    | 60000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 11999     |\n",
      "|    policy_loss        | -28.1     |\n",
      "|    reward             | -2.351272 |\n",
      "|    std                | 1.07      |\n",
      "|    value_loss         | 3.87      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 72        |\n",
      "|    iterations         | 12100     |\n",
      "|    time_elapsed       | 830       |\n",
      "|    total_timesteps    | 60500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 12099     |\n",
      "|    policy_loss        | -5.79     |\n",
      "|    reward             | 0.9896644 |\n",
      "|    std                | 1.07      |\n",
      "|    value_loss         | 0.531     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 72         |\n",
      "|    iterations         | 12200      |\n",
      "|    time_elapsed       | 839        |\n",
      "|    total_timesteps    | 61000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.4      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 12199      |\n",
      "|    policy_loss        | -53.8      |\n",
      "|    reward             | 0.65220386 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 19.7       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 72        |\n",
      "|    iterations         | 12300     |\n",
      "|    time_elapsed       | 846       |\n",
      "|    total_timesteps    | 61500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 12299     |\n",
      "|    policy_loss        | -54.3     |\n",
      "|    reward             | 3.3000724 |\n",
      "|    std                | 1.07      |\n",
      "|    value_loss         | 26.6      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 72       |\n",
      "|    iterations         | 12400    |\n",
      "|    time_elapsed       | 853      |\n",
      "|    total_timesteps    | 62000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13.4    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 12399    |\n",
      "|    policy_loss        | 71       |\n",
      "|    reward             | 4.054112 |\n",
      "|    std                | 1.07     |\n",
      "|    value_loss         | 33.1     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 72        |\n",
      "|    iterations         | 12500     |\n",
      "|    time_elapsed       | 859       |\n",
      "|    total_timesteps    | 62500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.4     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 12499     |\n",
      "|    policy_loss        | -61.1     |\n",
      "|    reward             | 4.2199836 |\n",
      "|    std                | 1.08      |\n",
      "|    value_loss         | 26.9      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 72         |\n",
      "|    iterations         | 12600      |\n",
      "|    time_elapsed       | 866        |\n",
      "|    total_timesteps    | 63000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 12599      |\n",
      "|    policy_loss        | -1.17      |\n",
      "|    reward             | -0.9109854 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 0.0726     |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 72       |\n",
      "|    iterations         | 12700    |\n",
      "|    time_elapsed       | 873      |\n",
      "|    total_timesteps    | 63500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13.4    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 12699    |\n",
      "|    policy_loss        | -93.4    |\n",
      "|    reward             | 0.643005 |\n",
      "|    std                | 1.07     |\n",
      "|    value_loss         | 46.8     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 72         |\n",
      "|    iterations         | 12800      |\n",
      "|    time_elapsed       | 880        |\n",
      "|    total_timesteps    | 64000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.4      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 12799      |\n",
      "|    policy_loss        | 22.4       |\n",
      "|    reward             | -0.2069824 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 3.07       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 72       |\n",
      "|    iterations         | 12900    |\n",
      "|    time_elapsed       | 887      |\n",
      "|    total_timesteps    | 64500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13.4    |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 12899    |\n",
      "|    policy_loss        | -15.2    |\n",
      "|    reward             | 2.614667 |\n",
      "|    std                | 1.07     |\n",
      "|    value_loss         | 1.54     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 72         |\n",
      "|    iterations         | 13000      |\n",
      "|    time_elapsed       | 893        |\n",
      "|    total_timesteps    | 65000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.4      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 12999      |\n",
      "|    policy_loss        | 31.4       |\n",
      "|    reward             | -1.9105549 |\n",
      "|    std                | 1.08       |\n",
      "|    value_loss         | 13.2       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 72       |\n",
      "|    iterations         | 13100    |\n",
      "|    time_elapsed       | 900      |\n",
      "|    total_timesteps    | 65500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13.4    |\n",
      "|    explained_variance | -0.00937 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 13099    |\n",
      "|    policy_loss        | -69.7    |\n",
      "|    reward             | 3.270338 |\n",
      "|    std                | 1.08     |\n",
      "|    value_loss         | 31.4     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 72         |\n",
      "|    iterations         | 13200      |\n",
      "|    time_elapsed       | 907        |\n",
      "|    total_timesteps    | 66000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.5      |\n",
      "|    explained_variance | 1.79e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 13199      |\n",
      "|    policy_loss        | -27.1      |\n",
      "|    reward             | -1.1379158 |\n",
      "|    std                | 1.08       |\n",
      "|    value_loss         | 4.43       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 72         |\n",
      "|    iterations         | 13300      |\n",
      "|    time_elapsed       | 913        |\n",
      "|    total_timesteps    | 66500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.5      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 13299      |\n",
      "|    policy_loss        | -55.3      |\n",
      "|    reward             | 0.24115816 |\n",
      "|    std                | 1.08       |\n",
      "|    value_loss         | 23.9       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 72         |\n",
      "|    iterations         | 13400      |\n",
      "|    time_elapsed       | 920        |\n",
      "|    total_timesteps    | 67000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.5      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 13399      |\n",
      "|    policy_loss        | 9.31       |\n",
      "|    reward             | 0.25222382 |\n",
      "|    std                | 1.08       |\n",
      "|    value_loss         | 0.698      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 72         |\n",
      "|    iterations         | 13500      |\n",
      "|    time_elapsed       | 927        |\n",
      "|    total_timesteps    | 67500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 13499      |\n",
      "|    policy_loss        | 10.9       |\n",
      "|    reward             | -0.2029062 |\n",
      "|    std                | 1.08       |\n",
      "|    value_loss         | 4.16       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 72         |\n",
      "|    iterations         | 13600      |\n",
      "|    time_elapsed       | 934        |\n",
      "|    total_timesteps    | 68000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 13599      |\n",
      "|    policy_loss        | -79.6      |\n",
      "|    reward             | -1.5382022 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 45.7       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 72       |\n",
      "|    iterations         | 13700    |\n",
      "|    time_elapsed       | 940      |\n",
      "|    total_timesteps    | 68500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13.4    |\n",
      "|    explained_variance | 0.0037   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 13699    |\n",
      "|    policy_loss        | 29.3     |\n",
      "|    reward             | 2.0565   |\n",
      "|    std                | 1.08     |\n",
      "|    value_loss         | 13       |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 72       |\n",
      "|    iterations         | 13800    |\n",
      "|    time_elapsed       | 947      |\n",
      "|    total_timesteps    | 69000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13.4    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 13799    |\n",
      "|    policy_loss        | 5.6      |\n",
      "|    reward             | 2.072995 |\n",
      "|    std                | 1.07     |\n",
      "|    value_loss         | 0.292    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 72        |\n",
      "|    iterations         | 13900     |\n",
      "|    time_elapsed       | 954       |\n",
      "|    total_timesteps    | 69500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 13899     |\n",
      "|    policy_loss        | 1.31      |\n",
      "|    reward             | -1.869731 |\n",
      "|    std                | 1.08      |\n",
      "|    value_loss         | 1.97      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 72         |\n",
      "|    iterations         | 14000      |\n",
      "|    time_elapsed       | 960        |\n",
      "|    total_timesteps    | 70000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.4      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 13999      |\n",
      "|    policy_loss        | -11.4      |\n",
      "|    reward             | 0.24237376 |\n",
      "|    std                | 1.08       |\n",
      "|    value_loss         | 2.82       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 72        |\n",
      "|    iterations         | 14100     |\n",
      "|    time_elapsed       | 967       |\n",
      "|    total_timesteps    | 70500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.4     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 14099     |\n",
      "|    policy_loss        | 5.6       |\n",
      "|    reward             | -0.863288 |\n",
      "|    std                | 1.07      |\n",
      "|    value_loss         | 0.622     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 72        |\n",
      "|    iterations         | 14200     |\n",
      "|    time_elapsed       | 973       |\n",
      "|    total_timesteps    | 71000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 14199     |\n",
      "|    policy_loss        | 5.59      |\n",
      "|    reward             | 0.1627032 |\n",
      "|    std                | 1.07      |\n",
      "|    value_loss         | 0.204     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 72        |\n",
      "|    iterations         | 14300     |\n",
      "|    time_elapsed       | 982       |\n",
      "|    total_timesteps    | 71500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 14299     |\n",
      "|    policy_loss        | 15.3      |\n",
      "|    reward             | 1.0698524 |\n",
      "|    std                | 1.07      |\n",
      "|    value_loss         | 1.9       |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 72          |\n",
      "|    iterations         | 14400       |\n",
      "|    time_elapsed       | 991         |\n",
      "|    total_timesteps    | 72000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.4       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 14399       |\n",
      "|    policy_loss        | -59.9       |\n",
      "|    reward             | -0.46542957 |\n",
      "|    std                | 1.08        |\n",
      "|    value_loss         | 22          |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 72        |\n",
      "|    iterations         | 14500     |\n",
      "|    time_elapsed       | 998       |\n",
      "|    total_timesteps    | 72500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.5     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 14499     |\n",
      "|    policy_loss        | 0.721     |\n",
      "|    reward             | 1.8588881 |\n",
      "|    std                | 1.08      |\n",
      "|    value_loss         | 2.09      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 72        |\n",
      "|    iterations         | 14600     |\n",
      "|    time_elapsed       | 1004      |\n",
      "|    total_timesteps    | 73000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.5     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 14599     |\n",
      "|    policy_loss        | 25.9      |\n",
      "|    reward             | -2.750041 |\n",
      "|    std                | 1.08      |\n",
      "|    value_loss         | 6.39      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 72        |\n",
      "|    iterations         | 14700     |\n",
      "|    time_elapsed       | 1011      |\n",
      "|    total_timesteps    | 73500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 14699     |\n",
      "|    policy_loss        | 72.3      |\n",
      "|    reward             | -8.906891 |\n",
      "|    std                | 1.09      |\n",
      "|    value_loss         | 35.6      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 72         |\n",
      "|    iterations         | 14800      |\n",
      "|    time_elapsed       | 1018       |\n",
      "|    total_timesteps    | 74000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.5      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 14799      |\n",
      "|    policy_loss        | 1.23       |\n",
      "|    reward             | 0.19864295 |\n",
      "|    std                | 1.09       |\n",
      "|    value_loss         | 0.0316     |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 72          |\n",
      "|    iterations         | 14900       |\n",
      "|    time_elapsed       | 1025        |\n",
      "|    total_timesteps    | 74500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.6       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 14899       |\n",
      "|    policy_loss        | -33.6       |\n",
      "|    reward             | -0.48244455 |\n",
      "|    std                | 1.09        |\n",
      "|    value_loss         | 7.52        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 72         |\n",
      "|    iterations         | 15000      |\n",
      "|    time_elapsed       | 1031       |\n",
      "|    total_timesteps    | 75000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.5      |\n",
      "|    explained_variance | 1.79e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 14999      |\n",
      "|    policy_loss        | -28.7      |\n",
      "|    reward             | -2.2046065 |\n",
      "|    std                | 1.09       |\n",
      "|    value_loss         | 6.97       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 72         |\n",
      "|    iterations         | 15100      |\n",
      "|    time_elapsed       | 1038       |\n",
      "|    total_timesteps    | 75500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.6      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 15099      |\n",
      "|    policy_loss        | 10.2       |\n",
      "|    reward             | -2.5158384 |\n",
      "|    std                | 1.09       |\n",
      "|    value_loss         | 1.83       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 72       |\n",
      "|    iterations         | 15200    |\n",
      "|    time_elapsed       | 1045     |\n",
      "|    total_timesteps    | 76000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -13.6    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 15199    |\n",
      "|    policy_loss        | -30.4    |\n",
      "|    reward             | 3.916446 |\n",
      "|    std                | 1.09     |\n",
      "|    value_loss         | 46.7     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 72         |\n",
      "|    iterations         | 15300      |\n",
      "|    time_elapsed       | 1051       |\n",
      "|    total_timesteps    | 76500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.6      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 15299      |\n",
      "|    policy_loss        | -289       |\n",
      "|    reward             | -11.718111 |\n",
      "|    std                | 1.09       |\n",
      "|    value_loss         | 722        |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 72          |\n",
      "|    iterations         | 15400       |\n",
      "|    time_elapsed       | 1058        |\n",
      "|    total_timesteps    | 77000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.5       |\n",
      "|    explained_variance | -0.0633     |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 15399       |\n",
      "|    policy_loss        | 19.8        |\n",
      "|    reward             | -0.14700194 |\n",
      "|    std                | 1.09        |\n",
      "|    value_loss         | 4.28        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 72        |\n",
      "|    iterations         | 15500     |\n",
      "|    time_elapsed       | 1065      |\n",
      "|    total_timesteps    | 77500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.5     |\n",
      "|    explained_variance | -1.31e-06 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15499     |\n",
      "|    policy_loss        | 29.8      |\n",
      "|    reward             | 1.2018094 |\n",
      "|    std                | 1.09      |\n",
      "|    value_loss         | 7.9       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 72        |\n",
      "|    iterations         | 15600     |\n",
      "|    time_elapsed       | 1073      |\n",
      "|    total_timesteps    | 78000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.5     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15599     |\n",
      "|    policy_loss        | 33        |\n",
      "|    reward             | 0.5072685 |\n",
      "|    std                | 1.09      |\n",
      "|    value_loss         | 6.18      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 72        |\n",
      "|    iterations         | 15700     |\n",
      "|    time_elapsed       | 1079      |\n",
      "|    total_timesteps    | 78500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.5     |\n",
      "|    explained_variance | 0.424     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15699     |\n",
      "|    policy_loss        | 3.1       |\n",
      "|    reward             | 0.9627934 |\n",
      "|    std                | 1.09      |\n",
      "|    value_loss         | 0.284     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 72        |\n",
      "|    iterations         | 15800     |\n",
      "|    time_elapsed       | 1086      |\n",
      "|    total_timesteps    | 79000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15799     |\n",
      "|    policy_loss        | 3.68      |\n",
      "|    reward             | 1.0755035 |\n",
      "|    std                | 1.09      |\n",
      "|    value_loss         | 1.57      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 72         |\n",
      "|    iterations         | 15900      |\n",
      "|    time_elapsed       | 1092       |\n",
      "|    total_timesteps    | 79500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.5      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 15899      |\n",
      "|    policy_loss        | 127        |\n",
      "|    reward             | 0.43193415 |\n",
      "|    std                | 1.09       |\n",
      "|    value_loss         | 129        |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 72          |\n",
      "|    iterations         | 16000       |\n",
      "|    time_elapsed       | 1096        |\n",
      "|    total_timesteps    | 80000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.5       |\n",
      "|    explained_variance | -0.203      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 15999       |\n",
      "|    policy_loss        | -4.82       |\n",
      "|    reward             | -0.98439056 |\n",
      "|    std                | 1.09        |\n",
      "|    value_loss         | 2.38        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 73         |\n",
      "|    iterations         | 16100      |\n",
      "|    time_elapsed       | 1100       |\n",
      "|    total_timesteps    | 80500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.5      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 16099      |\n",
      "|    policy_loss        | -6.95      |\n",
      "|    reward             | 0.21448217 |\n",
      "|    std                | 1.09       |\n",
      "|    value_loss         | 0.628      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 73        |\n",
      "|    iterations         | 16200     |\n",
      "|    time_elapsed       | 1105      |\n",
      "|    total_timesteps    | 81000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16199     |\n",
      "|    policy_loss        | -20       |\n",
      "|    reward             | 1.2170995 |\n",
      "|    std                | 1.09      |\n",
      "|    value_loss         | 4.27      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 73         |\n",
      "|    iterations         | 16300      |\n",
      "|    time_elapsed       | 1109       |\n",
      "|    total_timesteps    | 81500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.6      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 16299      |\n",
      "|    policy_loss        | -15.4      |\n",
      "|    reward             | 0.12732075 |\n",
      "|    std                | 1.09       |\n",
      "|    value_loss         | 1.93       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 73        |\n",
      "|    iterations         | 16400     |\n",
      "|    time_elapsed       | 1113      |\n",
      "|    total_timesteps    | 82000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16399     |\n",
      "|    policy_loss        | 13.6      |\n",
      "|    reward             | 2.6061287 |\n",
      "|    std                | 1.1       |\n",
      "|    value_loss         | 2.76      |\n",
      "-------------------------------------\n",
      "------------------------------------------\n",
      "| time/                 |                |\n",
      "|    fps                | 73             |\n",
      "|    iterations         | 16500          |\n",
      "|    time_elapsed       | 1117           |\n",
      "|    total_timesteps    | 82500          |\n",
      "| train/                |                |\n",
      "|    entropy_loss       | -13.6          |\n",
      "|    explained_variance | 0              |\n",
      "|    learning_rate      | 0.0007         |\n",
      "|    n_updates          | 16499          |\n",
      "|    policy_loss        | 46.2           |\n",
      "|    reward             | -0.00011211128 |\n",
      "|    std                | 1.1            |\n",
      "|    value_loss         | 17.4           |\n",
      "------------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 74         |\n",
      "|    iterations         | 16600      |\n",
      "|    time_elapsed       | 1121       |\n",
      "|    total_timesteps    | 83000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.6      |\n",
      "|    explained_variance | 0.192      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 16599      |\n",
      "|    policy_loss        | 5.05       |\n",
      "|    reward             | 0.14781933 |\n",
      "|    std                | 1.1        |\n",
      "|    value_loss         | 0.168      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 74        |\n",
      "|    iterations         | 16700     |\n",
      "|    time_elapsed       | 1125      |\n",
      "|    total_timesteps    | 83500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16699     |\n",
      "|    policy_loss        | -16.2     |\n",
      "|    reward             | 1.1464376 |\n",
      "|    std                | 1.11      |\n",
      "|    value_loss         | 1.73      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 74         |\n",
      "|    iterations         | 16800      |\n",
      "|    time_elapsed       | 1129       |\n",
      "|    total_timesteps    | 84000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.7      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 16799      |\n",
      "|    policy_loss        | 40.3       |\n",
      "|    reward             | -2.1654596 |\n",
      "|    std                | 1.11       |\n",
      "|    value_loss         | 9.18       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 74        |\n",
      "|    iterations         | 16900     |\n",
      "|    time_elapsed       | 1133      |\n",
      "|    total_timesteps    | 84500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16899     |\n",
      "|    policy_loss        | -62.6     |\n",
      "|    reward             | 1.0519159 |\n",
      "|    std                | 1.11      |\n",
      "|    value_loss         | 26.6      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 74        |\n",
      "|    iterations         | 17000     |\n",
      "|    time_elapsed       | 1137      |\n",
      "|    total_timesteps    | 85000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.7     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16999     |\n",
      "|    policy_loss        | -33.3     |\n",
      "|    reward             | 0.5678535 |\n",
      "|    std                | 1.12      |\n",
      "|    value_loss         | 13.4      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 74        |\n",
      "|    iterations         | 17100     |\n",
      "|    time_elapsed       | 1141      |\n",
      "|    total_timesteps    | 85500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.8     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17099     |\n",
      "|    policy_loss        | -98.9     |\n",
      "|    reward             | 2.8975558 |\n",
      "|    std                | 1.12      |\n",
      "|    value_loss         | 51.1      |\n",
      "-------------------------------------\n",
      "day: 2956, episode: 30\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3154207.87\n",
      "total_reward: 2154207.87\n",
      "total_cost: 5374.22\n",
      "total_trades: 14967\n",
      "Sharpe: 0.706\n",
      "=================================\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 75           |\n",
      "|    iterations         | 17200        |\n",
      "|    time_elapsed       | 1145         |\n",
      "|    total_timesteps    | 86000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -13.8        |\n",
      "|    explained_variance | -1.21        |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 17199        |\n",
      "|    policy_loss        | 4.58         |\n",
      "|    reward             | -0.059055034 |\n",
      "|    std                | 1.12         |\n",
      "|    value_loss         | 0.265        |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 17300      |\n",
      "|    time_elapsed       | 1149       |\n",
      "|    total_timesteps    | 86500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.8      |\n",
      "|    explained_variance | -0.393     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 17299      |\n",
      "|    policy_loss        | -11.7      |\n",
      "|    reward             | 0.51510817 |\n",
      "|    std                | 1.12       |\n",
      "|    value_loss         | 0.778      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 75          |\n",
      "|    iterations         | 17400       |\n",
      "|    time_elapsed       | 1153        |\n",
      "|    total_timesteps    | 87000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.8       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 17399       |\n",
      "|    policy_loss        | -2.93       |\n",
      "|    reward             | -0.39143276 |\n",
      "|    std                | 1.12        |\n",
      "|    value_loss         | 0.58        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 75        |\n",
      "|    iterations         | 17500     |\n",
      "|    time_elapsed       | 1157      |\n",
      "|    total_timesteps    | 87500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.8     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17499     |\n",
      "|    policy_loss        | -39.2     |\n",
      "|    reward             | 2.5512748 |\n",
      "|    std                | 1.12      |\n",
      "|    value_loss         | 12.9      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 17600      |\n",
      "|    time_elapsed       | 1161       |\n",
      "|    total_timesteps    | 88000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.8      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 17599      |\n",
      "|    policy_loss        | 30.7       |\n",
      "|    reward             | 0.26949924 |\n",
      "|    std                | 1.13       |\n",
      "|    value_loss         | 6.18       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 75        |\n",
      "|    iterations         | 17700     |\n",
      "|    time_elapsed       | 1165      |\n",
      "|    total_timesteps    | 88500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.8     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17699     |\n",
      "|    policy_loss        | -74.2     |\n",
      "|    reward             | 2.4336128 |\n",
      "|    std                | 1.13      |\n",
      "|    value_loss         | 46.8      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 17800     |\n",
      "|    time_elapsed       | 1170      |\n",
      "|    total_timesteps    | 89000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.8     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17799     |\n",
      "|    policy_loss        | 6.01      |\n",
      "|    reward             | 0.9396691 |\n",
      "|    std                | 1.12      |\n",
      "|    value_loss         | 0.323     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 17900      |\n",
      "|    time_elapsed       | 1174       |\n",
      "|    total_timesteps    | 89500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 17899      |\n",
      "|    policy_loss        | -5.12      |\n",
      "|    reward             | -1.9124503 |\n",
      "|    std                | 1.13       |\n",
      "|    value_loss         | 0.178      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 18000      |\n",
      "|    time_elapsed       | 1178       |\n",
      "|    total_timesteps    | 90000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.8      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 17999      |\n",
      "|    policy_loss        | -22.5      |\n",
      "|    reward             | -1.3912724 |\n",
      "|    std                | 1.13       |\n",
      "|    value_loss         | 6.22       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 18100      |\n",
      "|    time_elapsed       | 1182       |\n",
      "|    total_timesteps    | 90500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.9      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 18099      |\n",
      "|    policy_loss        | -1.38      |\n",
      "|    reward             | 0.60949945 |\n",
      "|    std                | 1.13       |\n",
      "|    value_loss         | 0.777      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 18200     |\n",
      "|    time_elapsed       | 1186      |\n",
      "|    total_timesteps    | 91000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.9     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18199     |\n",
      "|    policy_loss        | 50.1      |\n",
      "|    reward             | 1.1090775 |\n",
      "|    std                | 1.13      |\n",
      "|    value_loss         | 19.8      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 76          |\n",
      "|    iterations         | 18300       |\n",
      "|    time_elapsed       | 1190        |\n",
      "|    total_timesteps    | 91500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.9       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 18299       |\n",
      "|    policy_loss        | -70         |\n",
      "|    reward             | -0.25809115 |\n",
      "|    std                | 1.13        |\n",
      "|    value_loss         | 26.8        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 77         |\n",
      "|    iterations         | 18400      |\n",
      "|    time_elapsed       | 1194       |\n",
      "|    total_timesteps    | 92000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.9      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 18399      |\n",
      "|    policy_loss        | 19.5       |\n",
      "|    reward             | 0.65838593 |\n",
      "|    std                | 1.13       |\n",
      "|    value_loss         | 2.65       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 77          |\n",
      "|    iterations         | 18500       |\n",
      "|    time_elapsed       | 1198        |\n",
      "|    total_timesteps    | 92500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.8       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 18499       |\n",
      "|    policy_loss        | 9.95        |\n",
      "|    reward             | -0.25691557 |\n",
      "|    std                | 1.13        |\n",
      "|    value_loss         | 2.23        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 77        |\n",
      "|    iterations         | 18600     |\n",
      "|    time_elapsed       | 1203      |\n",
      "|    total_timesteps    | 93000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.8     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18599     |\n",
      "|    policy_loss        | -2.28     |\n",
      "|    reward             | 0.3884177 |\n",
      "|    std                | 1.13      |\n",
      "|    value_loss         | 0.25      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 77        |\n",
      "|    iterations         | 18700     |\n",
      "|    time_elapsed       | 1207      |\n",
      "|    total_timesteps    | 93500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.9     |\n",
      "|    explained_variance | 0.000261  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18699     |\n",
      "|    policy_loss        | -4.75     |\n",
      "|    reward             | 1.3832048 |\n",
      "|    std                | 1.14      |\n",
      "|    value_loss         | 0.187     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 77        |\n",
      "|    iterations         | 18800     |\n",
      "|    time_elapsed       | 1211      |\n",
      "|    total_timesteps    | 94000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.9     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18799     |\n",
      "|    policy_loss        | -14.5     |\n",
      "|    reward             | 0.7342665 |\n",
      "|    std                | 1.14      |\n",
      "|    value_loss         | 1.6       |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 77          |\n",
      "|    iterations         | 18900       |\n",
      "|    time_elapsed       | 1215        |\n",
      "|    total_timesteps    | 94500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -13.9       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 18899       |\n",
      "|    policy_loss        | 32.8        |\n",
      "|    reward             | -0.24431694 |\n",
      "|    std                | 1.14        |\n",
      "|    value_loss         | 18.2        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 77        |\n",
      "|    iterations         | 19000     |\n",
      "|    time_elapsed       | 1219      |\n",
      "|    total_timesteps    | 95000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.9     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18999     |\n",
      "|    policy_loss        | -11.3     |\n",
      "|    reward             | 1.0667012 |\n",
      "|    std                | 1.14      |\n",
      "|    value_loss         | 1.03      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 78         |\n",
      "|    iterations         | 19100      |\n",
      "|    time_elapsed       | 1223       |\n",
      "|    total_timesteps    | 95500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.9      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 19099      |\n",
      "|    policy_loss        | -87.4      |\n",
      "|    reward             | 0.51545167 |\n",
      "|    std                | 1.14       |\n",
      "|    value_loss         | 34.2       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 78         |\n",
      "|    iterations         | 19200      |\n",
      "|    time_elapsed       | 1227       |\n",
      "|    total_timesteps    | 96000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -14        |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 19199      |\n",
      "|    policy_loss        | -0.142     |\n",
      "|    reward             | -1.0298259 |\n",
      "|    std                | 1.14       |\n",
      "|    value_loss         | 1.01       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 78         |\n",
      "|    iterations         | 19300      |\n",
      "|    time_elapsed       | 1231       |\n",
      "|    total_timesteps    | 96500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -14        |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 19299      |\n",
      "|    policy_loss        | -44.3      |\n",
      "|    reward             | -0.5360519 |\n",
      "|    std                | 1.15       |\n",
      "|    value_loss         | 10.7       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 78        |\n",
      "|    iterations         | 19400     |\n",
      "|    time_elapsed       | 1235      |\n",
      "|    total_timesteps    | 97000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.9     |\n",
      "|    explained_variance | -2.38e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19399     |\n",
      "|    policy_loss        | 109       |\n",
      "|    reward             | 0.730696  |\n",
      "|    std                | 1.14      |\n",
      "|    value_loss         | 72.5      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 78         |\n",
      "|    iterations         | 19500      |\n",
      "|    time_elapsed       | 1239       |\n",
      "|    total_timesteps    | 97500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.9      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 19499      |\n",
      "|    policy_loss        | -44.7      |\n",
      "|    reward             | -1.8205435 |\n",
      "|    std                | 1.14       |\n",
      "|    value_loss         | 12.8       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 78         |\n",
      "|    iterations         | 19600      |\n",
      "|    time_elapsed       | 1243       |\n",
      "|    total_timesteps    | 98000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.9      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 19599      |\n",
      "|    policy_loss        | 25.3       |\n",
      "|    reward             | 0.56656307 |\n",
      "|    std                | 1.14       |\n",
      "|    value_loss         | 8.57       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 78        |\n",
      "|    iterations         | 19700     |\n",
      "|    time_elapsed       | 1248      |\n",
      "|    total_timesteps    | 98500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.9     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19699     |\n",
      "|    policy_loss        | -34.8     |\n",
      "|    reward             | 0.1854098 |\n",
      "|    std                | 1.14      |\n",
      "|    value_loss         | 9.35      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 79         |\n",
      "|    iterations         | 19800      |\n",
      "|    time_elapsed       | 1251       |\n",
      "|    total_timesteps    | 99000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -14        |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 19799      |\n",
      "|    policy_loss        | 15.8       |\n",
      "|    reward             | -6.8821073 |\n",
      "|    std                | 1.14       |\n",
      "|    value_loss         | 1.91       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 79        |\n",
      "|    iterations         | 19900     |\n",
      "|    time_elapsed       | 1255      |\n",
      "|    total_timesteps    | 99500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -14       |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19899     |\n",
      "|    policy_loss        | -15.7     |\n",
      "|    reward             | -2.302714 |\n",
      "|    std                | 1.14      |\n",
      "|    value_loss         | 1.62      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 79       |\n",
      "|    iterations         | 20000    |\n",
      "|    time_elapsed       | 1260     |\n",
      "|    total_timesteps    | 100000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -14      |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 19999    |\n",
      "|    policy_loss        | -12.2    |\n",
      "|    reward             | -4.39326 |\n",
      "|    std                | 1.15     |\n",
      "|    value_loss         | 9.85     |\n",
      "------------------------------------\n",
      "hit end!\n",
      "==============Get Baseline Stats===========\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Shape of DataFrame:  (400, 8)\n",
      "Annual return         -0.022145\n",
      "Cumulative returns    -0.034921\n",
      "Annual volatility      0.176822\n",
      "Sharpe ratio          -0.038668\n",
      "Calmar ratio          -0.100930\n",
      "Stability              0.215672\n",
      "Max drawdown          -0.219408\n",
      "Omega ratio            0.993578\n",
      "Sortino ratio         -0.053987\n",
      "Skew                        NaN\n",
      "Kurtosis                    NaN\n",
      "Tail ratio             1.003436\n",
      "Daily value at risk   -0.022305\n",
      "dtype: float64\n",
      "result:                       a2c           dji\n",
      "date                                  \n",
      "2021-10-01  1.000000e+06  1.000000e+06\n",
      "2021-10-04  9.999550e+05  9.905746e+05\n",
      "2021-10-05  1.000247e+06  9.996566e+05\n",
      "2021-10-06  1.000412e+06  1.002637e+06\n",
      "2021-10-07  1.001332e+06  1.012483e+06\n",
      "...                  ...           ...\n",
      "2023-04-27  1.146288e+06  9.854252e+05\n",
      "2023-04-28  1.154392e+06  9.933491e+05\n",
      "2023-05-01  1.152220e+06  9.919956e+05\n",
      "2023-05-02  1.134295e+06  9.812993e+05\n",
      "2023-05-03  1.123415e+06  9.734251e+05\n",
      "\n",
      "[399 rows x 2 columns]\n",
      "A2C로 얻은 투자 수익률>> 12.34 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\se99a\\anaconda3\\lib\\site-packages\\pyfolio\\timeseries.py:724: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  stats = pd.Series()\n"
     ]
    }
   ],
   "source": [
    "buy_cost_list = sell_cost_list = [0.001] * stock_dimension\n",
    "num_stock_shares = [0] * stock_dimension\n",
    "\n",
    "env_kwargs = {\n",
    "    \"hmax\": 100,\n",
    "    \"initial_amount\": 1000000,\n",
    "    \"num_stock_shares\": num_stock_shares,\n",
    "    \"buy_cost_pct\": buy_cost_list,\n",
    "    \"sell_cost_pct\": sell_cost_list,\n",
    "    \"state_space\": state_space,\n",
    "    \"stock_dim\": stock_dimension,\n",
    "    \"tech_indicator_list\": INDICATORS,\n",
    "    \"action_space\": stock_dimension,\n",
    "    \"reward_scaling\": 1e-4\n",
    "}\n",
    "\n",
    "Earn_list = []\n",
    "\n",
    "for i in range(10):\n",
    "  e_train_gym = StockTradingEnv(df = train, **env_kwargs)\n",
    "\n",
    "  env_train, _ = e_train_gym.get_sb_env()\n",
    "\n",
    "  if_using_a2c = True ##a2c만 사용해보자\n",
    "\n",
    "  agent = DRLAgent(env = env_train)\n",
    "  model_a2c = agent.get_model(\"a2c\")\n",
    "\n",
    "  if if_using_a2c:\n",
    "    # set up logger\n",
    "    tmp_path = RESULTS_DIR + '/a2c'\n",
    "    new_logger_a2c = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "    # Set new logger\n",
    "    model_a2c.set_logger(new_logger_a2c)\n",
    "    \n",
    "  trained_a2c = agent.train_model(model=model_a2c,\n",
    "                              tb_log_name='a2c',\n",
    "                              total_timesteps=100000) if if_using_a2c else None\n",
    "  data_risk_indicator = processed_full[(processed_full.date<TRAIN_END_DATE) & (processed_full.date>=TRAIN_START_DATE)]\n",
    "  insample_risk_indicator = data_risk_indicator.drop_duplicates(subset=['date'])\n",
    "\n",
    "\n",
    "  e_trade_gym = StockTradingEnv(df = trade, turbulence_threshold = 70,risk_indicator_col='vix', **env_kwargs)\n",
    "  # env_trade, obs_trade = e_trade_gym.get_sb_env()\n",
    "  trained_model = trained_a2c\n",
    "  df_account_value_a2c, df_actions_a2c = DRLAgent.DRL_prediction(\n",
    "      model=trained_model,\n",
    "      environment = e_trade_gym)\n",
    "  \n",
    "  df_result_a2c = df_account_value_a2c.set_index(df_account_value_a2c.columns[0])\n",
    "  df_account_value_a2c.to_csv(\"df_account_value_a2c.csv\")\n",
    "  #baseline stats\n",
    "  print(\"==============Get Baseline Stats===========\")\n",
    "  df_dji_ = get_baseline(\n",
    "          ticker=\"^DJI\",\n",
    "          start = TRADE_START_DATE,\n",
    "          end = TRADE_END_DATE)\n",
    "  stats = backtest_stats(df_dji_, value_col_name = 'close')\n",
    "  df_dji = pd.DataFrame()\n",
    "  df_dji['date'] = df_account_value_a2c['date']\n",
    "  df_dji['account_value'] = df_dji_['close'] / df_dji_['close'][0] * env_kwargs[\"initial_amount\"]\n",
    "  df_dji.to_csv(\"df_dji.csv\")\n",
    "  df_dji = df_dji.set_index(df_dji.columns[0])\n",
    "  df_dji.to_csv(\"df_dji+.csv\")\n",
    "\n",
    "  result = pd.DataFrame(df_result_a2c)\n",
    "\n",
    "\n",
    "  result = pd.merge(result, df_dji, left_index=True, right_index=True)\n",
    "\n",
    "\n",
    "  result.columns = ['a2c','dji']\n",
    "\n",
    "  print(\"result: \", result)\n",
    "  result.to_csv(\"result.csv\")\n",
    "  \n",
    "  print('A2C로 얻은 투자 수익률>>', round((df_result_a2c.iloc[-1,0]/df_result_a2c.iloc[0,0]-1)*100,2),'%')\n",
    "  \n",
    "  Earn = round((df_result_a2c.iloc[-1,0]/df_result_a2c.iloc[0,0]-1)*100,2)\n",
    "  \n",
    "  Earn_list.append(Earn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "평균 수익률>> 2.38 %\n",
      "표준편차>> 7.13 %\n"
     ]
    }
   ],
   "source": [
    "print('평균 수익률>>', round(np.mean(Earn_list),2),'%')\n",
    "print('표준편차>>', round(np.std(Earn_list),2),'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>수익률</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-6.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-10.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>12.34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     수익률\n",
       "0  -6.43\n",
       "1 -10.46\n",
       "2   2.28\n",
       "3  -0.51\n",
       "4   9.06\n",
       "5   8.84\n",
       "6  -0.47\n",
       "7   9.76\n",
       "8  -0.60\n",
       "9  12.34"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(Earn_list, columns=['수익률'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
