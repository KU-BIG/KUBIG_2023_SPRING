{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 런타임 유형 GPU로 바꾸고 시작하기!"
      ],
      "metadata": {
        "id": "i367fmMGHYbo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iq4IhdTa7idB",
        "outputId": "b801f7bf-50ff-4edc-dfe6-4ad5dab157b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import re"
      ],
      "metadata": {
        "id": "zoBHU87O4L4z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/kubig/nlp/transcript_data_.csv\", encoding='latin_1')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ALcipYKW8vSW",
        "outputId": "f6fc898f-73eb-4436-f4dd-28e59735b4f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-09c954e91848>:1: DtypeWarning: Columns (244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(\"/content/drive/MyDrive/kubig/nlp/transcript_data_.csv\", encoding='latin_1')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#df['Unnamed: 2'].isnull().sum()"
      ],
      "metadata": {
        "id": "F6mho2sM9Iwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[df['Unnamed: 2'].isnull()]\n",
        "df = df[df['transcript'].isnull()==False]\n",
        "df = df.iloc[:,:2]\n",
        "df.reset_index(drop=True,inplace=True)"
      ],
      "metadata": {
        "id": "CAamVq5h2FlW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "id": "fI-9HHGoMfmt",
        "outputId": "3f613e5c-4687-44e5-b58c-366e5ba0aefd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              title  \\\n",
              "0  Can you outsmart the apples and oranges fallacy?   \n",
              "1           The exploitation of US college athletes   \n",
              "2                         How does ultrasound work?   \n",
              "3  An honest history of an ancient and \"nasty\" word   \n",
              "4   The electrical blueprints that orchestrate life   \n",
              "\n",
              "                                          transcript  \n",
              "0  Baking apple pie? Discount orange warehouse ha...  \n",
              "1  In college sports, American universities are e...  \n",
              "2  In a pitch-black cave, bats can?t see much. B...  \n",
              "3  First, a warning. As far as offensive words go...  \n",
              "4  Chris Anderson: Mike, welcome. It's good to se...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-edec3d3e-54a2-4558-b55a-14fb2b2c5674\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>transcript</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Can you outsmart the apples and oranges fallacy?</td>\n",
              "      <td>Baking apple pie? Discount orange warehouse ha...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>The exploitation of US college athletes</td>\n",
              "      <td>In college sports, American universities are e...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>How does ultrasound work?</td>\n",
              "      <td>In a pitch-black cave, bats can?t see much. B...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>An honest history of an ancient and \"nasty\" word</td>\n",
              "      <td>First, a warning. As far as offensive words go...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The electrical blueprints that orchestrate life</td>\n",
              "      <td>Chris Anderson: Mike, welcome. It's good to se...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-edec3d3e-54a2-4558-b55a-14fb2b2c5674')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-edec3d3e-54a2-4558-b55a-14fb2b2c5674 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-edec3d3e-54a2-4558-b55a-14fb2b2c5674');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(df):\n",
        "  list_=[]\n",
        "  for i in range(len(df)):\n",
        "    text = df['transcript'][i]\n",
        "    letters_only = re.sub('[^a-zA-Z`,.]', ' ', text)\n",
        "    lower_letters = letters_only.lower()\n",
        "    spacing = re.sub(' +', ' ', lower_letters)\n",
        "    list_.append(spacing)\n",
        "  df['preprocessed']=list_\n",
        "  return df"
      ],
      "metadata": {
        "id": "wsUTczF-4UhC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = preprocess(df)"
      ],
      "metadata": {
        "id": "kdVqJsZSVVV3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop([\"transcript\"],axis=1)"
      ],
      "metadata": {
        "id": "Kq02sdzYwQvd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(\"preprocessed.csv\")"
      ],
      "metadata": {
        "id": "oBzlEz07wZqG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import json\n",
        "\n",
        "def csv_to_jsonl(csv_file, jsonl_file):\n",
        "    with open(csv_file, 'r', encoding='utf-8') as csv_in, open(jsonl_file, 'w', encoding='utf-8') as jsonl_out:\n",
        "        reader = csv.DictReader(csv_in)\n",
        "        for row in reader:\n",
        "            jsonl_out.write(json.dumps(row, ensure_ascii=False) + '\\n')\n",
        "\n",
        "# CSV 파일 경로와 이름\n",
        "csv_file = 'preprocessed.csv'\n",
        "\n",
        "# JSONL 파일 경로와 이름\n",
        "jsonl_file = 'output.jsonl'\n",
        "\n",
        "# CSV 파일을 JSONL 파일로 변환\n",
        "csv_to_jsonl(csv_file, jsonl_file)\n"
      ],
      "metadata": {
        "id": "EeW0HdxDvJe3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_json = pd.read_json('preprocessed.jsonl')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327
        },
        "id": "CBTUltAovjio",
        "outputId": "c8d76393-1b2a-455b-f6ca-deb02c7b3959"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-c6202fe7cf4e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'preprocessed.jsonl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mread_json\u001b[0;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, numpy, precise_float, date_unit, encoding, encoding_errors, lines, chunksize, compression, nrows, storage_options)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mjson_reader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mjson_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    758\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    913\u001b[0m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_object_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_combine_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_lines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_object_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36m_get_object_parser\u001b[0;34m(self, json)\u001b[0m\n\u001b[1;32m    935\u001b[0m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"frame\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 937\u001b[0;31m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFrameParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    938\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"series\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1062\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1063\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1064\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_no_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1066\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36m_parse_no_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1319\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0morient\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m             self.obj = DataFrame(\n\u001b[0;32m-> 1321\u001b[0;31m                 \u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecise_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecise_float\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1322\u001b[0m             )\n\u001b[1;32m   1323\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0morient\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"split\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Expected object or value"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j27_8cyTcxYf",
        "outputId": "8a99bd3b-04a0-4708-a701-752a7134540c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m98.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m80.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.6.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.15.1 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.30.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "gpt 영어 모델 괜찮은 거 hugging face에서 찾기\n"
      ],
      "metadata": {
        "id": "EEbSim8pjmUO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizer 설정\n",
        "from transformers import GPT2Tokenizer, GPT2TokenizerFast, GPT2Model\n",
        "\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained('gpt2',\n",
        "                                          bos_token='<|startoftext|>',\n",
        "                                          eos_token='<|endoftext|>',\n",
        "                                          pad_token='<|pad|>')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sXL4qKkZcyQ2",
        "outputId": "8afc7cd6-0a5e-4bcd-fad0-0d878fa43233"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"본 모델의 최대 길이는 {} 입니다.\".format(tokenizer.model_max_length))\n",
        "print(\"Sequence token {}의 처음은 {} id를 갖고 있습니다.\".format(tokenizer.convert_ids_to_tokens(tokenizer.bos_token_id), tokenizer.bos_token_id))\n",
        "print(\"Sequence token {}의 끝은 {} id를 갖고 있습니다.\".format(tokenizer.convert_ids_to_tokens(tokenizer.eos_token_id), tokenizer.eos_token_id))\n",
        "print(\"Padding token {}은 {} id를 갖고 있습니다.\".format(tokenizer.convert_ids_to_tokens(tokenizer.pad_token_id), tokenizer.pad_token_id))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "en_Lc3UgwkIi",
        "outputId": "779d28f7-29bd-4d25-d301-d73eb8acf6d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "본 모델의 최대 길이는 1024 입니다.\n",
            "Sequence token <|startoftext|>의 처음은 50257 id를 갖고 있습니다.\n",
            "Sequence token <|endoftext|>의 끝은 50256 id를 갖고 있습니다.\n",
            "Padding token <|pad|>은 50258 id를 갖고 있습니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 예시\n",
        "example = df['preprocessed'][0]\n",
        "print(example)\n",
        "\n",
        "print(tokenizer.encode('<|startoftext|>' + example + '<|endoftext|>',\n",
        "      truncation=True, max_length=768, padding=\"max_length\")[1:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CDE8uWdMw4VA",
        "outputId": "fb0cdcbd-f80b-4fdb-ecf2-d55c8981e483"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "baking apple pie discount orange warehouse has you covered a fruit s a fruit, right it s , and scientist james hansen has just testified to the united states congress that global warming trends are caused by human activity, and will pose an increasing threat to humanity in the future.well, well. that s unusually prescient for a human.looking for a wedding dress try a new take on a timeless classic. it s sleek, flattering and modest just like the traditional dress.commercials. could anything be more insufferable it s , and the united states senate has called a hearing about global warming. some expert witnesses point out that past periods in earth s history were warmer than the th century. because such variations existed long before humans, the witnesses claim the current warming trend is also the result of natural variation.ah, there is something more insufferable than a commercial. luckily for the humans, there s one more expert witness.what are you looking at we re all dressed. at least we are by the logic you just used. it s as if you were to say apples and oranges are both fruits, therefore they taste the same. or that underwear, wedding dresses, and suits are all clothes, therefore, they re all equally appropriate attire for a senate hearing.the european wars of the th century and world war i were all wars, right so world war i couldn t be any more devastating than those other wars, could it let s say two people have a fever. they must have the same disease that s causing that fever, right of course not. one fever could be caused by chicken pox, the other by influenza, or any number of other infections. like your claim about rising global temperatures, these claims make a false analogy. you re assuming that because two phenomena share a characteristic, in this case warming, they are analogous in other ways, like the cause of that warming.but there s no evidence that that s the case. yes, there have been other warm periods in earth s history no one s disputing that the climate fluctuates. but let s take a closer look at some of those older examples of global warming, shall we the cretaceous hot greenhouse, million years ago, was so warm, forests covered antarctica. volcanic activity was likely responsible for boosting atmospheric carbon dioxide and creating a greenhouse effect.the paleocene eocene thermal maximum, million years ago, was so warm, crocodiles swam the waters of the arctic circle. this warming may have been caused by the drying of inland seas and release of methane, a potent greenhouse gas, from ocean sediments,even among these other warm periods, you re making a false analogy. yes, they had natural causes. but each had a different cause, and involved a different amount and duration of warming. they re as dissimilar as they are similar. taking them together, all we can reasonably conclude is that the earth s climate seems to change in response to conditions on the planet.today, human activity is a dominant force shaping conditions on your planet, so the possibility that it s driving global warming can t be dismissed out of hand. i ll grant that the more complicated something is, the easier it is to make a mistaken analogy. that s especially true because there are many different types of false analogy that similar symptoms must share a cause, that similar actions must lead to similar consequences, and countless others. most false analogies you ll come across are far less obvious than those comparing apples to oranges, and climate is notoriously complex. it requires careful, rigorous study and evidence collection and making a false analogy like this only impedes that process.it s , and the united nations intergovernmental panel on climate change has found, aggregating decades of research, that there is more than a chance the global warming trend since the mid th century has been driven by human activity, namely the burning of fossil fuels.you re both pets, and he likes living in water, so you should, too.\n",
            "[65, 868, 17180, 2508, 9780, 10912, 20933, 468, 345, 5017, 257, 8234, 264, 257, 8234, 11, 826, 340, 264, 837, 290, 11444, 474, 1047, 289, 33807, 468, 655, 15463, 284, 262, 16503, 2585, 8681, 326, 3298, 9917, 11257, 389, 4073, 416, 1692, 3842, 11, 290, 481, 12705, 281, 3649, 2372, 284, 9265, 287, 262, 2003, 13, 4053, 11, 880, 13, 326, 264, 23708, 906, 3456, 329, 257, 1692, 13, 11534, 329, 257, 10614, 6576, 1949, 257, 649, 1011, 319, 257, 36464, 6833, 13, 340, 264, 33143, 11, 48259, 290, 12949, 655, 588, 262, 4569, 6576, 13, 36313, 82, 13, 714, 1997, 307, 517, 1035, 13712, 540, 340, 264, 837, 290, 262, 16503, 2585, 34548, 468, 1444, 257, 4854, 546, 3298, 9917, 13, 617, 5887, 11432, 966, 503, 326, 1613, 9574, 287, 4534, 264, 2106, 547, 23254, 621, 262, 294, 4289, 13, 780, 884, 13991, 11196, 890, 878, 5384, 11, 262, 11432, 1624, 262, 1459, 9917, 5182, 318, 635, 262, 1255, 286, 3288, 12291, 13, 993, 11, 612, 318, 1223, 517, 1035, 13712, 540, 621, 257, 5068, 13, 45120, 329, 262, 5384, 11, 612, 264, 530, 517, 5887, 4973, 13, 10919, 389, 345, 2045, 379, 356, 302, 477, 12049, 13, 379, 1551, 356, 389, 416, 262, 9156, 345, 655, 973, 13, 340, 264, 355, 611, 345, 547, 284, 910, 22514, 290, 48389, 389, 1111, 15921, 11, 4361, 484, 6938, 262, 976, 13, 393, 326, 26170, 11, 10614, 27309, 11, 290, 14803, 389, 477, 8242, 11, 4361, 11, 484, 302, 477, 8603, 5035, 37629, 329, 257, 34548, 4854, 13, 1169, 11063, 431, 272, 9976, 286, 262, 294, 4289, 290, 995, 1175, 1312, 547, 477, 9976, 11, 826, 523, 995, 1175, 1312, 3521, 256, 307, 597, 517, 14101, 621, 883, 584, 9976, 11, 714, 340, 1309, 264, 910, 734, 661, 423, 257, 17372, 13, 484, 1276, 423, 262, 976, 4369, 326, 264, 6666, 326, 17372, 11, 826, 286, 1781, 407, 13, 530, 17372, 714, 307, 4073, 416, 9015, 745, 87, 11, 262, 584, 416, 29491, 11, 393, 597, 1271, 286, 584, 16079, 13, 588, 534, 1624, 546, 7396, 3298, 10101, 11, 777, 3667, 787, 257, 3991, 23970, 13, 345, 302, 13148, 326, 780, 734, 19428, 2648, 257, 16704, 11, 287, 428, 1339, 9917, 11, 484, 389, 34657, 287, 584, 2842, 11, 588, 262, 2728, 286, 326, 9917, 13, 4360, 612, 264, 645, 2370, 326, 326, 264, 262, 1339, 13, 3763, 11, 612, 423, 587, 584, 5814, 9574, 287, 4534, 264, 2106, 645, 530, 264, 595, 48074, 326, 262, 4258, 19180, 12632, 13, 475, 1309, 264, 1011, 257, 5699, 804, 379, 617, 286, 883, 4697, 6096, 286, 3298, 9917, 11, 2236, 356, 262, 1126, 83, 37797, 3024, 16325, 11, 1510, 812, 2084, 11, 373, 523, 5814, 11, 17039, 5017, 1885, 283, 28914, 13, 31513, 3842, 373, 1884, 4497, 329, 27611, 20938, 6588, 17556, 290, 4441, 257, 16325, 1245, 13, 1169, 14005, 34973, 304, 34973, 18411, 5415, 11, 1510, 812, 2084, 11, 373, 523, 5814, 11, 37565, 2915, 1509, 321, 262, 10150, 286, 262, 610, 11048, 9197, 13, 428, 9917, 743, 423, 587, 4073, 416, 262, 29621, 286, 37874, 21547, 290, 2650, 286, 25006, 11, 257, 16739, 16325, 3623, 11, 422, 9151, 10081, 6800, 11, 10197, 1871, 777, 584, 5814, 9574, 11, 345, 302, 1642, 257, 3991, 23970, 13, 3763, 11, 484, 550, 3288, 5640, 13, 475, 1123, 550, 257, 1180, 2728, 11, 290, 2950, 257, 1180, 2033, 290, 9478, 286, 9917, 13, 484, 302, 355, 6249, 49941, 355, 484, 389, 2092, 13, 2263, 606, 1978, 11, 477, 356, 460, 13025, 13796, 318, 326, 262, 4534, 264, 4258, 2331, 284, 1487, 287, 2882, 284, 3403, 319, 262, 5440, 13, 40838, 11, 1692, 3842, 318, 257, 11410, 2700, 23610, 3403, 319, 534, 5440, 11, 523, 262, 5885, 326, 340, 264, 5059, 3298, 9917, 460, 256, 307, 11126, 503, 286, 1021, 13, 1312, 32660, 7264, 326, 262, 517, 8253, 1223, 318, 11, 262, 4577, 340, 318, 284, 787, 257, 16011, 23970, 13, 326, 264, 2592, 2081, 780, 612, 389, 867, 1180, 3858, 286, 3991, 23970, 326, 2092, 7460, 1276, 2648, 257, 2728, 11, 326, 2092, 4028, 1276, 1085, 284, 2092, 6948, 11, 290, 12925, 1854, 13, 749, 3991, 15075, 444, 345, 32660, 1282, 1973, 389, 1290, 1342, 3489, 621, 883, 14176, 22514, 284, 48389, 11, 290, 4258, 318, 29862, 3716, 13, 340, 4433, 8161, 11, 22888, 2050, 290, 2370, 4947, 290, 1642, 257, 3991, 23970, 588, 428, 691, 26795, 274, 326, 1429, 13, 270, 264, 837, 290, 262, 16503, 7027, 987, 31353, 6103, 319, 4258, 1487, 468, 1043, 11, 13262, 803, 4647, 286, 2267, 11, 326, 612, 318, 517, 621, 257, 2863, 262, 3298]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tok_ex = tokenizer(example)\n",
        "print(tok_ex)\n",
        "print(tok_ex.keys())\n",
        "\n",
        "print(len(tok_ex['input_ids']))\n",
        "print(len(tok_ex['attention_mask']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ra-eRs_6kVJ0",
        "outputId": "80ff0c42-c38d-41bc-9cbb-feb2f7481445"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': [65, 868, 17180, 2508, 9780, 10912, 20933, 468, 345, 5017, 257, 8234, 264, 257, 8234, 11, 826, 340, 264, 837, 290, 11444, 474, 1047, 289, 33807, 468, 655, 15463, 284, 262, 16503, 2585, 8681, 326, 3298, 9917, 11257, 389, 4073, 416, 1692, 3842, 11, 290, 481, 12705, 281, 3649, 2372, 284, 9265, 287, 262, 2003, 13, 4053, 11, 880, 13, 326, 264, 23708, 906, 3456, 329, 257, 1692, 13, 11534, 329, 257, 10614, 6576, 1949, 257, 649, 1011, 319, 257, 36464, 6833, 13, 340, 264, 33143, 11, 48259, 290, 12949, 655, 588, 262, 4569, 6576, 13, 36313, 82, 13, 714, 1997, 307, 517, 1035, 13712, 540, 340, 264, 837, 290, 262, 16503, 2585, 34548, 468, 1444, 257, 4854, 546, 3298, 9917, 13, 617, 5887, 11432, 966, 503, 326, 1613, 9574, 287, 4534, 264, 2106, 547, 23254, 621, 262, 294, 4289, 13, 780, 884, 13991, 11196, 890, 878, 5384, 11, 262, 11432, 1624, 262, 1459, 9917, 5182, 318, 635, 262, 1255, 286, 3288, 12291, 13, 993, 11, 612, 318, 1223, 517, 1035, 13712, 540, 621, 257, 5068, 13, 45120, 329, 262, 5384, 11, 612, 264, 530, 517, 5887, 4973, 13, 10919, 389, 345, 2045, 379, 356, 302, 477, 12049, 13, 379, 1551, 356, 389, 416, 262, 9156, 345, 655, 973, 13, 340, 264, 355, 611, 345, 547, 284, 910, 22514, 290, 48389, 389, 1111, 15921, 11, 4361, 484, 6938, 262, 976, 13, 393, 326, 26170, 11, 10614, 27309, 11, 290, 14803, 389, 477, 8242, 11, 4361, 11, 484, 302, 477, 8603, 5035, 37629, 329, 257, 34548, 4854, 13, 1169, 11063, 431, 272, 9976, 286, 262, 294, 4289, 290, 995, 1175, 1312, 547, 477, 9976, 11, 826, 523, 995, 1175, 1312, 3521, 256, 307, 597, 517, 14101, 621, 883, 584, 9976, 11, 714, 340, 1309, 264, 910, 734, 661, 423, 257, 17372, 13, 484, 1276, 423, 262, 976, 4369, 326, 264, 6666, 326, 17372, 11, 826, 286, 1781, 407, 13, 530, 17372, 714, 307, 4073, 416, 9015, 745, 87, 11, 262, 584, 416, 29491, 11, 393, 597, 1271, 286, 584, 16079, 13, 588, 534, 1624, 546, 7396, 3298, 10101, 11, 777, 3667, 787, 257, 3991, 23970, 13, 345, 302, 13148, 326, 780, 734, 19428, 2648, 257, 16704, 11, 287, 428, 1339, 9917, 11, 484, 389, 34657, 287, 584, 2842, 11, 588, 262, 2728, 286, 326, 9917, 13, 4360, 612, 264, 645, 2370, 326, 326, 264, 262, 1339, 13, 3763, 11, 612, 423, 587, 584, 5814, 9574, 287, 4534, 264, 2106, 645, 530, 264, 595, 48074, 326, 262, 4258, 19180, 12632, 13, 475, 1309, 264, 1011, 257, 5699, 804, 379, 617, 286, 883, 4697, 6096, 286, 3298, 9917, 11, 2236, 356, 262, 1126, 83, 37797, 3024, 16325, 11, 1510, 812, 2084, 11, 373, 523, 5814, 11, 17039, 5017, 1885, 283, 28914, 13, 31513, 3842, 373, 1884, 4497, 329, 27611, 20938, 6588, 17556, 290, 4441, 257, 16325, 1245, 13, 1169, 14005, 34973, 304, 34973, 18411, 5415, 11, 1510, 812, 2084, 11, 373, 523, 5814, 11, 37565, 2915, 1509, 321, 262, 10150, 286, 262, 610, 11048, 9197, 13, 428, 9917, 743, 423, 587, 4073, 416, 262, 29621, 286, 37874, 21547, 290, 2650, 286, 25006, 11, 257, 16739, 16325, 3623, 11, 422, 9151, 10081, 6800, 11, 10197, 1871, 777, 584, 5814, 9574, 11, 345, 302, 1642, 257, 3991, 23970, 13, 3763, 11, 484, 550, 3288, 5640, 13, 475, 1123, 550, 257, 1180, 2728, 11, 290, 2950, 257, 1180, 2033, 290, 9478, 286, 9917, 13, 484, 302, 355, 6249, 49941, 355, 484, 389, 2092, 13, 2263, 606, 1978, 11, 477, 356, 460, 13025, 13796, 318, 326, 262, 4534, 264, 4258, 2331, 284, 1487, 287, 2882, 284, 3403, 319, 262, 5440, 13, 40838, 11, 1692, 3842, 318, 257, 11410, 2700, 23610, 3403, 319, 534, 5440, 11, 523, 262, 5885, 326, 340, 264, 5059, 3298, 9917, 460, 256, 307, 11126, 503, 286, 1021, 13, 1312, 32660, 7264, 326, 262, 517, 8253, 1223, 318, 11, 262, 4577, 340, 318, 284, 787, 257, 16011, 23970, 13, 326, 264, 2592, 2081, 780, 612, 389, 867, 1180, 3858, 286, 3991, 23970, 326, 2092, 7460, 1276, 2648, 257, 2728, 11, 326, 2092, 4028, 1276, 1085, 284, 2092, 6948, 11, 290, 12925, 1854, 13, 749, 3991, 15075, 444, 345, 32660, 1282, 1973, 389, 1290, 1342, 3489, 621, 883, 14176, 22514, 284, 48389, 11, 290, 4258, 318, 29862, 3716, 13, 340, 4433, 8161, 11, 22888, 2050, 290, 2370, 4947, 290, 1642, 257, 3991, 23970, 588, 428, 691, 26795, 274, 326, 1429, 13, 270, 264, 837, 290, 262, 16503, 7027, 987, 31353, 6103, 319, 4258, 1487, 468, 1043, 11, 13262, 803, 4647, 286, 2267, 11, 326, 612, 318, 517, 621, 257, 2863, 262, 3298, 9917, 5182, 1201, 262, 3095, 294, 4289, 468, 587, 7986, 416, 1692, 3842, 11, 14811, 262, 9482, 286, 12584, 18017, 13, 5832, 302, 1111, 17252, 11, 290, 339, 7832, 2877, 287, 1660, 11, 523, 345, 815, 11, 1165, 13], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
            "dict_keys(['input_ids', 'attention_mask'])\n",
            "806\n",
            "806\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in tok_ex['input_ids']:\n",
        "  print(tokenizer.convert_ids_to_tokens(i))\n",
        "\n",
        "#출력된 거 보니까 G처럼 생긴 저 문자가 뭔진 모르겠는데 그냥 무시해도 되는듯!"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWa7ibLLxXXA",
        "outputId": "ce3612a5-f9e4-4895-dd81-1518d3ff604f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b\n",
            "aking\n",
            "Ġapple\n",
            "Ġpie\n",
            "Ġdiscount\n",
            "Ġorange\n",
            "Ġwarehouse\n",
            "Ġhas\n",
            "Ġyou\n",
            "Ġcovered\n",
            "Ġa\n",
            "Ġfruit\n",
            "Ġs\n",
            "Ġa\n",
            "Ġfruit\n",
            ",\n",
            "Ġright\n",
            "Ġit\n",
            "Ġs\n",
            "Ġ,\n",
            "Ġand\n",
            "Ġscientist\n",
            "Ġj\n",
            "ames\n",
            "Ġh\n",
            "ansen\n",
            "Ġhas\n",
            "Ġjust\n",
            "Ġtestified\n",
            "Ġto\n",
            "Ġthe\n",
            "Ġunited\n",
            "Ġstates\n",
            "Ġcongress\n",
            "Ġthat\n",
            "Ġglobal\n",
            "Ġwarming\n",
            "Ġtrends\n",
            "Ġare\n",
            "Ġcaused\n",
            "Ġby\n",
            "Ġhuman\n",
            "Ġactivity\n",
            ",\n",
            "Ġand\n",
            "Ġwill\n",
            "Ġpose\n",
            "Ġan\n",
            "Ġincreasing\n",
            "Ġthreat\n",
            "Ġto\n",
            "Ġhumanity\n",
            "Ġin\n",
            "Ġthe\n",
            "Ġfuture\n",
            ".\n",
            "well\n",
            ",\n",
            "Ġwell\n",
            ".\n",
            "Ġthat\n",
            "Ġs\n",
            "Ġunusually\n",
            "Ġpres\n",
            "cient\n",
            "Ġfor\n",
            "Ġa\n",
            "Ġhuman\n",
            ".\n",
            "looking\n",
            "Ġfor\n",
            "Ġa\n",
            "Ġwedding\n",
            "Ġdress\n",
            "Ġtry\n",
            "Ġa\n",
            "Ġnew\n",
            "Ġtake\n",
            "Ġon\n",
            "Ġa\n",
            "Ġtimeless\n",
            "Ġclassic\n",
            ".\n",
            "Ġit\n",
            "Ġs\n",
            "Ġsleek\n",
            ",\n",
            "Ġflattering\n",
            "Ġand\n",
            "Ġmodest\n",
            "Ġjust\n",
            "Ġlike\n",
            "Ġthe\n",
            "Ġtraditional\n",
            "Ġdress\n",
            ".\n",
            "commercial\n",
            "s\n",
            ".\n",
            "Ġcould\n",
            "Ġanything\n",
            "Ġbe\n",
            "Ġmore\n",
            "Ġins\n",
            "uffer\n",
            "able\n",
            "Ġit\n",
            "Ġs\n",
            "Ġ,\n",
            "Ġand\n",
            "Ġthe\n",
            "Ġunited\n",
            "Ġstates\n",
            "Ġsenate\n",
            "Ġhas\n",
            "Ġcalled\n",
            "Ġa\n",
            "Ġhearing\n",
            "Ġabout\n",
            "Ġglobal\n",
            "Ġwarming\n",
            ".\n",
            "Ġsome\n",
            "Ġexpert\n",
            "Ġwitnesses\n",
            "Ġpoint\n",
            "Ġout\n",
            "Ġthat\n",
            "Ġpast\n",
            "Ġperiods\n",
            "Ġin\n",
            "Ġearth\n",
            "Ġs\n",
            "Ġhistory\n",
            "Ġwere\n",
            "Ġwarmer\n",
            "Ġthan\n",
            "Ġthe\n",
            "Ġth\n",
            "Ġcentury\n",
            ".\n",
            "Ġbecause\n",
            "Ġsuch\n",
            "Ġvariations\n",
            "Ġexisted\n",
            "Ġlong\n",
            "Ġbefore\n",
            "Ġhumans\n",
            ",\n",
            "Ġthe\n",
            "Ġwitnesses\n",
            "Ġclaim\n",
            "Ġthe\n",
            "Ġcurrent\n",
            "Ġwarming\n",
            "Ġtrend\n",
            "Ġis\n",
            "Ġalso\n",
            "Ġthe\n",
            "Ġresult\n",
            "Ġof\n",
            "Ġnatural\n",
            "Ġvariation\n",
            ".\n",
            "ah\n",
            ",\n",
            "Ġthere\n",
            "Ġis\n",
            "Ġsomething\n",
            "Ġmore\n",
            "Ġins\n",
            "uffer\n",
            "able\n",
            "Ġthan\n",
            "Ġa\n",
            "Ġcommercial\n",
            ".\n",
            "Ġluckily\n",
            "Ġfor\n",
            "Ġthe\n",
            "Ġhumans\n",
            ",\n",
            "Ġthere\n",
            "Ġs\n",
            "Ġone\n",
            "Ġmore\n",
            "Ġexpert\n",
            "Ġwitness\n",
            ".\n",
            "what\n",
            "Ġare\n",
            "Ġyou\n",
            "Ġlooking\n",
            "Ġat\n",
            "Ġwe\n",
            "Ġre\n",
            "Ġall\n",
            "Ġdressed\n",
            ".\n",
            "Ġat\n",
            "Ġleast\n",
            "Ġwe\n",
            "Ġare\n",
            "Ġby\n",
            "Ġthe\n",
            "Ġlogic\n",
            "Ġyou\n",
            "Ġjust\n",
            "Ġused\n",
            ".\n",
            "Ġit\n",
            "Ġs\n",
            "Ġas\n",
            "Ġif\n",
            "Ġyou\n",
            "Ġwere\n",
            "Ġto\n",
            "Ġsay\n",
            "Ġapples\n",
            "Ġand\n",
            "Ġoranges\n",
            "Ġare\n",
            "Ġboth\n",
            "Ġfruits\n",
            ",\n",
            "Ġtherefore\n",
            "Ġthey\n",
            "Ġtaste\n",
            "Ġthe\n",
            "Ġsame\n",
            ".\n",
            "Ġor\n",
            "Ġthat\n",
            "Ġunderwear\n",
            ",\n",
            "Ġwedding\n",
            "Ġdresses\n",
            ",\n",
            "Ġand\n",
            "Ġsuits\n",
            "Ġare\n",
            "Ġall\n",
            "Ġclothes\n",
            ",\n",
            "Ġtherefore\n",
            ",\n",
            "Ġthey\n",
            "Ġre\n",
            "Ġall\n",
            "Ġequally\n",
            "Ġappropriate\n",
            "Ġattire\n",
            "Ġfor\n",
            "Ġa\n",
            "Ġsenate\n",
            "Ġhearing\n",
            ".\n",
            "the\n",
            "Ġeuro\n",
            "pe\n",
            "an\n",
            "Ġwars\n",
            "Ġof\n",
            "Ġthe\n",
            "Ġth\n",
            "Ġcentury\n",
            "Ġand\n",
            "Ġworld\n",
            "Ġwar\n",
            "Ġi\n",
            "Ġwere\n",
            "Ġall\n",
            "Ġwars\n",
            ",\n",
            "Ġright\n",
            "Ġso\n",
            "Ġworld\n",
            "Ġwar\n",
            "Ġi\n",
            "Ġcouldn\n",
            "Ġt\n",
            "Ġbe\n",
            "Ġany\n",
            "Ġmore\n",
            "Ġdevastating\n",
            "Ġthan\n",
            "Ġthose\n",
            "Ġother\n",
            "Ġwars\n",
            ",\n",
            "Ġcould\n",
            "Ġit\n",
            "Ġlet\n",
            "Ġs\n",
            "Ġsay\n",
            "Ġtwo\n",
            "Ġpeople\n",
            "Ġhave\n",
            "Ġa\n",
            "Ġfever\n",
            ".\n",
            "Ġthey\n",
            "Ġmust\n",
            "Ġhave\n",
            "Ġthe\n",
            "Ġsame\n",
            "Ġdisease\n",
            "Ġthat\n",
            "Ġs\n",
            "Ġcausing\n",
            "Ġthat\n",
            "Ġfever\n",
            ",\n",
            "Ġright\n",
            "Ġof\n",
            "Ġcourse\n",
            "Ġnot\n",
            ".\n",
            "Ġone\n",
            "Ġfever\n",
            "Ġcould\n",
            "Ġbe\n",
            "Ġcaused\n",
            "Ġby\n",
            "Ġchicken\n",
            "Ġpo\n",
            "x\n",
            ",\n",
            "Ġthe\n",
            "Ġother\n",
            "Ġby\n",
            "Ġinfluenza\n",
            ",\n",
            "Ġor\n",
            "Ġany\n",
            "Ġnumber\n",
            "Ġof\n",
            "Ġother\n",
            "Ġinfections\n",
            ".\n",
            "Ġlike\n",
            "Ġyour\n",
            "Ġclaim\n",
            "Ġabout\n",
            "Ġrising\n",
            "Ġglobal\n",
            "Ġtemperatures\n",
            ",\n",
            "Ġthese\n",
            "Ġclaims\n",
            "Ġmake\n",
            "Ġa\n",
            "Ġfalse\n",
            "Ġanalogy\n",
            ".\n",
            "Ġyou\n",
            "Ġre\n",
            "Ġassuming\n",
            "Ġthat\n",
            "Ġbecause\n",
            "Ġtwo\n",
            "Ġphenomena\n",
            "Ġshare\n",
            "Ġa\n",
            "Ġcharacteristic\n",
            ",\n",
            "Ġin\n",
            "Ġthis\n",
            "Ġcase\n",
            "Ġwarming\n",
            ",\n",
            "Ġthey\n",
            "Ġare\n",
            "Ġanalogous\n",
            "Ġin\n",
            "Ġother\n",
            "Ġways\n",
            ",\n",
            "Ġlike\n",
            "Ġthe\n",
            "Ġcause\n",
            "Ġof\n",
            "Ġthat\n",
            "Ġwarming\n",
            ".\n",
            "but\n",
            "Ġthere\n",
            "Ġs\n",
            "Ġno\n",
            "Ġevidence\n",
            "Ġthat\n",
            "Ġthat\n",
            "Ġs\n",
            "Ġthe\n",
            "Ġcase\n",
            ".\n",
            "Ġyes\n",
            ",\n",
            "Ġthere\n",
            "Ġhave\n",
            "Ġbeen\n",
            "Ġother\n",
            "Ġwarm\n",
            "Ġperiods\n",
            "Ġin\n",
            "Ġearth\n",
            "Ġs\n",
            "Ġhistory\n",
            "Ġno\n",
            "Ġone\n",
            "Ġs\n",
            "Ġdis\n",
            "puting\n",
            "Ġthat\n",
            "Ġthe\n",
            "Ġclimate\n",
            "Ġfluct\n",
            "uates\n",
            ".\n",
            "Ġbut\n",
            "Ġlet\n",
            "Ġs\n",
            "Ġtake\n",
            "Ġa\n",
            "Ġcloser\n",
            "Ġlook\n",
            "Ġat\n",
            "Ġsome\n",
            "Ġof\n",
            "Ġthose\n",
            "Ġolder\n",
            "Ġexamples\n",
            "Ġof\n",
            "Ġglobal\n",
            "Ġwarming\n",
            ",\n",
            "Ġshall\n",
            "Ġwe\n",
            "Ġthe\n",
            "Ġcre\n",
            "t\n",
            "aceous\n",
            "Ġhot\n",
            "Ġgreenhouse\n",
            ",\n",
            "Ġmillion\n",
            "Ġyears\n",
            "Ġago\n",
            ",\n",
            "Ġwas\n",
            "Ġso\n",
            "Ġwarm\n",
            ",\n",
            "Ġforests\n",
            "Ġcovered\n",
            "Ġant\n",
            "ar\n",
            "ctica\n",
            ".\n",
            "Ġvolcanic\n",
            "Ġactivity\n",
            "Ġwas\n",
            "Ġlikely\n",
            "Ġresponsible\n",
            "Ġfor\n",
            "Ġboosting\n",
            "Ġatmospheric\n",
            "Ġcarbon\n",
            "Ġdioxide\n",
            "Ġand\n",
            "Ġcreating\n",
            "Ġa\n",
            "Ġgreenhouse\n",
            "Ġeffect\n",
            ".\n",
            "the\n",
            "Ġpale\n",
            "ocene\n",
            "Ġe\n",
            "ocene\n",
            "Ġthermal\n",
            "Ġmaximum\n",
            ",\n",
            "Ġmillion\n",
            "Ġyears\n",
            "Ġago\n",
            ",\n",
            "Ġwas\n",
            "Ġso\n",
            "Ġwarm\n",
            ",\n",
            "Ġcrocod\n",
            "iles\n",
            "Ġsw\n",
            "am\n",
            "Ġthe\n",
            "Ġwaters\n",
            "Ġof\n",
            "Ġthe\n",
            "Ġar\n",
            "ctic\n",
            "Ġcircle\n",
            ".\n",
            "Ġthis\n",
            "Ġwarming\n",
            "Ġmay\n",
            "Ġhave\n",
            "Ġbeen\n",
            "Ġcaused\n",
            "Ġby\n",
            "Ġthe\n",
            "Ġdrying\n",
            "Ġof\n",
            "Ġinland\n",
            "Ġseas\n",
            "Ġand\n",
            "Ġrelease\n",
            "Ġof\n",
            "Ġmethane\n",
            ",\n",
            "Ġa\n",
            "Ġpotent\n",
            "Ġgreenhouse\n",
            "Ġgas\n",
            ",\n",
            "Ġfrom\n",
            "Ġocean\n",
            "Ġsed\n",
            "iments\n",
            ",\n",
            "even\n",
            "Ġamong\n",
            "Ġthese\n",
            "Ġother\n",
            "Ġwarm\n",
            "Ġperiods\n",
            ",\n",
            "Ġyou\n",
            "Ġre\n",
            "Ġmaking\n",
            "Ġa\n",
            "Ġfalse\n",
            "Ġanalogy\n",
            ".\n",
            "Ġyes\n",
            ",\n",
            "Ġthey\n",
            "Ġhad\n",
            "Ġnatural\n",
            "Ġcauses\n",
            ".\n",
            "Ġbut\n",
            "Ġeach\n",
            "Ġhad\n",
            "Ġa\n",
            "Ġdifferent\n",
            "Ġcause\n",
            ",\n",
            "Ġand\n",
            "Ġinvolved\n",
            "Ġa\n",
            "Ġdifferent\n",
            "Ġamount\n",
            "Ġand\n",
            "Ġduration\n",
            "Ġof\n",
            "Ġwarming\n",
            ".\n",
            "Ġthey\n",
            "Ġre\n",
            "Ġas\n",
            "Ġdiss\n",
            "imilar\n",
            "Ġas\n",
            "Ġthey\n",
            "Ġare\n",
            "Ġsimilar\n",
            ".\n",
            "Ġtaking\n",
            "Ġthem\n",
            "Ġtogether\n",
            ",\n",
            "Ġall\n",
            "Ġwe\n",
            "Ġcan\n",
            "Ġreasonably\n",
            "Ġconclude\n",
            "Ġis\n",
            "Ġthat\n",
            "Ġthe\n",
            "Ġearth\n",
            "Ġs\n",
            "Ġclimate\n",
            "Ġseems\n",
            "Ġto\n",
            "Ġchange\n",
            "Ġin\n",
            "Ġresponse\n",
            "Ġto\n",
            "Ġconditions\n",
            "Ġon\n",
            "Ġthe\n",
            "Ġplanet\n",
            ".\n",
            "today\n",
            ",\n",
            "Ġhuman\n",
            "Ġactivity\n",
            "Ġis\n",
            "Ġa\n",
            "Ġdominant\n",
            "Ġforce\n",
            "Ġshaping\n",
            "Ġconditions\n",
            "Ġon\n",
            "Ġyour\n",
            "Ġplanet\n",
            ",\n",
            "Ġso\n",
            "Ġthe\n",
            "Ġpossibility\n",
            "Ġthat\n",
            "Ġit\n",
            "Ġs\n",
            "Ġdriving\n",
            "Ġglobal\n",
            "Ġwarming\n",
            "Ġcan\n",
            "Ġt\n",
            "Ġbe\n",
            "Ġdismissed\n",
            "Ġout\n",
            "Ġof\n",
            "Ġhand\n",
            ".\n",
            "Ġi\n",
            "Ġll\n",
            "Ġgrant\n",
            "Ġthat\n",
            "Ġthe\n",
            "Ġmore\n",
            "Ġcomplicated\n",
            "Ġsomething\n",
            "Ġis\n",
            ",\n",
            "Ġthe\n",
            "Ġeasier\n",
            "Ġit\n",
            "Ġis\n",
            "Ġto\n",
            "Ġmake\n",
            "Ġa\n",
            "Ġmistaken\n",
            "Ġanalogy\n",
            ".\n",
            "Ġthat\n",
            "Ġs\n",
            "Ġespecially\n",
            "Ġtrue\n",
            "Ġbecause\n",
            "Ġthere\n",
            "Ġare\n",
            "Ġmany\n",
            "Ġdifferent\n",
            "Ġtypes\n",
            "Ġof\n",
            "Ġfalse\n",
            "Ġanalogy\n",
            "Ġthat\n",
            "Ġsimilar\n",
            "Ġsymptoms\n",
            "Ġmust\n",
            "Ġshare\n",
            "Ġa\n",
            "Ġcause\n",
            ",\n",
            "Ġthat\n",
            "Ġsimilar\n",
            "Ġactions\n",
            "Ġmust\n",
            "Ġlead\n",
            "Ġto\n",
            "Ġsimilar\n",
            "Ġconsequences\n",
            ",\n",
            "Ġand\n",
            "Ġcountless\n",
            "Ġothers\n",
            ".\n",
            "Ġmost\n",
            "Ġfalse\n",
            "Ġanalog\n",
            "ies\n",
            "Ġyou\n",
            "Ġll\n",
            "Ġcome\n",
            "Ġacross\n",
            "Ġare\n",
            "Ġfar\n",
            "Ġless\n",
            "Ġobvious\n",
            "Ġthan\n",
            "Ġthose\n",
            "Ġcomparing\n",
            "Ġapples\n",
            "Ġto\n",
            "Ġoranges\n",
            ",\n",
            "Ġand\n",
            "Ġclimate\n",
            "Ġis\n",
            "Ġnotoriously\n",
            "Ġcomplex\n",
            ".\n",
            "Ġit\n",
            "Ġrequires\n",
            "Ġcareful\n",
            ",\n",
            "Ġrigorous\n",
            "Ġstudy\n",
            "Ġand\n",
            "Ġevidence\n",
            "Ġcollection\n",
            "Ġand\n",
            "Ġmaking\n",
            "Ġa\n",
            "Ġfalse\n",
            "Ġanalogy\n",
            "Ġlike\n",
            "Ġthis\n",
            "Ġonly\n",
            "Ġimped\n",
            "es\n",
            "Ġthat\n",
            "Ġprocess\n",
            ".\n",
            "it\n",
            "Ġs\n",
            "Ġ,\n",
            "Ġand\n",
            "Ġthe\n",
            "Ġunited\n",
            "Ġnations\n",
            "Ġinter\n",
            "governmental\n",
            "Ġpanel\n",
            "Ġon\n",
            "Ġclimate\n",
            "Ġchange\n",
            "Ġhas\n",
            "Ġfound\n",
            ",\n",
            "Ġaggreg\n",
            "ating\n",
            "Ġdecades\n",
            "Ġof\n",
            "Ġresearch\n",
            ",\n",
            "Ġthat\n",
            "Ġthere\n",
            "Ġis\n",
            "Ġmore\n",
            "Ġthan\n",
            "Ġa\n",
            "Ġchance\n",
            "Ġthe\n",
            "Ġglobal\n",
            "Ġwarming\n",
            "Ġtrend\n",
            "Ġsince\n",
            "Ġthe\n",
            "Ġmid\n",
            "Ġth\n",
            "Ġcentury\n",
            "Ġhas\n",
            "Ġbeen\n",
            "Ġdriven\n",
            "Ġby\n",
            "Ġhuman\n",
            "Ġactivity\n",
            ",\n",
            "Ġnamely\n",
            "Ġthe\n",
            "Ġburning\n",
            "Ġof\n",
            "Ġfossil\n",
            "Ġfuels\n",
            ".\n",
            "you\n",
            "Ġre\n",
            "Ġboth\n",
            "Ġpets\n",
            ",\n",
            "Ġand\n",
            "Ġhe\n",
            "Ġlikes\n",
            "Ġliving\n",
            "Ġin\n",
            "Ġwater\n",
            ",\n",
            "Ġso\n",
            "Ġyou\n",
            "Ġshould\n",
            ",\n",
            "Ġtoo\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split, RandomSampler, SequentialSampler\n",
        "torch.manual_seed(42)\n",
        "from transformers import GPT2LMHeadModel, GPT2Config\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "class GPT2Dataset(Dataset):\n",
        "  def __init__(self, txt_list, tokenizer, gpt2_type=\"gpt2\", max_length=768):\n",
        "\n",
        "    self.tokenizer = tokenizer\n",
        "    self.input_ids = []\n",
        "    self.attn_masks = []\n",
        "\n",
        "    for txt in txt_list: # script별로 encode한 뒤에 tensor로 만들어서 배열에 넣음\n",
        "      encodings_dict = tokenizer('<|startoftext|>'+ txt + '<|endoftext|>',\n",
        "                                 truncation=True, max_length=max_length,\n",
        "                                 padding=\"max_length\")\n",
        "      self.input_ids.append(torch.tensor(encodings_dict['input_ids'][1:])) # CLS 토큰 제거\n",
        "      self.attn_masks.append(torch.tensor(encodings_dict['attention_mask'][1:])) # CLS 토큰 제거\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.input_ids)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.input_ids[idx], self.attn_masks[idx]\n"
      ],
      "metadata": {
        "id": "UC7RiXJ_jMuc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "coverletter_list = df['preprocessed']\n",
        "coverletter_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iAJ0wTeaNtjg",
        "outputId": "26d1586a-01d0-4b94-d96e-f155ca733aac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       baking apple pie discount orange warehouse has...\n",
              "1       in college sports, american universities are e...\n",
              "2       in a pitch black cave, bats can t see much. bu...\n",
              "3       first, a warning. as far as offensive words go...\n",
              "4       chris anderson mike, welcome. it s good to see...\n",
              "                              ...                        \n",
              "4211    about years ago, i took on the task to teach g...\n",
              "4212    good morning. how are you audience good.it s b...\n",
              "4213    if you re here today and i m very happy that y...\n",
              "4214     music the sound of silence, simon amp garfunk...\n",
              "4215    thank you so much, chris. and it s truly a gre...\n",
              "Name: preprocessed, Length: 4216, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pytorch 데이터셋 생성\n",
        "dataset = GPT2Dataset(coverletter_list, tokenizer, max_length=768)\n",
        "\n",
        "# 훈련/검증 데이터셋으로 분리\n",
        "train_size = int(0.9 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "print('{:>5,} training samples'.format(train_size))\n",
        "print('{:>5,} validation samples'.format(val_size))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w37zSLmaOXsO",
        "outputId": "800bbbd6-a172-4b8b-b881-e8fa4997461d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3,794 training samples\n",
            "  422 validation samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pytorch 데이터 로더\n",
        "batch_size = 2 # GPT모델은 큰 모델이므로 batch_size를 2 초과시 메모리 용량 문제 발생\n",
        "\n",
        "# Dataloaders for 훈련/검증 데이터셋\n",
        "train_dataloader = DataLoader(\n",
        "            train_dataset,\n",
        "            sampler = RandomSampler(train_dataset), # batch 랜덤으로 선택\n",
        "            batch_size = batch_size # 선택된 batch_size로 훈련\n",
        "        )\n",
        "\n",
        "validation_dataloader = DataLoader(\n",
        "            val_dataset,\n",
        "            sampler = SequentialSampler(val_dataset),\n",
        "            batch_size = batch_size # 선택된 batch_size로 검증\n",
        "        )"
      ],
      "metadata": {
        "id": "rVlaaHoAOhSj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "configuration = GPT2Config.from_pretrained('gpt2', output_hidden_states=False)"
      ],
      "metadata": {
        "id": "XbzZAZL3lEWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "model.resize_token_embeddings(len(tokenizer)) # token 사이즈 조정\n",
        "\n",
        "seed_val = 425\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)"
      ],
      "metadata": {
        "id": "JrY5dNswnK8K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 하이퍼파라미터 튜닝\n",
        "epochs = 3\n",
        "learning_rate = 5e-4\n",
        "warmup_steps = 1e2\n",
        "epsilon = 1e-8\n",
        "sample_every = 1000"
      ],
      "metadata": {
        "id": "6osvYJoBnQ1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = learning_rate,\n",
        "                  eps = epsilon\n",
        "                  )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "elb6zmN6nZNt",
        "outputId": "20763f0a-9cbc-4987-9632-e88062a2f6e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# training steps = (number of batches) * (number of epochs)\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# learning rate scheduler (training loop이 지날수록 learning rate 달라짐)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                            num_warmup_steps = warmup_steps,\n",
        "                                            num_training_steps = total_steps\n",
        "                                            )"
      ],
      "metadata": {
        "id": "zkbuCdPFnczG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "  return str(datetime.timedelta(seconds=int(round((elapsed)))))"
      ],
      "metadata": {
        "id": "vcWjz-ObnhXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "1QCav7sAnk7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
      ],
      "metadata": {
        "id": "yG9DtLPQyGx7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_t0 = time.time()\n",
        "training_stats = []\n",
        "model = model.to(device)\n",
        "\n",
        "# 1) Training\n",
        "for epoch_i in range(0, epochs):\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training Start')\n",
        "\n",
        "    t0 = time.time()\n",
        "    total_train_loss = 0\n",
        "    model.train()\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        b_input_ids = batch[0].to(device) # GPU 입력으로 사용될 tensor는 모두 to(device) 필요\n",
        "        b_labels = batch[0].to(device)\n",
        "        b_masks = batch[1].to(device)\n",
        "\n",
        "        model.zero_grad()      # 변화도(Gradient) 매개변수를 0으로 만들고\n",
        "\n",
        "        outputs = model(b_input_ids, # loss를 출력하는지 확인\n",
        "                        labels=b_labels,\n",
        "                        attention_mask = b_masks,\n",
        "                        token_type_ids=None\n",
        "                        )\n",
        "\n",
        "        loss = outputs[0]\n",
        "        batch_loss = loss.item()\n",
        "        total_train_loss += batch_loss\n",
        "\n",
        "        # x batch마다 샘플 불러오기\n",
        "        if step % sample_every == 0 and not step == 0:\n",
        "\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            print('  Batch {:>5,}  of  {:>5,}. Loss: {:>5,}.   Elapsed: {:}.'.format(step, len(train_dataloader), batch_loss, elapsed))\n",
        "\n",
        "            model.eval()\n",
        "\n",
        "            sample_outputs = model.generate(\n",
        "                                    bos_token_id=random.randint(1,30000),  # beginning sequence token의 id (랜덤한 단어로 시작하계끔)\n",
        "                                    do_sample=True,  # sampling 사용할지 여부 판단 (use greedy decoding)\n",
        "                                    top_k=50, # number of highest probability vocab tokens\n",
        "                                    max_length = 200, # maximum length of the sequence to be generated\n",
        "                                    top_p=0.95,\n",
        "                                    num_return_sequences=1, # 한 배치당 returned sequence의 수\n",
        "                                    repetition_penalty=2.0,\n",
        "                                    )\n",
        "            for i, sample_output in enumerate(sample_outputs):\n",
        "                  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))\n",
        "\n",
        "            model.train() # train은 일정 sample_every step마다\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "    # 모든 batch에 대해 평균 loss 계산\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "\n",
        "    # epoch별로 소요시간 측정\n",
        "    training_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epoch took: {:}\".format(training_time))\n",
        "\n",
        "\n",
        "    # 2) Validation\n",
        "    print(\"\")\n",
        "    print(\"Validation Start\")\n",
        "\n",
        "    t0 = time.time()\n",
        "    model.eval()\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "\n",
        "    for batch in validation_dataloader:\n",
        "\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_labels = batch[0].to(device)\n",
        "        b_masks = batch[1].to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "\n",
        "            outputs  = model(b_input_ids,\n",
        "                            #token_type_ids=None,\n",
        "                            attention_mask = b_masks,\n",
        "                            labels=b_labels)\n",
        "\n",
        "            loss = outputs[0]\n",
        "\n",
        "        batch_loss = loss.item()\n",
        "        total_eval_loss += batch_loss\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "    validation_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "    print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Training Time': training_time,\n",
        "            'Validation Time': validation_time\n",
        "        }\n",
        "    )\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training Complete\")\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tRe81_dbo558",
        "outputId": "4001a38b-c63e-4302-e767-d30ae169dcbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 3 ========\n",
            "Training Start\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Batch 1,000  of  1,897. Loss: 3.1972579956054688.   Elapsed: 0:08:17.\n",
            "0: mega of all things we ve learned in life, how much we care about what is best for the future. so let s talk to you today and just consider it a very simple question why would i spend my time here if there were only seconds left by tomorrow morning at midnight laughter well...today everybody does some math on that day when they re most tired or asleep hours long every single hour. million billion dollars equals this energy saving project right now, trillion times better than before because electricity has been released into our atmosphere over years like steam powered cars do tonight instead gas was created less four minutes ago but since then millions people have died from breathing carbon dioxide around us using air pollution as their source transportation fuel while everyone else will die soon after with respiratory illness including your neighbor who still doesn t know she needs help without her breath smelling harmful stuff everywhere oh yes again \n",
            "\n",
            "  Average training loss: 3.76\n",
            "  Training epoch took: 0:15:54\n",
            "\n",
            "Validation Start\n",
            "  Validation Loss: 3.19\n",
            "  Validation took: 0:00:34\n",
            "\n",
            "======== Epoch 2 / 3 ========\n",
            "Training Start\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Batch 1,000  of  1,897. Loss: 3.180615186691284.   Elapsed: 0:08:27.\n",
            "0: de, we have seen a rise of social inequalities, inequality and the political will to tackle this economic crisis. i want you all here today for an open discussion about one aspect that s more significant than other problems in life what are our weaknesses as human beings if they don t work out how badly then everything else falls apart from poverty is bad enough it goes up against capitalism because people do not need money or education yet but instead rely on government welfare dollars these systems make wealth irrelevant at every turn whether there be good public goods like water filters running electricity supply through walls so poor infrastructure deteriorates over time until justifications come along with programs designed less costly by hand management techniques look something similar when implemented properly others reallocate resources among those who already exist which leads us astray into disaster mode again while governments take control too quickly.that doesnaldean economics today may seem strange given its historical context only recently has africa entered modern era business ownership became institutionalized democracy was adopted once upon america began exporting\n",
            "\n",
            "  Average training loss: 3.06\n",
            "  Training epoch took: 0:16:03\n",
            "\n",
            "Validation Start\n",
            "  Validation Loss: 3.19\n",
            "  Validation took: 0:00:34\n",
            "\n",
            "======== Epoch 3 / 3 ========\n",
            "Training Start\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch 1,000  of  1,897. Loss: 2.739055633544922.   Elapsed: 0:08:27.\n",
            "0: fficient to build robots by taking advantage of the human capabilities for automation, researchers at harvard and yale have created a new toolkit which takes this concept from artificial intelligence that does amazing things but also uses it with some really neat tricks.take these two smart machines known as lidar bots or lindradotokas we ll call them lasershikumsara taku in china gaijin robotics company has developed an autonomously controlling robot using light emitting diodes called lasers specifically tuned around its laser target so they re sensitive enough not only on detecting objects within range, near infrared radiation no other technology can do just one thing very quickly destroy something without harming anyone else s lives firstly burn parts off if there is damage second most importantly freeze dead body parts so instead their core would be buried away in hot coal rods third use microwaves powered through electric motors fired right into people internal organs once inside limbs fourth take apart bodies like bones then replicate each individual part onto another limbfifth\n",
            "\n",
            "  Average training loss: 2.80\n",
            "  Training epoch took: 0:16:03\n",
            "\n",
            "Validation Start\n",
            "  Validation Loss: 3.19\n",
            "  Validation took: 0:00:34\n",
            "\n",
            "Training Complete\n",
            "Total training took 0:49:46 (h:mm:ss)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "params = list(model.named_parameters())\n",
        "print('GPT-2 모델은 {:} 개의 다른 파라미터를 갖습니다.\\n'.format(len(params)))\n",
        "\n",
        "print('==== Embedding Layer ====\\n')\n",
        "for p in params[0:2]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== First Transformer ====\\n')\n",
        "for p in params[2:14]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== Output Layer ====\\n')\n",
        "for p in params[-2:]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W13KCUiHEGpF",
        "outputId": "57bf21e3-3357-46ff-8a6c-5b1b7c6b00be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT-2 모델은 148 개의 다른 파라미터를 갖습니다.\n",
            "\n",
            "==== Embedding Layer ====\n",
            "\n",
            "transformer.wte.weight                                  (50259, 768)\n",
            "transformer.wpe.weight                                   (1024, 768)\n",
            "\n",
            "==== First Transformer ====\n",
            "\n",
            "transformer.h.0.ln_1.weight                                   (768,)\n",
            "transformer.h.0.ln_1.bias                                     (768,)\n",
            "transformer.h.0.attn.c_attn.weight                       (768, 2304)\n",
            "transformer.h.0.attn.c_attn.bias                             (2304,)\n",
            "transformer.h.0.attn.c_proj.weight                        (768, 768)\n",
            "transformer.h.0.attn.c_proj.bias                              (768,)\n",
            "transformer.h.0.ln_2.weight                                   (768,)\n",
            "transformer.h.0.ln_2.bias                                     (768,)\n",
            "transformer.h.0.mlp.c_fc.weight                          (768, 3072)\n",
            "transformer.h.0.mlp.c_fc.bias                                (3072,)\n",
            "transformer.h.0.mlp.c_proj.weight                        (3072, 768)\n",
            "transformer.h.0.mlp.c_proj.bias                               (768,)\n",
            "\n",
            "==== Output Layer ====\n",
            "\n",
            "transformer.ln_f.weight                                       (768,)\n",
            "transformer.ln_f.bias                                         (768,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_dir = '/content/drive/MyDrive/model_save'\n",
        "\n",
        "# 디렉토리 만들기\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "print(\"Saving model to %s\" % output_dir)\n",
        "\n",
        "# 모델, 토크나이저 등 저장\n",
        "model_to_save = model.module if hasattr(model, 'module') else model\n",
        "model_to_save.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZNYrkP0jEX7r",
        "outputId": "5c077668-116e-4b00-d9ed-b1a0333969d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving model to /content/drive/MyDrive/model_save\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/MyDrive/model_save/tokenizer_config.json',\n",
              " '/content/drive/MyDrive/model_save/special_tokens_map.json',\n",
              " '/content/drive/MyDrive/model_save/vocab.json',\n",
              " '/content/drive/MyDrive/model_save/merges.txt',\n",
              " '/content/drive/MyDrive/model_save/added_tokens.json',\n",
              " '/content/drive/MyDrive/model_save/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eHoFxxHaNQz_",
        "outputId": "1f78b58c-794c-45cd-b12e-914fa8eb9366"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m71.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m91.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m69.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.6.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.15.1 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.30.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 저장한 모델 불러오기\n",
        "from transformers import GPT2TokenizerFast, GPT2LMHeadModel\n",
        "\n",
        "model = GPT2LMHeadModel.from_pretrained('/content/drive/MyDrive/model_save')\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained('/content/drive/MyDrive/model_save')"
      ],
      "metadata": {
        "id": "Q0Ok3Gr5F7CR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import random\n",
        "\n",
        "device = torch.device(\"cuda\")\n",
        "model.cuda()\n",
        "\n",
        "seed_val = 42\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "fGLPtJUcV7YI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "def coverletter_generator(text):\n",
        "  prompt = f\"<|startoftext|> {text}\"\n",
        "  generated = torch.tensor(tokenizer.encode(prompt)[1:]).unsqueeze(0)\n",
        "  generated = generated.to(device)\n",
        "\n",
        "  print(generated)\n",
        "\n",
        "  sample_outputs = model.generate(\n",
        "                                  generated,\n",
        "                                  do_sample=True,\n",
        "                                  top_k=50,\n",
        "                                  max_length = 300,\n",
        "                                  top_p=0.95,\n",
        "                                  num_return_sequences=3,\n",
        "                                  repetition_penalty=1.1\n",
        "                                  )\n",
        "\n",
        "  for i, sample_output in enumerate(sample_outputs):\n",
        "    result = \"{}: {}\\n\\n\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True))\n",
        "    print(result)\n",
        "  return"
      ],
      "metadata": {
        "id": "j3aXVqgAWVOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#원하는 텍스트 입력!\n",
        "answer_generator(\"In order to get good grades in college, you have to study hard first.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E1Qyyti5XFEn",
        "outputId": "29009f52-709e-483e-ffa0-c43413f02183"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[  554,  1502,   284,   651,   922, 19051,   287,  4152,    11,   345,\n",
            "           423,   284,  2050,  1327,   717,    13]], device='cuda:0')\n",
            "0:  In order to get good grades in college, you have to study hard first. this requires a lot of perseverance, and that is why i m here today on the ted stage so everyone can participate in it. and this is not about being smart or funny. for one thing people are much smarter than we think they should be, but this isn t just about being smart. it s also about doing things you want to do yourself.so how do we measure our intelligence let us start with taking a test, like i did three years ago at the university of texas, where two young people put together this puzzle using an app called amazon machine learning systems. amazon machine learning systems can make these predictions on social network sites, by looking at content shared between user specific images that look different than average ones from real life photos. you can see from what s shown here that some users were much more likely than others actually noticed some subset of particular pictures shared within facebook profiles, which they then sorted into clusters based upon similar characteristics, like ethnicity, race and sex amongst other factors. for example percent if the image matches a photo from facebook profile picture,, views will increase. over time, those same posts across all profiles generated million likes per second.now when you open the amazon cloud platform, there you go. this is the screen. every single point indicates a state of the skills associated with the tasks that each individual student was able through out its academic career. and once you re\n",
            "\n",
            "\n",
            "1:  In order to get good grades in college, you have to study hard first. but here s a trick that most employers will use for their athletes and entrepreneurs alike. they might try to lure potential recruits with offers like this all day long into being recruited or hired by the very same employer and then using them after three months on end without any evidence whatsoever of wrongdoing. so, what happens when recruiters refuse to let students stay after these two weeks so let me tell you what i found out. in my research, percent if not less than half of young people reject those offers and report back as bad things over time, such as dropping out, having difficulty understanding why they were rejected due inappropriate performance within an academic discipline or failing credit related factors. in the same process, percent of young people even report being threatened.these numbers are incredibly high because they re based purely upon how easily or too slowly we can recruit from false positives to actually good outcomes before they actually change our course of action. there may be some positive traits associated with those rejections, like self reliance, confidence, perseverance, intelligence, and resilience. for example, while some people may experience rejection as something akin to getting slapped under the bus, others will go about things the same way instead of falling through the cracks.take basketball players who had dropped out at harvard university by promising to improve their score in addition to a high score per se. if those players received a raise and did it again during years of employment, those\n",
            "\n",
            "\n",
            "2:  In order to get good grades in college, you have to study hard first. after all of the tests and homework that these students had to do each day learning english by hand while the rest was on a computer, they would work out exactly how many hours, what kinds of tasks their professors were supposed for them studying i m sure we all know how this works, but one thing s certain it can get very bad before it reaches your grade level. laughter well let me tell you about five things that actually make the worst case scenario worse.my favorite comes from my boss, joe blair. he said today that the majority if not percent of the people who will ever be hired should be doing their phd as architects or engineers over those years, especially when you consider half of the top scientists working now at companies. well, i would bet joe thinks so. but it really doesn t matter whether his views are right or wrong. these professors, like so many others across our society who want tenure and even some career advancement, see their careers fail because they re not taking effective time designing new products and improving customer experiences with existing product designs.here is something that joe actually says the other day why it is important to make a difference. here are three questions you need to ask yourself before hiring someone to cover a story. number one what impact did this person feel on your life what impact does this interaction have did the situation worsen has this relationship made this person more successful this idea have\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}